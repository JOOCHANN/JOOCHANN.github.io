<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CV Researcher</title>
    <description>Study Room</description>
    <link>https://joochann.github.io/</link>
    <atom:link href="https://joochann.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 17 Feb 2022 17:16:46 +0900</pubDate>
    <lastBuildDate>Thu, 17 Feb 2022 17:16:46 +0900</lastBuildDate>
    <generator>Jekyll v4.2.1</generator>
    
      <item>
        <title>Paper Review. Unsupervised Monocular Depth Estimation with Left-Right Consistency@CVPR' 20217</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2017/html/Godard_Unsupervised_Monocular_Depth_CVPR_2017_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;ë…¼ë¬¸-ì„ ì •-ë°°ê²½&quot;&gt;ë…¼ë¬¸ ì„ ì • ë°°ê²½&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\5.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;depth-estimation-ë°°ê²½-ì§€ì‹&quot;&gt;Depth Estimation ë°°ê²½ ì§€ì‹&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\24.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;introduction-1&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\25.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;depth-estimation-as-image-reconstruction&quot;&gt;Depth Estimation as Image Reconstruction&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\35.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\36.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;depth-estimation-network&quot;&gt;Depth Estimation Network&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\41.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\42.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\43.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\44.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\45.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\46.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\47.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\48.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\49.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\50.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\51.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\52.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\53.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\54.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\55.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\56.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-loss&quot;&gt;Training Loss&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\57.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\58.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\59.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\60.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\61.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation Details&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\62.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\63.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\64.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;experiments-1&quot;&gt;Experiments&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\65.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\66.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\67.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\68.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\69.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\70.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Single imageë¡œë¶€í„° depthë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unsupervised learningì´ê¸° ë•Œë¬¸ì— ê³ ë¹„ìš©ì˜ depth ì´ë¯¸ì§€ê°€ ì—†ì–´ë„ ë¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Supervised learning ë°©ë²•ë¡ ë“¤ë³´ë‹¤ ì„±ëŠ¥ì´ ë†’ìœ¼ë©° KITTI ë°ì´í„°ì…‹ì—ì„œ SOTAë¥¼ ë‹¬ì„±í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ìµœê·¼ì—ë„ depth estimationê´€ë ¨í•˜ì—¬ ê³„ì†í•´ì„œ ì—°êµ¬ê°€ ë˜ê³  ìˆìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ê¸°ë³¸ìœ¼ë¡œ ì•Œì•„ ë‘ì–´ì•¼í•  ì§€ì‹ì´ ë°©ëŒ€í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Depth estimation ë¶„ì•¼ê°€ ì–´ë–»ê²Œ ì´ë£¨ì–´ì§€ëŠ”ì§€ ëŒ€ëµì ìœ¼ë¡œ ì•Œê²Œ ë¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ìµœëŒ€í•œ ìì„¸íˆ ì‚´í´ë³´ë ¤ê³  í–ˆìœ¼ë‚˜ ì›ë¡ ì ì¸ ë¶€ë¶„ ì¤‘ ì•„ì§ ì´í•´í•˜ì§€ ëª»í•˜ê³  ê¶ê¸ˆí•œ ë¶€ë¶„ì´ ë‹¤ìˆ˜ ìˆì—ˆìœ¼ë©°, ì¶”í›„ ê³µë¶€í•  ì˜ˆì •&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì•ìœ¼ë¡œ ì•Œì•„ ë‘ì–´ì•¼ í•  ëª‡ê°€ì§€ ë…¼ë¬¸ì„ ë” ë³´ê³  3D object detection ê³µë¶€ë¥¼ ì‹œì‘í•˜ê³  ì‹¶ìŒ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Arnold, Eduardo, et al. â€œA survey on 3d object detection methods for autonomous driving applications.â€Â IEEE Transactions on Intelligent Transportation Systems., 2019&lt;/li&gt;
  &lt;li&gt;Learning Depth-Guided Convolutions for Monocular 3D Object Detection@CVPRâ€™ 2020&lt;/li&gt;
  &lt;li&gt;https://github.com/OniroAI/MonoDepth-PyTorch&lt;/li&gt;
  &lt;li&gt;http://vision.middlebury.edu/stereo/&lt;/li&gt;
  &lt;li&gt;https://blog.naver.com/dldlsrb45/&lt;/li&gt;
  &lt;li&gt;https://eehoeskrap.tistory.com/103&lt;/li&gt;
  &lt;li&gt;https://www.youtube.com/watch?v=jI1Qf7zMeIs&amp;amp;ab_channel=ComputerVisionFoundationVideos&lt;/li&gt;
  &lt;li&gt;Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. â€œSpatial transformer networks@NIPSâ€™ 2015&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 11 Aug 2021 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Unsupervised-Monocular-Depth-Estimation-with-Left-Right-Consistency/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Unsupervised-Monocular-Depth-Estimation-with-Left-Right-Consistency/</guid>
        
        <category>Depth Estimation</category>
        
        <category>Disparity</category>
        
        <category>3D</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Depth Estimation</category>
        
      </item>
    
      <item>
        <title>Paper Review. Scale-aware Automatic Augmentation for Object Detection@CVPR' 2021</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Scale-Aware_Automatic_Augmentation_for_Object_Detection_CVPR_2021_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Object detection taskì—ì„œ ìµœì ì˜ augmentation policyë¥¼ ì°¾ëŠ” &lt;strong&gt;scale-aware AutoAug&lt;/strong&gt;ë°©ë²•ì„ ì œì•ˆí•¨&lt;/li&gt;
  &lt;li&gt;Scale ë¶ˆë³€ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ image-levelê³¼ box-levelì—ì„œ augmentationì„ ì§„í–‰í•œ &lt;strong&gt;scale-aware search space&lt;/strong&gt;ë¥¼ ì •ì˜í•¨&lt;/li&gt;
  &lt;li&gt;Object detection taskì—ì„œ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” AutoAugment-det, RandAugment ë³´ë‹¤ ìš°ìˆ˜í•¨&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Data Augmentation
    &lt;ul&gt;
      &lt;li&gt;Color operations : Brightness, contrast, and whitening â€¦&lt;/li&gt;
      &lt;li&gt;Geometric operations : Re-scaling, and flip â€¦&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;autoaug-det1&quot;&gt;AutoAug-det[1]&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Object detectionì˜ data augmentation policy&lt;/li&gt;
  &lt;li&gt;Main idea : Classificationì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ë³´ë‹¤ detectionì„ ìœ„í•œ ë°ì´í„°ì…‹ì˜ ìˆ˜ê°€ ë” ì ê¸° ë•Œë¬¸ì—, &lt;strong&gt;bounding box&lt;/strong&gt; ê°’ ë³€í™”ë¥¼ í•¨ê»˜ ì£¼ì–´ì•¼í•¨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img1.PNG&quot; width=&quot;300&quot; /&gt;
&lt;em&gt;Fig1.Bounding box augmentation ì˜ˆì‹œ&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë°•ìŠ¤ì˜ contextë¥¼ ë³´ê³  color or geometric augmentationì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì´ ì„¸ê°€ì§€ operationì´ ìˆë‹¤ê³  ë³´ë©´ ë¨
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;Color operations : ì´ë¯¸ì§€ì˜ color ê°’ì„ ë³€í™˜í•˜ë˜, bounding boxì˜ ìœ„ì¹˜ ì¢Œí‘œ ê°’ì—ëŠ” ë³€í™”ë¥¼ ì£¼ì§€ ì•ŠìŒ 
  ex) Equalize, contrast, brightness â€¦&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Geometric operations : ì´ë¯¸ì§€ì˜ ìœ„ì¹˜ì •ë³´ë¥¼ ë°”ê¾¸ë©°, bounding boxì˜ ìœ„ì¹˜ë‚˜ ì‚¬ì´ì¦ˆë¥¼ ê°™ì´ ë³€í™”í•¨ 
  ex) Rotate, shearX, tanslationY â€¦&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Bounding box operations : ì´ë¯¸ì§€ ë‚´ì—ì„œ bounding boxê°€ ìˆëŠ” ë¶€ë¶„ì˜ í”½ì…€ë§Œ ë³€í™”ì‹œí‚´ 
  ex) Bbox_Only_Equalize, Bbox_Only_Rotate â€¦&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training ê³¼ì •ì—ì„œë§Œ data augmentationì„ ìˆ˜í–‰í•˜ë©°, í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ augmentationì´ ì ìš©ë˜ëŠ”ë° ì–´ë–¤ ìˆœì„œë¡œ ì‚¬ìš©í• ì§€ search methodë¥¼ í†µí•´ ìµœì í™” ì‹œí‚´&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Search spaceì˜ ê³„ì‚°&lt;/p&gt;

\[(22ğ¿ğ‘€)^{ğ‘ğ¾}=(22Ã—6Ã—6)^{2Ã—5}â‰ˆ9.6Ã—{10}^{28}\]
  &lt;/li&gt;
  &lt;li&gt;ê°ê° Nê°œì˜ ìˆœì°¨ì ì¸ transformation operandsë¥¼ ê°–ëŠ” Kê°œì˜ sub-policyë¥¼ í•™ìŠµí•˜ê³ , training ê³¼ì •ì—ì„œ ê° ì´ë¯¸ì§€ì— ì ìš©ë  policyê°€ ëœë¤ìœ¼ë¡œ ì„ íƒë¨&lt;/li&gt;
  &lt;li&gt;Mì€ transformationì´ ì ìš©ë  í™•ë¥ , Lì€ transformationì´ ì ìš©ë  í¬ê¸°(magnitude)&lt;/li&gt;
  &lt;li&gt;ë¬¸ì œì 1 : object detectionì˜ í•„ìˆ˜ì ì¸ imageì™€ box levelì˜ í¬ê¸° ë¬¸ì œë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•˜ìŒ&lt;/li&gt;
  &lt;li&gt;ë¬¸ì œì 2 : ì—°ì‚°ëŸ‰ì´ ë§¤ìš° í¼ (400 TPU for 2days)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;-ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê°ì²´ íƒì§€ë¥¼ ìœ„í•œ ìë™ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” scale-aware data augmentation ì „ëµì„ ì œì•ˆí•¨-&lt;/p&gt;

&lt;h3 id=&quot;review-of-autoaug&quot;&gt;Review of AutoAug&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Auto augmentation
    &lt;ul&gt;
      &lt;li&gt;ì„¸ê°€ì§€ì˜ ë©”ì¸ êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì§
        &lt;ol&gt;
          &lt;li&gt;Search space : ì–¼ë§ˆë§Œí¼ì˜ augmentation ì¢…ë¥˜ê°€ ì¡´ì¬í•˜ëŠ”ì§€&lt;/li&gt;
          &lt;li&gt;Search algorithm : ì–´ë–»ê²Œ ì¢‹ì€ augmentationì„ ì„ íƒí•  ê²ƒì¸ì§€&lt;/li&gt;
          &lt;li&gt;Estimation metric : ì–´ë–»ê²Œ ì¢‹ì€ augmentationì„ í‰ê°€í•  ê²ƒì¸ì§€&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;scale-aware-search-space&quot;&gt;Scale-Aware Search Space&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Motivation
    &lt;ul&gt;
      &lt;li&gt;ì €ìë“¤ì€ ì˜¤ë¸Œì íŠ¸ ë°•ìŠ¤ ì´ì™¸ì˜ pixelì„ ì§€ìš°ê³  í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë¹„êµí•´ ë´„&lt;/li&gt;
      &lt;li&gt;ê·¸ ê²°ê³¼ small objectì˜ ê²½ìš° ì£¼ë³€ pixelì´ ì¤‘ìš”í•¨ì„ ì•Œ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” í•™ìŠµ ëª¨ë¸ì´ ëª¨ë“  í¬ê¸°ì˜ ê°ì²´ë¥¼ ì ì ˆíˆ ì²˜ë¦¬í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŒ&lt;/li&gt;
      &lt;li&gt;Bounding boxì˜ scaleì— ë”°ë¼ augmentationì„ ë‹¤ë¥´ê²Œ í•´ì£¼ëŠ” area ratioë¥¼ ì œì•ˆí•¨&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img2.PNG&quot; width=&quot;800&quot; /&gt;
  &lt;em&gt;Fig2. Motivation&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Image-level augmentations
    &lt;ul&gt;
      &lt;li&gt;Probabilities ğ‘ƒ
        &lt;ul&gt;
          &lt;li&gt;\(ğ‘ƒ_{ğ‘œğ‘¢ğ‘¡}\)ì´ ì„ íƒë  í™•ë¥  : \(Ã—0 ~ Ã—0.5\)&lt;/li&gt;
          &lt;li&gt;\(ğ‘ƒ_{ğ‘–ğ‘›}\)ì´ ì„ íƒë  í™•ë¥  : \(Ã—0 ~ Ã—0.5\)&lt;/li&gt;
          &lt;li&gt;\(ğ‘ƒ_{ğ‘œğ‘Ÿğ‘–}\)ì´ ì„ íƒë  í™•ë¥  : \(1âˆ’ğ‘ƒ_{ğ‘–ğ‘›}âˆ’ğ‘ƒ_{ğ‘œğ‘¢ğ‘¡}\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Magnitudes ğ‘€.
        &lt;ul&gt;
          &lt;li&gt;\(ğ‘ƒ_{ğ‘œğ‘¢ğ‘¡}\)ì˜ í¬ê¸° : \(Ã—1.0 ~ Ã—1.5\)&lt;/li&gt;
          &lt;li&gt;\(ğ‘ƒ_{ğ‘–ğ‘›}\)ì˜ í¬ê¸° : \(Ã—0.5 ~ Ã—1.0\)&lt;/li&gt;
          &lt;li&gt;\(ğ‘ƒ_{ğ‘œğ‘Ÿğ‘–}\)ì˜ í¬ê¸° : \(Ã—1.0\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Zoom-in operation
        &lt;ul&gt;
          &lt;li&gt;probability range : 6 discrete values \([0, 0.1, 0.2, 0.3, 0.4, 0.5]\)&lt;/li&gt;
          &lt;li&gt;magnitude range : 6 discrete values \([0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Zoom-out operation
        &lt;ul&gt;
          &lt;li&gt;probability range : 6 discrete values \([0, 0.1, 0.2, 0.3, 0.4, 0.5]\)&lt;/li&gt;
          &lt;li&gt;magnitude range : 6 discrete values \([1.0, 1.1, 1.2, 1.3, 1.4, 1.5]\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Box-level augmentations
    &lt;ul&gt;
      &lt;li&gt;8 color operationsì™€ 6 geometric operations ì¡´ì¬
        &lt;ul&gt;
          &lt;li&gt;probability range : 6 discrete values \([0, 0.2, 0.4, 0.6, 0.8, 1.0]\)&lt;/li&gt;
          &lt;li&gt;magnitude range : 6 discrete values \([0, 2, 4, 6, 8, 10]\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Area ratio (scale ratio)
        &lt;ul&gt;
          &lt;li&gt;area ratio type : small, middle, large&lt;/li&gt;
          &lt;li&gt;area ratio range : 10 discrete values \([0.2, 0.4, 0.6, 0.8, 1.0, 2, 4, 6, 8, 10]\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img3.PNG&quot; width=&quot;800&quot; /&gt;
  &lt;em&gt;Fig3. Image-level, and box-level augmentations&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Box-level augmentationsì˜ íš¨ê³¼
    &lt;ol&gt;
      &lt;li&gt;Gaussianì„ í†µí•œ ë¶€ë“œëŸ¬ìš´ augmentation&lt;/li&gt;
    &lt;/ol&gt;

\[ğ´=ğ›¼(ğ‘¥,ğ‘¦)âˆ™ğ¼+(1âˆ’ğ›¼(ğ‘¥,ğ‘¦))âˆ™ğ‘‡\]

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img4.PNG&quot; width=&quot;800&quot; /&gt;
  &lt;em&gt;Fig4. An example of Gaussian-based box-level augmentation&lt;/em&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Area ratioë¼ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°
        &lt;ul&gt;
          &lt;li&gt;Area ratio \(r\)ì— ë”°ë¼ í‘œì¤€í¸ì°¨ë¥¼ ë‹¤ë¥´ê²Œ ì ìš©í•˜ì—¬ \(r\)ì´ í¬ë©´ \(ğ‘Ÿ(ğ‘ _{ğ‘ğ‘œğ‘¥})\)ê°€ ë” ë„“ì€ ë²”ìœ„ë¡œ augmentationì´ ìì—°ìŠ¤ëŸ½ê²Œ ì ìš©ë¨&lt;/li&gt;
          &lt;li&gt;ë°˜ëŒ€ë¡œ \(r\)ì´ ì‘ìœ¼ë©´ \(ğ‘Ÿ(ğ‘ _{ğ‘ğ‘œğ‘¥})\)ê°€ ë” ì‘ì•„ì ¸ ë¶€ë¶„ì ìœ¼ë¡œ augmentationì´ ì ìš©ë¨&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;

\[ğ›¼(ğ‘¥,ğ‘¦)=expâ¡(âˆ’((ğ‘¥âˆ’ğ‘¥_ğ‘)^2/(2ğœ_ğ‘¥^2))+((ğ‘¦âˆ’ğ‘¦_ğ‘)^2/(2ğœ_ğ‘¦^2)))\]

\[ğ‘‰=âˆ«_0^ğ»âˆ«_0^ğ‘Šğ›¼(ğ‘¥,ğ‘¦)dğ‘¥dğ‘¦\]

\[ğœ_ğ‘¥=â„\sqrt{(ğ‘Š/ğ»)/2ğœ‹}âˆ™ğ‘Ÿ,  ğœ_ğ‘¦=ğ‘¤\sqrt{(ğ»/ğ‘Š)/2ğœ‹}âˆ™ğ‘Ÿ\]

\[ğ‘Ÿ(ğ‘ _{ğ‘ğ‘œğ‘¥})=ğ‘‰/ğ‘ _{ğ‘ğ‘œğ‘¥}\]
  &lt;/li&gt;
  &lt;li&gt;Search space summary
    &lt;ul&gt;
      &lt;li&gt;Image-level augmentation : \((6^2)^2\)
        &lt;ul&gt;
          &lt;li&gt;Zoom-in : \(6Ã—6\)&lt;/li&gt;
          &lt;li&gt;Zoom-out : \(6Ã—6\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Box-level augmentation : \(({10}^3)Ã—((8Ã—6Ã—6)Ã—(6^3))^5\)
        &lt;ul&gt;
          &lt;li&gt;Area ratio : \(10Ã—10Ã—10\)&lt;/li&gt;
          &lt;li&gt;Color operation : \(8Ã—6Ã—6\)&lt;/li&gt;
          &lt;li&gt;Geometric operation : \(6Ã—6Ã—6\)&lt;/li&gt;
          &lt;li&gt;Box-level operationì€ ì´ 5ê°œì˜ sub-policyë¡œ êµ¬ì„±, 1ê°œì˜ sub-policyëŠ” 2ê°œì˜ augmentationìœ¼ë¡œ êµ¬ì„± (color, geometric operation)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Total search space : \(({(6^2)}^2Ã—{10}^3)Ã—{((8Ã—6Ã—6)Ã—(6^3))}^5=(1.2)^{30}\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;scale-aware-estimation-metric&quot;&gt;Scale-Aware Estimation Metric&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;â€œBalanced optimization over different scalesâ€ì´ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•˜ì—¬ search ê³¼ì • ì¤‘ì— different scaleë³„ë¡œ accumulated lossì™€ accuracyë¥¼ ì´ìš©í•¨&lt;/p&gt;

\[min_ğ‘â¡ğ‘“(\{ğ¿_{(ğ‘–âˆˆğ‘†)}^ğ‘\}, \{(AP)_{(ğ‘–âˆˆğ‘†)}^ğ‘\})\]

    &lt;ul&gt;
      &lt;li&gt;\(\{(AP)_{(ğ‘–âˆˆğ‘†)}^ğ‘\}\) : Data augmentation policy \(ğ‘\)ë¡œ í•™ìŠµí•œ plain modelì˜ ê° scale \(ğ‘–\)ë³„ë¡œ validation accuracy \((ğ´ğ‘ƒ)_ğ‘–\)&lt;/li&gt;
      &lt;li&gt;\(\{ğ¿_{(ğ‘–âˆˆğ‘†)}^ğ‘\}\) : Data augmentation policy \(ğ‘\)ë¡œ í•™ìŠµí•œ plain modelì˜ ê° scale \(ğ‘–\)ë³„ë¡œ Accumulated loss \(ğ¿_ğ‘–\)&lt;/li&gt;
      &lt;li&gt;ë‹¤ë¥¸ scaleë³„ë¡œ balanced optimizationì€ í•„ìˆ˜ì ì´ë©°, ê°„ë‹¨í•˜ê²Œ ë‹¤ì–‘í•œ scaleë³„ë¡œ lossì˜ standard deviationì„ ì¸¡ì •í•˜ë©´ ë˜ì§€ë§Œ, ì´ëŠ” sub-optimalì— ë¹ ì§€ê²Œ ë¨&lt;/li&gt;
      &lt;li&gt;ê·¸ë˜ì„œ Pareto Optimality[2]ë¥¼ ìˆ˜í–‰í•¨&lt;/li&gt;
      &lt;li&gt;Pareto Optimality[2]ì´ë€[3]
        &lt;ul&gt;
          &lt;li&gt;ìì› ë°°ë¶„ì´ ê°€ì¥ íš¨ìœ¨ì ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ìƒíƒœë¥¼ pareto optimalì´ë¼ê³  í•¨
  ParetoëŠ” ì†í•´ë¥¼ ë³´ëŠ” ì‚¬ëŒì€ í•˜ë‚˜ë„ ì—†ê³  ì´ìµì„ ë³´ëŠ” ì‚¬ëŒë§Œ ìˆëŠ” ê²½ìš°ë¥¼ pareto ê°œì„ ì´ë¼ê³  í•¨&lt;/li&gt;
          &lt;li&gt;Pareto ê°œì„ ì´ ë¶ˆê°€ëŠ¥í•  ì •ë„ë¡œ ê°œì„ ëœ ìƒíƒœë¥¼ pareto optimalì´ë¼ê³  í•¨
  ì¦‰, ëª¨ë“  ì‚¬ëŒì´ íƒ€ì¸ì˜ ë¶ˆë§Œì„ ì‚¬ëŠ” ì¼ ì—†ì´ëŠ” ìê¸° ë§Œì¡±ì„ ë” ì´ìƒ ì¦ê°€ì‹œí‚¬ ìˆ˜ ì—†ëŠ” ìƒíƒœê°€ â€˜íŒŒë ˆí†  ìµœì â€™ ì„&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Objective function
    &lt;ul&gt;
      &lt;li&gt;Pareto optimalityì— ì˜í•´ â€œThe optimization over scales can not be better without hurting the accuracy of any other scaleâ€ ë¼ëŠ” ê°œë…ì„ ê°€ì ¸ì˜´&lt;/li&gt;
    &lt;/ul&gt;

\[min_ğ‘â¡ğ‘“(\{ğ¿_{(ğ‘–âˆˆğ‘†)}^ğ‘\}, \{(AP)_{(ğ‘–âˆˆğ‘†)}^ğ‘\}) = ğœ(\{ğ¿_{(ğ‘–âˆˆğ‘†)}^ğ‘\})âˆ™Î¦(\{(AP)_{(ğ‘–âˆˆ\hat ğ‘†)}^ğ‘\})\]

\[Î¦(\{(AP)_{(ğ‘–âˆˆ\hat ğ‘†)}^ğ‘\})=âˆ_{(ğ‘–âˆˆ\hat ğ‘†)}{({ğ´ğ‘ƒ}_ğ‘–)}/{({ğ´ğ‘ƒ}_ğ‘–^ğ‘)}\]

    &lt;ul&gt;
      &lt;li&gt;Policy \(ğ‘\)ì„ í†µí•˜ì—¬ fine-tuning í–ˆì„ ë•Œ, ì„±ëŠ¥ ì €í•˜ê°€ ë°œìƒí•˜ëŠ” scale \(ğ‘†\)Â Ì‚ì— ì²˜ë²Œì„ ì£¼ëŠ” penalization factor \(Î¦\)ë¥¼ ì œì•ˆí•¨&lt;/li&gt;
      &lt;li&gt;\({({ğ´ğ‘ƒ}_ğ‘–)}/{({ğ´ğ‘ƒ}_ğ‘–^ğ‘)}\) : scale-wise ratio of original and the fine-tuned accuracy&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Autoaugment ë°©ë²•ì€ ì¼ë°˜ì ìœ¼ë¡œ proxy task(í›ˆë ¨ ì´ë¯¸ì§€ê°€ ìˆëŠ” ì‘ì€ í•˜ìœ„ ì§‘í•©)ì— ëŒ€í•œ validation accuracyë¥¼ search metricìœ¼ë¡œ ì‚¬ìš©í•¨. ì‹¤í—˜ ê²°ê³¼, proxy taskì—ì„œ accuracyë¡œ êµ¬í•œ policyì™€ scale-aware metricì„ ì´ìš©í•˜ì—¬ êµ¬í•œ policyë¥¼ ì‹¤ì œ datasetì—ì„œ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ì˜€ì„ ë•Œ, coefficentê°€ ë” ë†’ìŒ&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img5.PNG&quot; width=&quot;400&quot; /&gt;
  &lt;em&gt;Fig5. Coefficients between actual accuracy and metrics&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;search-algorithm&quot;&gt;Search algorithm&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Sample datasetì—ì„œ scale-aware estimation metricì´ ì œì¼ ìµœì†Œê°€ ë˜ë„ë¡ fine-tuningí•˜ì—¬ ìµœì ì˜ augmentation policyë¥¼ ì°¾ìŒ&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Scratchë¶€í„° í•™ìŠµí•˜ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê¸°ì—, data augmentationì—†ì´ í•™ìŠµí•œ plain modelì„ augmentation policyë¡œ fine-tuningí•˜ì—¬ ì‹œê°„ì„ ì¤„ì„&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img6.PNG&quot; width=&quot;400&quot; /&gt;
  &lt;em&gt;Fig6. Search algorithm&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Detectorë³„ ì„±ëŠ¥ ì§€í‘œ&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img7.PNG&quot; width=&quot;400&quot; /&gt;
  &lt;em&gt;Fig7. Comparison with object detection augmentation strategies on MS COCO dataset&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Improvement details on RetinaNet ResNet-50&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img8.PNG&quot; width=&quot;400&quot; /&gt;
  &lt;em&gt;Fig8. Improvement details&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comparison with AutoAug-det on RetinaNet ResNet-50&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img9.PNG&quot; width=&quot;400&quot; /&gt;
  &lt;em&gt;Fig9. Comparison with AutoAug-det&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Search on RetinaNet ResNet-50 with different metrics&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img10.PNG&quot; width=&quot;400&quot; /&gt;
  &lt;em&gt;Fig10. Different scale-aware estimation metrics&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Improvements across detection and segmentation frameworks&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img11.PNG&quot; width=&quot;800&quot; /&gt;
  &lt;em&gt;Fig11. Different frameworks&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comparison with SOTA data augmentation methods for object detection&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img12.PNG&quot; width=&quot;800&quot; /&gt;
  &lt;em&gt;Fig12. Comparison with SOTA data augmentation methods&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Augmentation policyë¥¼ ì°¾ëŠ”ë° 20 GPU-daysê°€ ê±¸ë¦¬ë©°, ë‹¤ë¥¸ RLì„ ì‚¬ìš©í•œ auto augmentationë°©ë²•ë³´ë‹¤ ë¹ ë¥´ì§€ë§Œ, ì—¬ì „íˆ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼&lt;/li&gt;
  &lt;li&gt;ë°ì´í„°ê°€ ì ì€ object detection taskì— ì¢‹ìŒ&lt;/li&gt;
  &lt;li&gt;Box levelì—ì„œ ë°ì´í„° augmentationí•˜ëŠ” ê²ƒê³¼ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜í•˜ëŠ” ê¸°ë²•ì´ íš¨ê³¼ì ì¸ ê²ƒ ê°™ìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Zoph, Barret, et al. â€œLearning data augmentation strategies for object detection.â€Â European Conference on Computer Vision. Springer, Cham, 2020.&lt;/li&gt;
  &lt;li&gt;John Black, Nigar Hashimzade, and Gareth Myles. A dictionary of economics. Oxford university press, 2012&lt;/li&gt;
  &lt;li&gt;http://www.aistudy.co.kr/economics/pareto_optimal.htm&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 14 Jul 2021 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Scale-aware-Automatic-Augmentation-for-Object-Detection/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Scale-aware-Automatic-Augmentation-for-Object-Detection/</guid>
        
        <category>Augmentation</category>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Augmentation</category>
        
      </item>
    
      <item>
        <title>Paper Review. Learning from Noisy Anchors for One-stage Object Detection@CVPR' 2020</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Learning_From_Noisy_Anchors_for_One-Stage_Object_Detection_CVPR_2020_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;ë¬¸ì œì 
    &lt;ul&gt;
      &lt;li&gt;IoU(Intersection over Union)ì„ í†µí•´ positiveì™€ negative anchorë¥¼ ë‚˜ëˆ„ê²Œ ë˜ë©´ noise anchorê°€ ë°œìƒí•´ í•™ìŠµì´ ì˜ ë˜ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ë°œìƒí•¨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;í•´ê²°ì±…
    &lt;ul&gt;
      &lt;li&gt;Anchorì˜ ì¢‹ê³ (positive) ë‚˜ì¨(negative)ì„ 2ë¶„í• í•˜ì—¬ í‘œí˜„í•˜ì§€ ì•Šê³  ë™ì ìœ¼ë¡œ ì—°ì†ì ì¸ ê°’ìœ¼ë¡œ í‘œí˜„í•¨&lt;/li&gt;
      &lt;li&gt;ì§€ë‚œë²ˆ í¬ìŠ¤íŒ…í–ˆë˜ DAL[1] ë°©ë²•ë¡ ê³¼ ìœ ì‚¬í•˜ê²Œ classification branchì™€ regression branchë¥¼ ì‚¬ìš©í•˜ì—¬ ì¢‹ì€ anchorë¥¼ ë¶„ë¥˜í•˜ëŠ” cleanliness scoreë¥¼ ì œì•ˆí•¨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ì¶”ê°€ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ê³„ì‚°ëŸ‰ì€ ê±°ì˜ ì—†ìœ¼ë©°, COCOë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ì„ 2%ì´ìƒ ì¦ê°€ì‹œí‚´&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;positivenegative-anchor-ì„ íƒì˜-ì¤‘ìš”ì„±-ë°-ë¬¸ì œì &quot;&gt;Positive/Negative Anchor ì„ íƒì˜ ì¤‘ìš”ì„± ë° ë¬¸ì œì &lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide8.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ì œì•ˆí•˜ëŠ”-positivenegative-anchor-ì„ íƒ-ë°©ë²•&quot;&gt;ì œì•ˆí•˜ëŠ” Positive/Negative Anchor ì„ íƒ ë°©ë²•&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ë¬¸ì œì 
    &lt;ul&gt;
      &lt;li&gt;IoU(Intersection over Union)ì„ í†µí•œ positive/negative anchor ì„ íƒìœ¼ë¡œ í•™ìŠµì´ ì˜ë˜ì§€ ì•Šì•„ classificationê³¼ regressionê°„ì˜ ë¶ˆì¼ì¹˜ ë¬¸ì œ ë°œìƒ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ì œì•ˆí•˜ëŠ” í•´ê²° ë°©ë²• 1
    &lt;ul&gt;
      &lt;li&gt;Anchorë¥¼ positive/negativeë¡œ discreteí•˜ê²Œ ë‚˜ëˆ„ì§€ ë§ê³  continuousí•œ ê°’ìœ¼ë¡œ í‘œí˜„í•˜ì&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ì œì•ˆí•˜ëŠ” í•´ê²° ë°©ë²• 2
    &lt;ul&gt;
      &lt;li&gt;Classification branchì™€ regression branchì˜ ê²°ê³¼ê°’ì„ ì‚¬ìš©í•˜ì—¬ ì¢‹ì€(positive) anchorë¥¼ ì„ íƒí•´ë³´ì&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide15.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;soft-label&quot;&gt;Soft Label&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide20.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;sample-re-weighting&quot;&gt;Sample Re-Weighting&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;One-stage detectorëŠ” two-stage detectorì™€ ë‹¤ë¥´ê²Œ RPNì´ ì¡´ì¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— negative proposalì´ ë§¤ìš° ë§ìœ¼ë©°, positive proposalì€ ì ìŒ&lt;/li&gt;
  &lt;li&gt;ì´ ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ focal lossê°€ ì¡´ì¬í•˜ì§€ë§Œ, ë ˆì´ë¸”ì´ ë…¸ì´ì¦ˆê°€ ìˆëŠ” ì¦‰ anchorì˜ ì¢‹ê³  ë‚˜ì¨ì´ ì˜ëª» íŒë³„ëœ ìœ„ì¹˜ì˜ proposalì€ í•™ìŠµí•˜ëŠ”ë° ë°©í•´ê°€ ë¨&lt;/li&gt;
  &lt;li&gt;ê·¸ëŸ¬ë¯€ë¡œ cleanliness scoreë¥¼ í™œìš©í•œ re-weight sampleë°©ë²•ì„ ì œì•ˆí•¨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide35.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;í•™ìŠµìš© : COCO trainval135k set&lt;/li&gt;
      &lt;li&gt;í…ŒìŠ¤íŠ¸ìš© : COCO test-dev2017&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Detectors
    &lt;ul&gt;
      &lt;li&gt;ì£¼ë¡œ ì‚¬ìš©í•œ ëª¨ë¸ : RetinaNet&lt;/li&gt;
      &lt;li&gt;ë²¡ë³¸ : ResNet-50, ResNet-101, ResNeXt-101 32x8d&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Implementation details
    &lt;ul&gt;
      &lt;li&gt;GPUs : 4&lt;/li&gt;
      &lt;li&gt;Batch size : 8 (2 images per GPU)&lt;/li&gt;
      &lt;li&gt;Optimizer : SGD&lt;/li&gt;
      &lt;li&gt;Input image size : 800 x 800&lt;/li&gt;
      &lt;li&gt;Augmentation : horizontal flip&lt;/li&gt;
      &lt;li&gt;Multi-scale training ì‚¬ìš© {640, 672, 704, 736, 768, 800}&lt;/li&gt;
      &lt;li&gt;Multi-scale testing ì‚¬ìš© {400, 500, 600, 700, 900, 1000, 1100, 1200}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ablation-study&quot;&gt;Ablation Study&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide38.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;main-results&quot;&gt;Main Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide39.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;discussions&quot;&gt;Discussions&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide41.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide42.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;IoU(Intersection over Union)ì„ í†µí•´ positiveì™€ negative anchorë¥¼ ë‚˜ëˆ„ê²Œ ë˜ë©´ noise anchorê°€ ë°œìƒí•´ í•™ìŠµì´ ì˜ ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— classification branchì™€ regression branchë¥¼ ì‚¬ìš©í•˜ì—¬ ì¢‹ì€ anchorë¥¼ ë¶„ë¥˜í•˜ëŠ” cleanliness scoreë¥¼ ì œì•ˆí•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Anchorì˜ ì¢‹ê³ (positive) ë‚˜ì¨(negative)ì„ 2ë¶„í• í•˜ì—¬ í‘œí˜„í•˜ì§€ ì•Šê³  ë™ì ìœ¼ë¡œ ì—°ì†ì ì¸ ê°’ soft labelë¡œ í‘œí˜„í•˜ì—¬ anchorì˜ í‘œí˜„ì„ ë‹¤ì–‘í•˜ê²Œ(í’ë¶€í•˜ê²Œ) í•´ì£¼ëŠ” ë°©ë²•ì´ ì¬ë°Œì—ˆìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Anchorë¥¼ ìƒì„±í•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•˜ì§€ë§Œ í•™ìŠµí•˜ê¸°ì— ì¢‹ì€ anchorì¸ì§€ ì•ˆ ì¢‹ì€ anchorì¸ì§€ íŒë³„í•˜ëŠ” ê²ƒ ë˜í•œ ë§¤ìš° ì¤‘ìš”í•¨&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Ming, Qi, et al. â€œDynamic Anchor Learning for Arbitrary-Oriented Object Detection.â€Â arXiv preprint arXiv:2012.04150Â (2020).&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 02 Jun 2021 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Learning-from-Noisy-Anchors-for-One-stage-Object-Detection/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Learning-from-Noisy-Anchors-for-One-stage-Object-Detection/</guid>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Align Deep Features for Oriented Object Detection@IEEE Transactions on Geoscience and Remote Sensing' 2021</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9377550?casa_token=ftqghqE0R94AAAAA:IrBS8DpsKG3WrrOyChjwLlgU_T7Tfo2OnmNiBv1cpI9SrNL_1gs9GVZbIW82mawhZ7-FTWov0cc&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ê¸°ì¡´ ë°©ë²•ë“¤ì€ scale, angle, aspect ratioë¥¼ ì‚¬ìš©í•˜ì—¬ heuristicí•˜ê²Œ ì •ì˜ëœ anchorì— ì˜ì¡´í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;íšŒì „ëœ Anchor boxì™€ íšŒì „ë˜ì§€ ì•Šì€(axis-aligned) convolutionì‚¬ì´ì˜ misalignmentë¡œ ì¸í•´ classificationê³¼ localization ì •í™•ë„ ê°„ì— ë¶ˆì¼ì¹˜ê°€ ì¡´ì¬í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Single-shot alignment Network(S^2A-Net)ì€ ë‘ê°œì˜ ëª¨ë“ˆë¡œ ì´ë£¨ì–´ì ¸ ìˆìŒ.
    &lt;ul&gt;
      &lt;li&gt;Feature Alignment Module(FAM) : ê³ í’ˆì§ˆ anchorë¥¼ ìƒì„±í•˜ê³ , alignment convolutionì„ í†µí•´ anchor ìœ„ì¹˜ì— ë§ëŠ” convolutionì„ ìˆ˜í–‰í•¨&lt;/li&gt;
      &lt;li&gt;Oriented Detection Module(ODM) : Active Rotating Filters(ARF)ë¥¼ ì‚¬ìš©í•´ ë°©í–¥ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ì—¬ orientation-sensitive featureë¥¼ ì œê³µí•¨ìœ¼ë¡œì¨ classificationê³¼ localization ì •í™•ë„ ê°„ì˜ ë¶ˆì¼ì¹˜ë¥¼ ì–´ëŠì •ë„ í•´ê²°í•¨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DOTA ë°ì´í„° ì…‹ì—ì„œ SOTAë¥¼ ë‹¬ì„±í•¨&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\5.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Anchor boxì™€ objectê°„ì˜ misalignmentë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë…¼ë¬¸ì—ì„œëŠ” Single-shot alignment Network(S^2A-Net)ì„ ì œì•ˆí•¨.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Feature Alignment Module(FAM)
    &lt;ul&gt;
      &lt;li&gt;ë‹¤ë¥¸ ë°©ë²•ë¡ ë“¤ê³¼ ë‹¤ë¥´ê²Œ í•˜ë‚˜ì˜ horizontal anchorë¥¼ ê°–ìŒ.&lt;/li&gt;
      &lt;li&gt;Anchor Refinement Network (ARN)ì—ì„œ ê³ í’ˆì§ˆì˜ íšŒì „ëœ anchorë¥¼ ìƒì„±í•¨.&lt;/li&gt;
      &lt;li&gt;Alignment convolutionì„ í†µí•´ anchor ìœ„ì¹˜ì— ë§ëŠ” convolutionì„ ìˆ˜í–‰í•¨.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Oriented Detection Module(ODM)
    &lt;ul&gt;
      &lt;li&gt;Active Rotating Filters(ARF)ë¥¼ ì‚¬ìš©í•´ ë°©í–¥ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ì—¬ orientation-sensitive featureë¥¼ ì œê³µí•¨ìœ¼ë¡œì¨ classificationê³¼ localization ì •í™•ë„ ê°„ì˜ ë¶ˆì¼ì¹˜ë¥¼ ì–´ëŠì •ë„ í•´ê²°í•¨.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;model-architecture---fpn&quot;&gt;Model Architecture - FPN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\15.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;model-architecture---fam&quot;&gt;Model Architecture - FAM&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\16.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;anchor-refinement-network&quot;&gt;Anchor Refinement Network&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\27.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;alignment-convolution-layer&quot;&gt;Alignment Convolution Layer&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\35.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\41.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\42.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\43.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\44.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\45.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\46.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\47.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\48.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\49.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\50.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\51.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\52.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\53.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\54.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\55.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\56.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\57.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\58.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\59.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\60.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\61.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;model-architecture---odm&quot;&gt;Model Architecture - ODM&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\62.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\63.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\64.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;active-rotating-filters&quot;&gt;Active Rotating Filters&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\65.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\66.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\67.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\68.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\69.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\70.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\71.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\72.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\73.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\74.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;model-architecture---summary&quot;&gt;Model Architecture - Summary&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\75.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\76.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\77.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\78.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;loss-function&quot;&gt;Loss Function&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\79.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\80.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;DOTA
        &lt;ul&gt;
          &lt;li&gt;í¬ê¸° ë²”ìœ„ : 800 x 800 ~ 4000 x 4000&lt;/li&gt;
          &lt;li&gt;ì´ë¯¸ì§€ ìˆ˜ : 2806&lt;/li&gt;
          &lt;li&gt;Augmentation : ì¢Œ,ìš°,ìƒ,í•˜ flip, multi scale training, multi scale test&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;HRSC2016
        &lt;ul&gt;
          &lt;li&gt;í¬ê¸° ë²”ìœ„ : 300 x 300 ~ 1500 x 900&lt;/li&gt;
          &lt;li&gt;ì´ë¯¸ì§€ ìˆ˜ : 617&lt;/li&gt;
          &lt;li&gt;Augmentation : ì¢Œ,ìš° flip&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\81.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\82.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\83.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\84.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\85.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\86.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\87.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\88.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\89.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\90.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Feature Alignment Moduleê³¼ Oriented Detection Moduleì´ í¬í•¨ëœ Single-Shot Alignment Networkì„ ì œì•ˆí•˜ë©° ì†ë„ì™€ ì •í™•ë„ ëª¨ë‘ ì±™ê¹€&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Predict box ì •ë³´ë¥¼ Anchor boxì— ë„£ì–´ì„œ í•™ìŠµ ê°€ëŠ¥í•œ refine anchorë¥¼ ë§Œë“œëŠ” ë°©ë²•ì´ ì¢‹ì•˜ìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deformable convolutionì„ ì“¸ ë•Œ, offset fieldë¥¼ refine anchorë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° ë•Œë¬¸ì— íš¨ê³¼ê°€ ìˆì—ˆìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ëª¨ë¸ì´ ê°ˆìˆ˜ë¡ ì–´ë ¤ì›Œì§€ëŠ” ê²ƒ ê°™ê³ , ë…¼ë¬¸ì—ì„œëŠ” ì•„ì£¼ ê°„ëµí•˜ê²Œ ì„¤ëª…í•˜ê¸° ë•Œë¬¸ì— ì „ë°˜ì ì¸ ê¸°ì´ˆ ì§€ì‹ì´ ì—†ìœ¼ë©´ ì´í•´í•˜ê¸° í˜ë“¬&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì½”ë“œë¥¼ ëœ¯ì–´ë³´ë©° ì‚´í´ë³´ë‹ˆ ëª¨ë¸ êµ¬ì¡°ë¥¼ ëª…í™•íˆ ì´í•´í•  ìˆ˜ ìˆì—ˆê³ , Lossë¶€ë¶„ë„ ìì„¸í•˜ê²Œ ëœ¯ì–´ë³¼ ê³„íšì„&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Zhou, Yanzhao, et al. â€œOriented response networks.â€ Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 14 Apr 2021 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Align-Deep-Features-for-Oriented-Object-Detection/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Align-Deep-Features-for-Oriented-Object-Detection/</guid>
        
        <category>Object Detection</category>
        
        <category>Oriented Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Feature Selective Anchor-Free Module for Single-Shot Object Detection@CVPR' 2019</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Feature_Selective_Anchor-Free_Module_for_Single-Shot_Object_Detection_CVPR_2019_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Anchor ê¸°ë°˜ì˜ detecting ë°©ì‹ì—ëŠ” ë‘ ê°€ì§€ì˜ í•œê³„ì ì´ ì¡´ì¬
    &lt;ol&gt;
      &lt;li&gt;heuristic-guideë¥¼ í†µí•œ feature selection&lt;/li&gt;
      &lt;li&gt;overlap-based anchor sampling&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ëª¨ë¸ì˜ ì „ì²´ì ì¸ êµ¬ì¡°ëŠ” FPNì„ í†µê³¼í•œ Feature mapì— ê¸°ì¡´ ë°©ì‹ì¸ anchor-based branchì™€ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” anchor-free branchì„ í†µí•´ ê°ì²´ë¥¼ ê²€ì¶œ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Anchor-free branchì—ì„œ online feature selectionì„ í•˜ëŠ” FSAF(Feature Selection Anchor Free)ë°©ì‹ì„ ì œì•ˆí•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;FSAF ë°©ì‹ì€ ê°€ë³ê³  ë¹ ë¥´ë©° í”ŒëŸ¬ê·¸ì¸ ì‹œí‚¤ê¸° ì‰¬ì›€&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\img1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;em&gt;Fig1. ì œì•ˆí•˜ëŠ” ë°©ë²• FSAFëŠ” ì‘ì€ ì˜¤ë¸Œì íŠ¸ë¥¼ ì˜ ì°¾ì„ ìˆ˜ ìˆìŒ&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\6.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\8.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ground-truth-and-loss&quot;&gt;Ground-truth and Loss&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\21.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;online-feature-selection&quot;&gt;Online Feature Selection&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\24.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\29.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ê°€ë³ê³  ë¹ ë¥´ë©° ì–´ë””ì—ë“  ì¶”ê°€í•˜ê¸° ì‰¬ìš´ FSAF ëª¨ë“ˆì„ ì œì•ˆí•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Anchor-based ë°©ë²•ì—ì„œëŠ” ì‘ì€ ì˜¤ë¸Œì íŠ¸ëŠ” low levelì˜ feature pyramid mapì„ ì‚¬ìš©í•˜ê³ , í° ì˜¤ë¸Œì íŠ¸ëŠ” high levelì˜ feature mapì„ ì‚¬ìš©í•˜ì—¬ detectionì„ í•˜ë©° ì´ëŠ” ì„±ëŠ¥ì ìœ¼ë¡œ í•œê³„ê°€ ìˆìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ë…¼ë¬¸ì—ì„œëŠ” anchor-free branchë¥¼ ì¶”ê°€í•¨ìœ¼ë¡œì¨ feature mapì„ ë™ì ìœ¼ë¡œ ì„ íƒí•˜ë¯€ë¡œ ì˜¤ë¸Œì íŠ¸ë¥¼ ë” ì˜ ì°¾ì„ ìˆ˜ ìˆì—ˆìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì‘ì€ ì˜¤ë¸Œì íŠ¸ëŠ” low levelì˜ feature pyramid mapì„ ì‚¬ìš©í•˜ê³ , í° ì˜¤ë¸Œì íŠ¸ëŠ” high levelì˜ feature mapì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” IoUë¥¼ í†µí•´ positive anchorë¥¼ ì„ íƒí•˜ê¸° ë•Œë¬¸ì´ë©°, ê²°êµ­ IoUë¥¼ í†µí•´ ì•µì»¤ë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•ì€ ì¢‹ì§€ ì•ŠìŒ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Wed, 31 Mar 2021 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Feature-Selective-Anchor-Free-Module-for-Single-Shot-Object-Detection/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Feature-Selective-Anchor-Free-Module-for-Single-Shot-Object-Detection/</guid>
        
        <category>Object Detection</category>
        
        <category>Anchor Free</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Multiple Anchor Learning for Visual Object Detection@CVPR' 2020</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/html/Ke_Multiple_Anchor_Learning_for_Visual_Object_Detection_CVPR_2020_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ê³ ì •ëœ ì•µì»¤ ì§‘í•©ì— ëŒ€í•˜ì—¬ classificationê³¼ regressionì€ ê³µë™ìœ¼ë¡œ ìµœì í™”í•˜ì§€ ëª»í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Anchor bagë¥¼ êµ¬ì„±í•˜ì—¬ ê°ì²´ë³„ ëŒ€í‘œì ì¸ anchorë¥¼ ì„ íƒí•˜ëŠ” ë°©ì‹ì¸ MAL(Multiple Anchor Learning) ë°©ë²•ì„ ì œì•ˆí•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ë°˜ë³µì ì¸ anchor ì„ íƒ í”„ë¡œì„¸ìŠ¤ë¥¼ ìµœì í™”í•˜ê¸° ìœ„í•´ anchorì˜ confidenceë¥¼ ë‚®ì¶¤ìœ¼ë¡œ êµë€(perturbing)í•˜ì—¬ ê°€ì¥ ì¢‹ì€ anchorë¥¼ ì„ íƒí•˜ë„ë¡ í•¨&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\10.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;retinanet-revisit&quot;&gt;RetinaNet Revisit&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\13.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;multiple-anchor-learning&quot;&gt;Multiple Anchor Learning&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\16.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;selection-depression-optimization&quot;&gt;Selection-Depression Optimization&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\21.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\25.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;optimization-analysis&quot;&gt;Optimization Analysis&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\26.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\31.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Best anchorë¥¼ ì„ íƒí•˜ë©´ì„œ Classification confidenceì™€ localizationì„ ê°™ì´ optimizeí•˜ê¸° ë•Œë¬¸ì— ê°ì²´ì™€ anchorë¥¼ ë§¤ì¹­í•˜ëŠ” ê²ƒì„ í•™ìŠµí•  ìˆ˜ ìˆìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ê·¼ë³¸ì ìœ¼ë¡œëŠ” IoUë¥¼ í†µí•´ anchorë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ë°©ë²•ë“¤ì´ ê³„ì†í•´ì„œ ì—°êµ¬ë˜ê³  ìˆìŒ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Wed, 24 Feb 2021 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Multiple-Anchor-Learning-for-Visual-Object-Detection/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Multiple-Anchor-Learning-for-Visual-Object-Detection/</guid>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Dynamic Anchor Learning for Arbitrary-Oriented Object Detection@AAAI' 2021</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ojs.aaai.org/index.php/AAAI/article/view/16336&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ê¸°ì¡´ ëª¨ë¸ë“¤ì€ IoUë¥¼ ì ìš©í•˜ì—¬ positive ì™€ negative ì•µì»¤ë¥¼ ìƒ˜í”Œë§ì„ í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Positive ì•µì»¤ëŠ” í•­ìƒ ì •í™•í•œ íƒì§€ë¥¼ ë³´ì¥í•  ìˆ˜ ì—†ì§€ë§Œ, ì¼ë¶€ negative ì•µì»¤ëŠ” ì •í™•í•œ ìœ„ì¹˜ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì´ëŠ” IoUë¥¼ í†µí•œ ì•µì»¤ì˜ í’ˆì§ˆ í‰ê°€ê°€ ì ì ˆí•˜ì§€ ì•ŠìŒì„ ë‚˜íƒ€ë‚´ë©°, ì´ë¡œ ì¸í•´ classification confidenceì™€ localization accuracy ì‚¬ì´ì— ë¶ˆì¼ì¹˜ê°€ ë°œìƒí•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì´ ë…¼ë¬¸ì—ì„œëŠ” ìƒˆë¡­ê²Œ ì •ì˜ëœ matching degreeë¥¼ í™œìš©í•˜ì—¬ ì•µì»¤ì˜ localizationì˜ ê°€ëŠ¥ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” dynamic anchor learning(DAL) ë°©ë²•ì„ ì œì•ˆí•¨&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\3.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ DAL(Dynamic Anchor Learning) ë°©ë²•ì„ ì œì•ˆí•¨
    &lt;ol&gt;
      &lt;li&gt;ì•µì»¤ì˜ localization ê°€ëŠ¥ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ MD(Matching Degree)ë¥¼ ì„¤ê³„&lt;/li&gt;
      &lt;li&gt;Sample selectionì„ í•™ìŠµí•˜ê¸° ìœ„í•´ matching degreeë¥¼ ì‚¬ìš©í•˜ì—¬ false-positive ìƒ˜í”Œì„ ì œê±°í•˜ê³  ì ì¬ì ì¸ ê³ í’ˆì§ˆ í›„ë³´ë¥¼ ë™ì ìœ¼ë¡œ ì„ íƒí•¨&lt;/li&gt;
      &lt;li&gt;matching-sensitive loss functionì„ í†µí•´ classificationê³¼ regressionê°„ì˜ ë¶ˆì¼ì¹˜ë¥¼ ì™„í™”í•¨&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;dynamic-anchor-selection&quot;&gt;Dynamic Anchor Selection&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\7.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;matching-sensitive-loss&quot;&gt;Matching-Sensitive Loss&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\11.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;Aerial datasets
        &lt;ul&gt;
          &lt;li&gt;HRSC2016&lt;/li&gt;
          &lt;li&gt;DOTA&lt;/li&gt;
          &lt;li&gt;UCAS-AOD&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Scene text datasets
        &lt;ul&gt;
          &lt;li&gt;ICDAR 2015&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\19.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ê³ ì„±ëŠ¥ì˜ íšŒì „ëœ ê°ì²´ íƒì§€ë¥¼ ìœ„í•´ Dynamic anchor learning ë°©ë²•ì„ ì œì•ˆí•¨.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Classification confidenceì™€ localization accuracy ì‚¬ì´ì˜ ë¶ˆì¼ì¹˜ë¥¼ ì–´ëŠì •ë„ í•´ê²°í•¨ (ì–‘ì˜ ìƒê´€ê´€ê³„)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì—¬ëŸ¬ ê°œì˜ ì•µì»¤ë¥¼ ë§Œë“œëŠ” ê²ƒì´ ì¤‘ìš”í•œ ê²ƒì´ ì•„ë‹ˆë¼ positive, negative ì•µì»¤ë¥¼ ì–´ë–»ê²Œ êµ¬ë¶„(í‰ê°€)í•˜ëŠëƒê°€ ì¤‘ìš”í•¨&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Wed, 20 Jan 2021 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Dynamic-Anchor-Learning-for-Arbitrary-Oriented-Object-Detection/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Dynamic-Anchor-Learning-for-Arbitrary-Oriented-Object-Detection/</guid>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Transformer-XL Attentive Language Models Beyond a Fixed-Length Context@ACL' 2019</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.02860&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ê¸°ì¡´ì˜ transformerëŠ” ê³ ì •ëœ ê°œìˆ˜(512)ì˜ tokenë“¤ì„ ê°–ëŠ” í•œ ê°œì˜ segmentë§Œì„ inputìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬, ì—°ì†ëœ segmentë“¤ ê°„ì˜ dependencyë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;í˜„ì¬ segmentë¥¼ ì²˜ë¦¬í•  ë•Œ, ì´ì „ segmentë¥¼ ì²˜ë¦¬í•  ë•Œ ê³„ì‚°ëœ hidden stateë“¤ì„ ì‚¬ìš©í•˜ëŠ” recurrenceë¥¼ ì¶”ê°€í•˜ì—¬ ìœ„ ë¬¸ì œë¥¼ í•´ê²°í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ëª¨ë¸ì— ì í•©í•œ positional encodingì„ ë³€í˜•í•˜ì˜€ìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Transformer-XLì€ language modelingì—ì„œ SOTAì˜ ì„±ëŠ¥ì„ ê¸°ë¡í•¨&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;transformer&quot;&gt;Transformer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\7.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;vanilla-transformer-language-model&quot;&gt;Vanilla Transformer Language Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\9.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;segment-level-recurrence-with-state-reuse&quot;&gt;Segment-Level Recurrence with State Reuse&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\12.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;relative-position-encoding&quot;&gt;Relative Position Encoding&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\22.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;transformer-xl&quot;&gt;Transformer XL&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\23.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\31.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;RNN ê³„ì—´ ëª¨ë¸ê³¼ vanilla Transformer modelë³´ë‹¤ long-term dependencyë¥¼ ë” ì˜ ì¡ì•„ëƒˆìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì´ì „ segmentë¥¼ ì €ì¥í•´ë‘ê³  ì‚¬ìš©í•¨ìœ¼ë¡œì¨ predictionì‹œ ìƒë‹¹í•œ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Recurrence ë°©ë²•ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ relative position encodingì„ ìˆ˜ì‹ì ìœ¼ë¡œ í’€ì–´ì„œ ì˜ë¯¸ë¶€ì—¬ë¥¼ í•˜ê³ , ì ìš©í•œ ê²ƒì´ ëŒ€ë‹¨í•˜ë‹¤ê³  ëŠë‚Œ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Tue, 10 Nov 2020 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context/</guid>
        
        <category>Attention</category>
        
        <category>Machine Translation</category>
        
        <category>NLP</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Machine Translation</category>
        
      </item>
    
      <item>
        <title>Paper Review. Attention is all you need@NIPS' 2017</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ë‹¹ì‹œ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì€ Attention ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ encoderì™€ decoderë¥¼ ì—°ê²°í•œ ëª¨ë¸ì´ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ê¸°ì¡´ ëª¨ë¸ë“¤ì²˜ëŸ¼ RNNë˜ëŠ” CNNì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  attention ë©”ì»¤ë‹ˆì¦˜ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” Transformer ëª¨ë¸ì„ ì œì•ˆí•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ê¸°ê³„ ë²ˆì—­ íƒœìŠ¤í¬ ì‹¤í—˜ì„ í†µí•´ ëª¨ë¸ì´ ë³‘ë ¬í™” ê°€ëŠ¥í•˜ë©°, í•™ìŠµ ì‹œê°„ì´ ë‹¨ì¶•ë˜ëŠ” ê²ƒì„ ë³´ì„&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\11.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\12.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;positional-encoding&quot;&gt;Positional Encoding&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\20.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;encoder&quot;&gt;Encoder&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\26.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;multi-head-attention&quot;&gt;Multi-Head Attention&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\35.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\41.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;position-wise-fc-layer&quot;&gt;Position-Wise FC Layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\42.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\43.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;decoder&quot;&gt;Decoder&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\44.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\45.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\46.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\47.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\48.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;why-self-attention&quot;&gt;Why Self-Attention&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\49.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;training&quot;&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\50.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\51.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\52.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\53.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\54.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\55.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Machine Translation íƒœìŠ¤í¬ì—ì„œ Transformer ëª¨ë¸ì€ ë¹ ë¥´ê²Œ í•™ìŠµí•˜ë©°, ì„±ëŠ¥ë„ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤Œ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Recurrent ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ sequential ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ëª¨ë¸&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Encoderì™€ decoderì—ì„œ attentionì„ í†µí•´ queryì™€ ê°€ì¥ ë°€ì ‘í•œ ì—°ê´€ì„±ì„ ê°€ì§€ëŠ” valueë¥¼ ê°•ì¡° í•  ìˆ˜ ìˆìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ëª¨ë¸ ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•´ì§&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Attentionì„ ë‹¤ì‹œ ì •ë¦¬í•˜ë©´ì„œ ê³µë¶€í•  ìˆ˜ ìˆì—ˆìŒ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Tue, 29 Sep 2020 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Attention-is-all-you-need/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Attention-is-all-you-need/</guid>
        
        <category>Attention</category>
        
        <category>Machine Translation</category>
        
        <category>NLP</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Machine Translation</category>
        
      </item>
    
      <item>
        <title>Paper Review. Bidirectional LSTM-CRF Models for Sequence Tagging@arXiv' 2015</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1508.01991&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;LSTM ë ˆì´ì–´ì™€ CRF ë ˆì´ì–´ë¥¼ ì´ìš©í•˜ì—¬ LSTM-CRF ëª¨ë¸ì„ ë§Œë“¦&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BI-LSTM ë ˆì´ì–´ì™€ CRF ë ˆì´ì–´ë¥¼ ì´ìš©í•˜ì—¬ BI-LSTM-CRF ëª¨ë¸ì„ ë§Œë“¦&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BI-LSTM-CRF ëª¨ë¸ì„ ì´ìš©í•˜ë©´ POS taggingíƒœìŠ¤í¬ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë„ë‹¬í•  ìˆ˜ ìˆìŒ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;crfë€&quot;&gt;CRFë€&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\20.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;feature-function&quot;&gt;Feature Function&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\32.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;models&quot;&gt;Models&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\35.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\41.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\42.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;lstm-crf-model&quot;&gt;LSTM-CRF Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\43.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\44.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\45.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\46.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\47.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\48.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\49.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\50.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\51.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\52.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\53.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\54.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\55.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\56.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\57.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\58.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;bi-lstm-crf-model&quot;&gt;BI-LSTM-CRF Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\59.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\60.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\61.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\62.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\63.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\64.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\65.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;POS taggingê³¼ ê°™ì´ ê·œì¹™ì´ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ì¼ ê²½ìš° CRFë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì˜ë¯¸ê°€ ìˆìŒ (ì˜ˆ. ë¶€ì‚¬ ë’¤ì— ë¶€ì‚¬ê°€ ì˜¬ ìˆ˜ ì—†ìŒ)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CRFì˜ feature functionì„ ì •ì˜í•˜ê¸° ìœ„í•´ì„œëŠ” ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ë¥¼ ê³ ë ¤í•´ì•¼í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Feature functionì´ ë§ì•„ì§ˆìˆ˜ë¡ ì—°ì‚°ëŸ‰ì´ ë§ì•„ ì†ë„ê°€ ëŠë ¤ì§ˆ í…ë° ë³´í†µ ëª‡ ê°œì¸ì§€ ê¶ê¸ˆí•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CRFë¥¼ ì‚¬ìš©í•˜ë©´ ì†ë„ê°€ ë§ì´ ëŠë ¤ì§ˆ í…ë° ì†ë„ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ì–´ ì•„ì‰¬ì›€&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Tue, 08 Sep 2020 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Bidirectional-LSTM-CRF-Models-for-Sequence-Tagging/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Bidirectional-LSTM-CRF-Models-for-Sequence-Tagging/</guid>
        
        <category>LSTM</category>
        
        <category>Tagging</category>
        
        <category>NLP</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Tagging</category>
        
      </item>
    
      <item>
        <title>Paper Review. SlowFast Networks for Video Recognition@ICCV' 2019</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2019/html/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;AVA Challenge 2019ì—ì„œ Action recognition 1ë“±&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Video Recognitionì„ ìœ„í•œ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;ë‘ê°œì˜ pathwayê°€ ì¡´ì¬
    &lt;ul&gt;
      &lt;li&gt;Slow pathway : spatial semantic ì •ë³´ë¥¼ íšë“&lt;/li&gt;
      &lt;li&gt;Fast pathway : motion at fine temporal resolution ì •ë³´ë¥¼ íšë“&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ì‚¬ëŒì˜ ì‹œê° ì‹œìŠ¤í…œì„ ëª¨ë°©í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì¶•í•¨&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\3.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;spatial-structure&quot;&gt;Spatial Structure&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\8.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;temporal-events&quot;&gt;Temporal Events&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\10.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;slowfast-networks&quot;&gt;SlowFast Networks&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\14.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;biological-derivation&quot;&gt;Biological Derivation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\19.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;slow-pathway&quot;&gt;Slow Pathway&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\22.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;fast-pathway&quot;&gt;Fast Pathway&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\29.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;overall-process&quot;&gt;Overall Process&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\35.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\41.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\42.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\43.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\44.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\45.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\46.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\47.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;Kinetics-400
        &lt;ul&gt;
          &lt;li&gt;306,245 video clips&lt;/li&gt;
          &lt;li&gt;400 human action classes&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Kinetics-600
        &lt;ul&gt;
          &lt;li&gt;495,547 video clips&lt;/li&gt;
          &lt;li&gt;600 human action classes&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Charades
        &lt;ul&gt;
          &lt;li&gt;9,848 video clips&lt;/li&gt;
          &lt;li&gt;157 human action classes&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;AVA dataset
        &lt;ul&gt;
          &lt;li&gt;430 video clips&lt;/li&gt;
          &lt;li&gt;80 human action classes&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;action-classification&quot;&gt;Action Classification&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\48.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\49.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\50.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\51.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\52.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ava-action-detection&quot;&gt;AVA Action Detection&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\53.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\54.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\55.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ì‚¬ëŒì˜ ì¸ì§€ ì‹œìŠ¤í…œì„ ëª¨ë°©í•´ ëª¨ë¸êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ì˜€ê¸° ë•Œë¬¸ì— ì™œ ì´ë ‡ê²Œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë§Œë“¤ì—ˆëŠ”ì§€ ì´í•´ê°€ ë¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì‚¬ëŒì„ íƒì§€í•˜ëŠ” ëª¨ë¸(Detectron)ì˜ ì„±ëŠ¥ì´ 90í”„ë¡œê°€ ë„˜ìœ¼ë¯€ë¡œ ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ , ì‚¬ëŒì˜ ì•¡ì…˜ì„ ì˜ˆì¸¡í•˜ëŠ”ë° ì´ˆì ì„ ë‘” ëª¨ë¸(Slowfast)ì„ ë§Œë“  ê²ƒ ê°™ìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AI Grand Challenge ì²«ë²ˆì§¸ Taskì—ì„œëŠ” ë‹¨ìˆœíˆ ì‹¤ì‹ í•œ ì‚¬ëŒë§Œ ì°¾ìœ¼ë©´ ë˜ë¯€ë¡œ, ì´ ëª¨ë¸ì´ ì í•©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ. í•˜ì§€ë§Œ, ë‚˜ì¤‘ì— ë³µì¡í•œ ì˜ìƒ ì¸ì‹ Taskê°€ ì§„í–‰ëœë‹¤ë©´ ì‚¬ìš©í•´ ë³¼ ìˆ˜ ìˆìŒ (í´ë˜ìŠ¤ìˆ˜ê°€ ë§ì•„ì§€ê±°ë‚˜, ì˜ìƒì´ ê¸¸ì–´ì§€ëŠ” ê²½ìš°)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Tue, 14 Jul 2020 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/SlowFast-Networks-for-Video-Recognition/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/SlowFast-Networks-for-Video-Recognition/</guid>
        
        <category>Activity Recognition</category>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Activity Recognition</category>
        
      </item>
    
      <item>
        <title>Paper Review. Gliding vertex on the horizontal bounding box for multi-oriented object@IEEE transactions on pattern analysis and machine intelligence' 2020</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9001201?casa_token=R_LrtCbzL4QAAAAA:3YPWwU6yo4ysQverP0MVY6M7ogWpzNIvCLQldEhaIJchm9r5DNjzHs3OiNYasEkLeYy4jqKn29s&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-oriented objectë¥¼ detectioní•˜ê¸° ê°„ë‹¨í•˜ì§€ë§Œ íš¨ê³¼ì ì¸ í”„ë ˆì„ì›Œí¬&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Oriented objectì˜ ë„¤ ê¼­ì§€ì ì„ ì§ì ‘ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, horizontalí•œ ë°•ìŠ¤ë¥¼ ê°€ì§€ê³  gliding offsetì„ ì˜ˆì¸¡í•˜ì—¬ oriented objectì˜ ê¼­ì§€ì ì„ ì˜ˆì¸¡í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Oriented objectì™€ horizontalí•œ ë°•ìŠ¤ ì‚¬ì´ì˜ ë©´ì  ë¹„ìœ¨ì„ ê¸°ë°˜ìœ¼ë¡œ obliquity factorë¥¼ ì¸¡ì •í•˜ì—¬ ì–´ë–¤ ë°•ìŠ¤ë¥¼ ì„ íƒí• ì§€ ê²°ì •í•¨&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\2.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\8.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;multi-oriented-object-representation&quot;&gt;Multi-Oriented Object Representation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\10.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\11.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-objective&quot;&gt;Training Objective&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\16.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;DOTA
        &lt;ul&gt;
          &lt;li&gt;A large-scale and challenging dataset for object detection in aerial images with quadrangle annotations&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;HRSC2016
        &lt;ul&gt;
          &lt;li&gt;Ship detection in aerial images&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;MSRA-TD500
        &lt;ul&gt;
          &lt;li&gt;Detecting long and oriented texts&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;RCTW-17
        &lt;ul&gt;
          &lt;li&gt;Detecting long and oriented texts&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;MW-18Mar
        &lt;ul&gt;
          &lt;li&gt;Multi-target horizontal pedestrian tracking dataset&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\25.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-oriented objectë¥¼ detectioní•˜ê¸° ê°„ë‹¨í•˜ì§€ë§Œ íš¨ê³¼ì ì¸ í”„ë ˆì„ì›Œí¬&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RPNì—ì„œ Anchorë¥¼ ì°¾ì„ ë•Œ ê°ë„ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ ê¸°ì¡´ 2-stage ë°©ë²•ë“¤ë³´ë‹¤ ì†ë„ê°€ ë¹ ë¦„&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;íšŒì „ëœ ê°ì²´ íƒì§€ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ë‹¹ì‹œ SOTAë¥¼ ì°ìŒìœ¼ë¡œì¨, ìì‹ ë“¤ì´ ì œì•ˆí•˜ëŠ” (íšŒì „ëœ ê°ì²´ë¥¼ ë°”ë¼ë³´ëŠ” representation)ë°©ë²•ì´ íšŒì „ëœ ê°ì²´ë¥¼ ì°¾ëŠ” ì¼ë°˜ì ì¸ íƒœìŠ¤í¬ì— íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒì„ ë³´ì—¬ì¤Œ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;íšŒì „ëœ ê°ì²´ë¥¼ ë°”ë¼ë³´ëŠ” ì¼ë°˜ì ì¸ ì‹œì„ ì„ ë‹¤ë¥¸ ì‹œì„ ìœ¼ë¡œ ë°”ë¼ë³¸ ê²ƒì´ ìƒˆë¡œì› ìŒ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Tue, 02 Jun 2020 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Gliding-vertex-on-the-horizontal-bounding-box-for-multi-oriented-object/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Gliding-vertex-on-the-horizontal-bounding-box-for-multi-oriented-object/</guid>
        
        <category>Object Detection</category>
        
        <category>Oriented Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Deformable Convolutional Networks@ICCV' 2017</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_iccv_2017/html/Dai_Deformable_Convolutional_Networks_ICCV_2017_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ê¸°ì¡´ CNNì—ì„œ ì‚¬ìš©í•˜ëŠ” conv, poolingì´ ê¸°í•˜í•™ì ìœ¼ë¡œ ì¼ì •í•œ íŒ¨í„´ì„ ê°€ì •í•˜ê³  ìˆê¸° ë•Œë¬¸ì— ë³µì¡í•œ transformationì— ìœ ì—°í•˜ê²Œ ëŒ€ì²˜í•˜ê¸° ì–´ë ¤ì›€&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ê¸°ì¡´ì—ëŠ” weightì„ êµ¬í•˜ëŠ” ë°©ë²•ì— ì´ˆì²¨ì„ ë§ì·„ë‹¤ë©´, ì´ ë…¼ë¬¸ì€ ì–´ë–¤ ë°ì´í„°ë¥¼ ë½‘ì„ ê²ƒì¸ì§€ì— ì´ˆì ì„ ë‘ &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Convolution -&amp;gt; Convolution + learnable offset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RoI pooling -&amp;gt; RoI pooling + learnable offset&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\5.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;deformable-convolution&quot;&gt;Deformable Convolution&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\11.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;deformable-roi-pooling&quot;&gt;Deformable RoI Pooling&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\17.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;understanding-deformable-convnets&quot;&gt;Understanding Deformable ConvNets&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\20.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\24.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ë°ì´í„°ì˜ íŠ¹ì§•ì„ í•„í„°ë¥¼ í†µí•´ ì°¾ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì…ë ¥ ë°ì´í„°ì—ì„œ offsetì„ í†µí•´ ì§ì ‘ ì°¾ìœ¼ë ¤ í•´ì„œ ì°¸ì‹ í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deformable convolutionê³¼ deformable RoI poolingì„ ì‚¬ìš©í–ˆì„ ë•Œ ì˜ë¯¸ ë¶„ì„ì„ ê·¸ë¦¼ìœ¼ë¡œ ë³´ì—¬ì£¼ì–´ ì§ê´€ì ìœ¼ë¡œ ì™œ ì¢‹ì•„ì¡ŒëŠ”ì§€ ëŠë‚„ ìˆ˜ ìˆì—ˆìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ëª¨ë“  ë¬¼ì²´ê°€ ì •ì‚¬ê°í˜•ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— ì •ì‚¬ê°í˜•ì˜ í•„í„°ë¥¼ ê°€ì§€ê³  convolutionì„ í•˜ëŠ” ê²ƒì€ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìœ¼ë©°, ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” í•˜ë‚˜ì˜ ë°©í–¥ì„±ì„ ì œì‹œí•´ ì¤€ ê²ƒ ê°™ìŒ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Wed, 29 Apr 2020 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Deformable-Convolutional-Networks/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Deformable-Convolutional-Networks/</guid>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Learning to Reweight Examples for Robust Deep Learning@ICML' 2018</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v80/ren18a.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ì¼ë°˜ì ì¸ ì§€ë„í•™ìŠµ ë°ì´í„°ëŠ” biasì™€ noisy labelê°€ ì¡´ì¬í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradient directionì— ë”°ë¼ í›ˆë ¨ ë°ì´í„°ì— ê°€ì¤‘ì¹˜ë¥¼ í• ë‹¹ í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•˜ë©° ì´ê²ƒì´ biasì™€ noisy labelì„ ì¡ì„ ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Biasì™€ noisy labelì´ ì—†ëŠ” ì†ŒëŸ‰ì˜ ê²€ì¦ ë°ì´í„°(validation set)ê°€ ìˆë‹¤ë©´ class imbalanceì™€ noisy label ë¬¸ì œë¥¼ ì™„í™” í•  ìˆ˜ ìˆìŒ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\11.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;meta-learning-objective&quot;&gt;Meta-Learning Objective&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\21.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;online-approximation&quot;&gt;Online Approximation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\35.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;learning-to-reweight-examples-in-a-mlp&quot;&gt;Learning to Reweight Examples in a MLP&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\41.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\42.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\43.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\44.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\45.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\46.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\47.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\48.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MNIST ë° CIFAR ë²¤ì¹˜ ë§ˆí¬ì—ì„œ ì„ì˜ë¡œ class imbalanceì™€ noisy labelì„ ì„¤ì •í•˜ê³  reweighting ì•Œê³ ë¦¬ì¦˜ì˜ íš¨ê³¼ë¥¼ í…ŒìŠ¤íŠ¸ í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MNIST data setting&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Class 4ì™€ 9ì— ëŒ€í•´ì„œ ì´ 5,000ì¥ì˜ ì´ë¯¸ì§€ë¥¼ ë‹¤ì–‘í•œ ë¹„ìœ¨ë¡œ class imbalanceê°€ ë°œìƒí•˜ë„ë¡ ìƒ˜í”Œë§ í•˜ì—¬ training setìœ¼ë¡œ ë‘ &lt;/li&gt;
      &lt;li&gt;Training setì—ì„œ 10ì¥ì˜ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœ class balanceí•œ validation setì„ ìƒì„±&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\49.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\50.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\51.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\52.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\53.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\54.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Training examplesì— reweightingí•˜ëŠ” online meta-learning ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Class imbalance, noisy label, class imbalance + noisy labelì¸ training examplesì— ì„±ëŠ¥ì´ ì¢‹ìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deep learning architectureì— ë°”ë¡œ ì ìš© ê°€ëŠ¥í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ëª¨ë“  training stepì— validationì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì°¸ì‹ í–ˆìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì†ŒëŸ‰ì´ì§€ë§Œ cleaní•œ validation setë§Œ ê°€ì§€ê³  class imbalanceì™€ noiseê°€ ë“¤ì–´ê°„ labelì„ ì–´ëŠì •ë„ í•´ê²° í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ í¥ë¯¸ë¡œì› ìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;í•™ìŠµ í•  ë•Œ validation setì„ ì‚¬ìš©í•´ lossë¥¼ ì£¼ëŠ” ë°©ì‹ì„ ìƒê°í•´ë³¼ ìˆ˜ ìˆì§€ë§Œ ì–´ë–»ê²Œ ì¤„ì§€ë¥¼ ìƒê°í•´ ë‚´ëŠ” ê²ƒì´ ì–´ë ¤ìš´ ê²ƒ ê°™ê³  ì–´ë–»ê²Œ ì¤„ì§€ë¥¼ ìƒê°í•´ë„ ì´ë ‡ê²Œ ìˆ˜í•™ì ìœ¼ë¡œ ì˜ë¯¸ ìˆê²Œ í‘¸ëŠ” ë°©ì‹ì´ ëŒ€ë‹¨í–ˆìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ìˆ˜ì‹ì´ ë§ì§€ë§Œ notationì´ ìì„¸íˆ ë˜ì–´ìˆì§€ ì•Šì•„ ì¶”ë¡ í•´ì•¼ í•˜ëŠ” ë¶€ë¶„ì´ ë§ì•˜ìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Convergence of the reweighted trainingì— ëŒ€í•œ ë‚´ìš©ì„ ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…í•˜ëŠ” ë¶€ë¶„ì´ ìˆëŠ”ë° ì´í•´í•˜ì§€ ëª»í•´ì„œ ë‹¤ìŒì— ì´í•´í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŒ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Wed, 15 Apr 2020 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Learning-to-Reweight-Examples-for-Robust-Deep-Learning/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Learning-to-Reweight-Examples-for-Robust-Deep-Learning/</guid>
        
        <category>Meta Learning</category>
        
        <category>Machine Learning</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Machine Learning</category>
        
      </item>
    
      <item>
        <title>Mutually exclusive, Correlation, Independenceê°„ì˜ ê´€ê³„</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Mutually exclusive, correlation, independenceê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•´ ê°œë…ì ìœ¼ë¡œ ì•Œì•„ë³´ê³  mutual exclusiveì™€ independenceê°„ì— ë¬´ìŠ¨ ê´€ê³„ê°€ ìˆëŠ”ì§€ ê·¸ë¦¬ê³  corre-lation ì™€ independenceê°„ì— ë¬´ìŠ¨ ê´€ê³„ê°€ ìˆëŠ”ì§€ ì•Œì•„ë³´ë„ë¡ í•œë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\Mutually exclusive, Correlation, Independence\Mutually exclusive, Correlation, Independence-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Mutually exclusive, Correlation, Independence\Mutually exclusive, Correlation, Independence-2.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Mutually exclusive, Correlation, Independenceê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•´ ê°œë…ì ìœ¼ë¡œ ì•Œì•„ë³´ì•˜ìœ¼ë©°, ì„œë¡œ ë¬´ìŠ¨ ê´€ê³„ê°€ ìˆëŠ”ì§€ì— ëŒ€í•´ ì•Œì•„ë³´ì•˜ë‹¤. Independenceì™€ mu-tually exclusiveëŠ” ì•„ë¬´ëŸ° ê´€ê³„ê°€ ì—†ë‹¤. Independenceì™€ CorrelationëŠ” ì„œë¡œ ê´€ê³„ê°€ ìˆë‹¤. ë§Œì•½ ë‘ random variableì´ independenceí•˜ë‹¤ë©´ ë‘ random variableì€ ì„œë¡œ uncorrelationì´ì§€ë§Œ, ì—­ì€ ì„±ë¦½í•˜ì§€ ì•ŠëŠ”ë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;cs109, lecture05&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 25 Mar 2020 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Mutually-exclusive,Correlation,Independence/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Mutually-exclusive,Correlation,Independence/</guid>
        
        <category>Probability</category>
        
        
        <category>Documentations</category>
        
        <category>Probability</category>
        
      </item>
    
      <item>
        <title>Zero One Few Shot Learning</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://web.stanford.edu/class/archive/cs/cs109/cs109.1196/schedule.html&quot;&gt;CS109&lt;/a&gt;, Lecture 21 MLEë¶€ë¶„ì„ ê³µë¶€í•˜ë©´ì„œ zero, one shot learningì— ëŒ€í•œ ì˜ˆì‹œê°€ ë‚˜ì™”ëŠ”ë° ì´í•´ê°€ ë¶€ì¡±í•˜ì—¬ ì •ë¦¬í•´ë³´ë ¤ í•œë‹¤. ë¨¼ì € zero-shot leaningì´ ë¬´ì—‡ì¸ì§€ë¶€í„° ì‹œì‘í•´ one-shot leaning, few-shot learningì´ ë¬´ì—‡ì¸ì§€ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;zero-shot-learning&quot;&gt;Zero Shot Learning&lt;/h3&gt;
&lt;p&gt;Zero-shot learningì´ë€ í•™ìŠµì— í•œë²ˆë„ ë³´ì§€ ëª»í–ˆë˜ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ê¸° ì›í•˜ëŠ” í•™ìŠµ ë°©ë²•ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìì „ê±°ë¼ëŠ” ë°ì´í„°ê°€ ë§ì•„ ìì „ê±° í´ë˜ìŠ¤ì— ëŒ€í•´ì„œ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµì„ í–ˆëŠ”ë° í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ ì˜¤í† ë°”ì´ê°€ ê°‘ìê¸° ë‚˜íƒ€ë‚¬ë‹¤ê³  í•˜ì. ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ëª¨ë¸ì´ ì˜¤í† ë°”ì´ë¼ëŠ” ìì „ê±°ì™€ ê°€ê¹Œìš´ í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì–´ ì£¼ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ê¸°ì¡´ ëª¨ë¸ë“¤ì€ í•œë²ˆë„ ëª» ë³¸ ì´ë¯¸ì§€ë¥¼ ìƒˆë¡œìš´ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•  ìˆ˜ ì—†ì—ˆë‹¤. ê·¸ë˜ì„œ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë§Œë“  ë°©ë²•ì´ zero-shot learningì´ë‹¤. ì•Œì•„ì„œ ìƒˆë¡œìš´ í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì–´ ë¶„ë¥˜í•´ì£¼ëŠ” ë°©ë²•ì´ë‹¤. ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•œ ë”¥ëŸ¬ë‹ì—ì„œ ë ˆì´ë¸”ë§ì´ ëœ ìˆ˜ë§ì€ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê¸° ì–´ë µê¸° ë•Œë¬¸ì— zero-shot learningì´ í•„ìš”í•˜ë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;one-shot-learning&quot;&gt;One Shot Learning&lt;/h3&gt;
&lt;p&gt;One-shot leaningì´ë€ ìƒˆë¡œìš´ í´ë˜ìŠ¤ì— ëŒ€í•´ ë°ì´í„° ìƒ˜í”Œì´ ë‹¨ í•œ ê°œë§Œ ì£¼ì–´ì§€ê³  ì´ë¥¼ êµ¬ë³„í•˜ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì—¬ëŸ¬ ê°œì˜ ìì „ê±°ê°€ ìˆë‹¤ê³  í•˜ì. ì‚¬ëŒì´ë¼ë©´ ìì „ê±°ì— ëŒ€í•´ ì•„ë¬´ëŸ° ì§€ì‹ì´ ì—†ë”ë¼ë„ ìì „ê±° ì´ë¯¸ì§€ í•œ ì¥ë§Œ ë³´ë©´ ìì „ê±°ë¼ëŠ” ê°œë…ì„ ë°”ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. ê·¸ ì´í›„ë¡œëŠ” ë‹¨ í•œë²ˆë„ ë³¸ì  ì—†ëŠ” íŠ¹ì´í•œ ëª¨ì–‘ì˜ ìì „ê±°ë¥¼ ë³¸ë‹¤ë©´ â€œìì „ê±°â€ë¼ëŠ” ì‚¬ì‹¤ì„ ë°”ë¡œ ì•Œì•„ë‚¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ì¸ ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ë“¤ì€ í•œ í´ë˜ìŠ¤ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ ìˆ˜ë°± ë˜ëŠ” ìˆ˜ì²œì¥ì´ ë„˜ëŠ” ì´ë¯¸ì§€ê°€ í•„ìš”í•˜ë‹¤. í•œ í´ë˜ìŠ¤ì— ëŒ€í•œ ì´ë¯¸ì§€ê°€ ìˆ˜ë°± ë˜ëŠ” ìˆ˜ì²œì¥ì„ ê°–ê³  ë ˆì´ë¸”ë§ì´ ëœ ìƒíƒœê°€ ì•„ë‹ˆë¼ë©´ ë”¥ëŸ¬ë‹ì— ì‚¬ìš©í•˜ê¸° ì–´ë µë‹¤. ê·¸ë ‡ê²Œ ë•Œë¬¸ì— ì ì€ ì´ë¯¸ì§€ë¡œë„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ ë§¤ìš° ì¤‘ìš”í•˜ë©° ì´ë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ëŠ” one-shot learningì´ë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;few-shot-learning&quot;&gt;Few Shot Learning&lt;/h3&gt;
&lt;p&gt;Few-shot learningì´ë€ ì•„ì£¼ ì ì€ ëª‡ ì¥ì˜ ì´ë¯¸ì§€ë¡œ ë°ì´í„°ì˜ íŠ¹ì§•ì„ ë½‘ì•„ ì‹ë³„í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. One-shot leaningì—ì„œëŠ” í•œ ì¥ì˜ ì´ë¯¸ì§€ë§Œ ë³´ì•˜ë‹¤ë©´ few-shot learningì€ ë§ì§€ëŠ” ì•Šì§€ë§Œ ì ì€ ì–‘ì˜ ì´ë¯¸ì§€ë§Œ ê°€ì§€ê³  ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ í•˜ëŠ” ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Zero, one, few shot learningëª¨ë‘ ë°ì´í„°ì˜ ìˆ˜ê°€ ì œí•œì ì´ê¸° ë•Œë¬¸ì— ë“±ì¥í•œ ë°©ë²•ë¡ ì´ë‹¤. ë§ì€ ë°ì´í„°ë¥¼ ì´ìš©í•´ ë³µì¡í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ì˜ ì´ëŒì–´ë‚´ëŠ” ì—°êµ¬ ë°©ë²•ê³¼ëŠ” ë°˜ëŒ€ë¡œ ì ì€ ë°ì´í„°ë¡œ ì´ë¯¸ì§€ë¥¼ ì˜ ë¶„ë¥˜í•˜ê³ ì ë°©ë²•ë¡ ë“¤ì´ë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;https://towardsdatascience.com/applications-of-zero-shot-learning-f65bb232963f&lt;/li&gt;
  &lt;li&gt;https://jayhey.github.io/&lt;/li&gt;
  &lt;li&gt;https://www.kakaobrain.com/blog&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 27 Feb 2020 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Zero,one,few-shot-learning/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Zero,one,few-shot-learning/</guid>
        
        <category>Probability</category>
        
        
        <category>Documentations</category>
        
        <category>Probability</category>
        
      </item>
    
      <item>
        <title>Paper Review. Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks@NIPS' 2015</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;r-cnn&quot;&gt;&lt;strong&gt;R-CNN&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;ìµœì´ˆë¡œ object detection taskì— CNNì„ ì‚¬ìš©í•˜ì—¬ ì •í™•ë„ì™€ ì†ë„ë¥¼ íšê¸°ì ìœ¼ë¡œ í–¥ìƒì‹œí‚´&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;r-cnnì˜-êµ¬ì¡°&quot;&gt;R-CNNì˜ êµ¬ì¡°&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\4.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;r-cnn-í•™ìŠµë°©ë²•&quot;&gt;R-CNN í•™ìŠµë°©ë²•&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\15.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;r-cnn-experiments&quot;&gt;R-CNN Experiments&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\16.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;r-cnn-conclusions--reviews&quot;&gt;R-CNN Conclusions &amp;amp; Reviews&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;í•™ìŠµì´ ì¼ì–´ë‚˜ëŠ” ë¶€ë¶„
    &lt;ul&gt;
      &lt;li&gt;ì´ë¯¸ì§€ ë„·ìœ¼ë¡œ ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì„ ê°€ì ¸ì™€ fine tuningí•˜ëŠ” ë¶€ë¶„&lt;/li&gt;
      &lt;li&gt;SVM classifierë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë¶€ë¶„&lt;/li&gt;
      &lt;li&gt;Bounding box regressioní•˜ëŠ” ë¶€ë¶„&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;í•™ìŠµì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼
    &lt;ul&gt;
      &lt;li&gt;Selective searchì—ì„œ ë½‘ì•„ë‚¸ 2000ê°œì˜ ì˜ì—­ ì´ë¯¸ì§€ë“¤ì— ëŒ€í•´ì„œ ëª¨ë‘ CNNëª¨ë¸ì„ í†µê³¼í•˜ë¯€ë¡œ ì˜¤ë˜ê±¸ë¦¼&lt;/li&gt;
      &lt;li&gt;Selective searchëŠ” CPUë¥¼ ì‚¬ìš©í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AlexNetì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ë¥¼ 224 x 224 í¬ê¸°ë¡œ ê°•ì œë¡œ warpingì‹œì¼°ê¸° ë•Œë¬¸ì— ì´ë¯¸ì§€ ë³€í˜•ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ì†ì‹¤&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CNN, SVM, Bounding box regression ì´ ì„¸ê°€ì§€ ëª¨ë¸ì„ í•„ìš”ë¡œ í•˜ëŠ” ë³µì¡í•œ êµ¬ì¡°&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-Stage Trainingì„ ìˆ˜í–‰, SVMì—ì„œ í•™ìŠµí•œ ê²°ê³¼ê°€ CNNì„ ì—…ë°ì´íŠ¸ ì‹œí‚¤ì§€ ëª»í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;R-CNNì€ ìµœì´ˆë¡œ object detectionì— deep learning ë°©ë²•ì¸ CNNì„ ì ìš©ì‹œí‚´&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;fast-r-cnn&quot;&gt;&lt;strong&gt;Fast R-CNN&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CNN fine tuning, bounding box regression, classificationì„ ëª¨ë‘ í•˜ë‚˜ì˜ ë„¤íŠ¸ì›Œí¬ì—ì„œ í•™ìŠµì‹œí‚¤ëŠ” end to end ê¸°ë²•&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;R-CNNì˜ ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ë…¼ë¬¸&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;í•µì‹¬ ì•„ì´ë””ì–´ëŠ” RoI pooling&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fast-r-cnnì˜-êµ¬ì¡°&quot;&gt;Fast R-CNNì˜ êµ¬ì¡°&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\22.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;fast-r-cnn-roi-pooling&quot;&gt;Fast R-CNN RoI Pooling&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\29.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;fast-r-cnn-multi-task-loss&quot;&gt;Fast R-CNN Multi Task Loss&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\31.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;fast-r-cnn-experiments&quot;&gt;Fast R-CNN Experiments&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\33.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;r-cnn-conclusions--reviews-1&quot;&gt;R-CNN Conclusions &amp;amp; Reviews&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Fast R-CNNì˜ trainingì€ end to endë¡œ single stage trainingì„ í•˜ì§€ë§Œ region proposalë¡œ selective searchë¥¼ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— ì „ì²´ì ìœ¼ë¡œ 2-stage detectorë¡œ ë´„&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fast R-CNNì€ R-CNNì´ë‚˜ SPP-netì— ë¹„í•˜ë©´ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë©° í•™ìŠµì‹œê°„ê³¼ í…ŒìŠ¤íŠ¸ ì‹œê°„ì´ ê°ì†Œë¨, í•˜ì§€ë§Œ region proposalì—ì„œ ê±¸ë¦¬ëŠ” ì‹œê°„ì´ ì´ 2.3ì´ˆ ì¤‘ ì•½ 2ì´ˆì˜ ì‹œê°„ì´ ê±¸ë¦¬ê¸° ë•Œë¬¸ì— Bottleneckì´ ìƒê¹€&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;faster-r-cnn&quot;&gt;&lt;strong&gt;Faster R-CNN&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Region proposal ë°©ë²•ì„ GPUë¥¼ í†µí•œ í•™ìŠµìœ¼ë¡œ ì§„í–‰í•˜ë©´ í™•ì‹¤íˆ ì„±ëŠ¥ì´ ì¦ê°€í•˜ê³  ì‹œê°„ì´ ê°ì†Œë  ê²ƒì´ë¼ê³  ë§í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Selective searchë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•´ì™”ë˜ region proposal ë‹¨ê³„ë¥¼ neural network ì•ˆìœ¼ë¡œ ê°€ì ¸ì˜´&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;í•µì‹¬ ì•„ì´ë””ì–´ëŠ” region proposal network(PRN)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;faster-r-cnn-êµ¬ì¡°&quot;&gt;Faster R-CNN êµ¬ì¡°&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\35.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\37.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;faster-r-cnn-region-proposal-network&quot;&gt;Faster R-CNN Region Proposal Network&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\41.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\42.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\43.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\44.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\45.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\46.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\47.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\48.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\49.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;faster-r-cnn-vs-fast-r-cnn&quot;&gt;Faster R-CNN vs Fast R-CNN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\50.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-stage-êµ¬ì¡°-vs-1-stage-êµ¬ì¡°&quot;&gt;2-Stage êµ¬ì¡° vs 1-Stage êµ¬ì¡°&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\51.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;faster-r-cnn-experiments&quot;&gt;Faster R-CNN Experiments&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\52.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\53.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;faster-r-cnn-conclusions--reviews&quot;&gt;Faster R-CNN Conclusions &amp;amp; Reviews&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CNNëª¨ë¸ì„ í†µí•´ Region proposalì„ ë½‘ì•„ ë¹ ë¥¸ ì†ë„ë¥¼ ëƒ„&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RPNì´ ìˆì–´ positiveí•œ anchorì™€ negative anchorì˜ ë¹„ìœ¨ì„ ë§ì¶° ì¤„ ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ì¢‹ìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RPNì—ì„œ sliding windowë¥¼ í†µí•´ anchor boxë¥¼ ë§Œë“¤ê³  ë‚œ ë’¤ì— positiveì™€ negativeë¥¼ íŒë³„ í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê²°êµ­ RPNë„ ë§ì€ anchor boxë¥¼ ë§Œë“¤ì–´ ì£¼ê³  GTì™€ IoU ê³„ì‚°ì„ ë‹¤ í•´ì£¼ì–´ì•¼ í•¨ (feature mapì´ í´ ê²½ìš°)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RPNì´ í•™ìŠµì´ ì˜ ë˜ë ¤ë©´ ì¢‹ì€ anchor boxë¥¼ ë§Œë“¤ì–´ ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°ë¨&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;roi-align&quot;&gt;&lt;strong&gt;RoI Align&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Mask-RCNN ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ë°©ë²•&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ê¸°ì¡´ RoI poolingì˜ ë¬¸ì œì ì„ ì§€ì í•˜ì—¬ ë‚˜ì˜¨ RoI align&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;roi-poolingì˜-ë¬¸ì œì &quot;&gt;RoI Poolingì˜ ë¬¸ì œì &lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Fast R-CNNì€ object detectionì„ ìœ„í•œ ëª¨ë¸ì´ì—ˆê¸° ë•Œë¬¸ì— RoI poolingì—ì„œ ì•„ì£¼ ì •í™•í•œ ìœ„ì¹˜ ì •ë³´ë¥¼ ë‹´ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ì§€ ì•Šì•˜ìŒ (Bounding boxì•ˆì— objectì´ ìˆìœ¼ë©´ ë¨)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\54.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\55.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\56.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\57.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\58.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;roi-align-1&quot;&gt;RoI Align&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;RoI alignì€ Mask R-CNNì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒˆìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;2-Stage object detectionì—ì„œ RoI poolingë³´ë‹¤ RoI alignì„ ì¼ì„ ë•Œ, ë” ìì„¸í•œ ë°•ìŠ¤ ì¢Œí‘œë¥¼ ì–»ê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì´ ë” ë†’ìŒ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\59.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\60.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\61.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;2-Stageê¸°ë°˜ì˜ ëª¨ë¸ì„ ìì„¸íˆ ì‚´í´ë³´ì•˜ìœ¼ë©°, RPNì´ ìˆìœ¼ë¯€ë¡œ í™•ì‹¤íˆ Objectì„ ì˜ ì°¾ì„ ìˆ˜ ìˆë‹¤ê³  ë´„&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì—¬ë¦„ì— ì—´ë¦¬ëŠ” ê·¸ëœë“œ ì±Œë¦°ì§€ 2020ì—ëŠ” 2-Stageê¸°ë°˜ì˜ ëª¨ë¸ì„ ì„ íƒí•´ êµ¬í˜„í•´ë³´ê³  ì‹¶ìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì•ìœ¼ë¡œ 2-Stage ê¸°ë°˜ì˜ ìµœì‹  Object Detection ë…¼ë¬¸ì„ ì°¾ì•„ë³¼ ê³„íšì„&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Girshick et al, â€œRich feature hierarchies for accurate object detection and semantic segmentation Tech report â€, CVPR, 2014&lt;/li&gt;
  &lt;li&gt;Uijlings et al, â€œSelective Search for Object Recognitionâ€, IJCV, 2012&lt;/li&gt;
  &lt;li&gt;Stanford CS231n, lecture12, p54&lt;/li&gt;
  &lt;li&gt;https://deepsense.ai/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 18 Feb 2020 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks/</guid>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Coin Flip Independent</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://web.stanford.edu/class/archive/cs/cs109/cs109.1196/schedule.html&quot;&gt;CS109&lt;/a&gt;, Lecture14, 25pageì— ë‚˜ì˜¤ëŠ” coin filp ë¬¸ì œì— ëŒ€í•´ í•´ê²°í•˜ë ¤ê³  í•œë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\Coin flip independent\Coin flip independent-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Coin flip independent\Coin flip independent-2.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Random variable Xì™€ Yê°€ ìˆì„ ë•Œ ì„œë¡œ ë…ë¦½ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ì‚´í´ë³´ì•˜ë‹¤. ë…ë¦½ì´ê¸° ìœ„í•´ì„œëŠ” \(P(X=x,Y=y)=P(X=x)P(Y=y)\)ì‹ì„ ë§Œì¡±í•´ì•¼ í•˜ë©° ì´ ì‹ì„ ë§Œì¡±í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, Xì™€ YëŠ” ì„œë¡œ ë…ë¦½ì´ ì•„ë‹ˆê²Œ ëœë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Mon, 10 Feb 2020 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Coin-Flip-Independent/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Coin-Flip-Independent/</guid>
        
        <category>Probability</category>
        
        
        <category>Documentations</category>
        
        <category>Probability</category>
        
      </item>
    
      <item>
        <title>Paper Review. EfficientDet - Scalable and Efficient Object Detection@CVPR' 2020</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/html/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Feature Pyramid Networ(FPN)ì˜ êµ¬ì¡° Weighted bi-directional FPN(BiFPN)ì„ ì œì•ˆí•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;EfficientNetì—ì„œ ì œì•ˆí•œ Compound Scalingê¸°ë²•ì„ Object Detectionì— ì ìš©í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One-stage Detection&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proposed-methods&quot;&gt;&lt;strong&gt;Proposed Methods&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;bifpn&quot;&gt;BiFPN&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;One-Stage Detectorì˜ ì—¬ëŸ¬ ëŒ€í‘œ ëª¨ë¸ì¸ SSD, RetinaNet, M2Detì˜ FPNë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ input featureë“¤ì„ í•©ì¹  ë•Œ ë‹¨ìˆœíˆ resizeí›„ ë”í•´ì£¼ëŠ” ë°©ì‹ì„ ì§€ì í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì„œë¡œ ë‹¤ë¥¸ input featureë“¤ì€ í•´ìƒë„ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— output featureì— ê¸°ì—¬í•˜ëŠ” ì •ë„ë¥¼ ë‹¤ë¥´ê²Œ ê°€ì ¸ì•¼í•¨ì„ ì£¼ì¥í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ê°„ë‹¨í•˜ì§€ë§Œ íš¨ê³¼ì ì¸ weighted bi-directional FPN êµ¬ì¡°ë¥¼ ì œì•ˆí•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BiFPNêµ¬ì¡°ëŠ” ì„œë¡œ ë‹¤ë¥¸ input featureë“¤ì˜ ì¤‘ìš”ì„±ì„ í•™ìŠµì„ í†µí•´ ë°°ìš¸ ìˆ˜ ìˆê³ , ì„±ëŠ¥ì„ ë§ì´ í–¥ìƒ ì‹œí‚¬ ìˆ˜ ìˆìŒ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\EfficientDet\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\4.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;efficientdet&quot;&gt;EfficientDet&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\EfficientDet\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\9.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;focal-loss&quot;&gt;Focal Loss&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cross Entropy Lossë¥¼ ì¡°ê¸ˆ ìˆ˜ì •í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì˜ ë¶„ë¥˜ë˜ëŠ” ì´ë¯¸ì§€ë“¤ì— ëŒ€í•´ì„œëŠ” ì‘ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ë°˜ë©´ ë¶„ë¥˜í•˜ê¸° ì–´ë ¤ìš´ ì¼ë¶€ ì´ë¯¸ì§€ì—ëŠ” í° ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì‰½ê²Œ ë¶„ë¥˜ë˜ëŠ” ëŒ€ë¶€ë¶„ì˜ negative ìƒ˜í”Œë“¤ì— ì˜í•´ì„œ í•™ìŠµì´ ì••ë„ ë˜ëŠ” ë¬¸ì œë¥¼ í•´ê²° í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One-Stage ë°©ì‹ì—ì„œë„ Two-Stage ë§Œí¼ì˜ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆìŒ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\EfficientDet\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\11.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\EfficientDet\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\16.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;EfficientNetì„ í™œìš©í•œ Object Detectionì´ì˜€ìœ¼ë©°, ì•ìœ¼ë¡œ Segmentationì—ë„ EfficientNetì„ í™œìš©í•œ ë…¼ë¬¸ì´ ë‚˜ì˜¬ ê²ƒ ê°™ìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Wed, 08 Jan 2020 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/EfficientDet-Scalable-and-Efficient-Object-Detection/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/EfficientDet-Scalable-and-Efficient-Object-Detection/</guid>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Arbitrary-Oriented Scene Text Detection via Rotation proposals@IEEE Transactions on Multimedia' 2018</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8323240?casa_token=nbnxyMRMpUgAAAAA:zvlxwwUb0TsqpOtKJb5Fw4p-Synz9pT3foz9RiWCFkKTtt6GMoZplHw_Vvwqwdawvih9boY5984&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ì´ë¯¸ì§€ì—ì„œì˜ í…ìŠ¤íŠ¸ ê°ì§€ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ íšŒì „ ê¸°ë°˜ í”„ë ˆì„ ì›Œí¬ë¥¼ ì†Œê°œ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;í…ìŠ¤íŠ¸ ë°©í–¥ ê°ë„ ì •ë³´ê°€ í¬í•¨ëœ Rotation Region Proposal Networks (RRPN)ë¥¼ ì œì•ˆí•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rotation Region-of-Interest (RRoI) pooling layerì„ ì œì•ˆí•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Faster-RCNNê³¼ ê°™ì€ êµ¬ì¡°ë¡œ region proposal-based architectureì´ë‹¤&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\1.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-methods&quot;&gt;&lt;strong&gt;Proposed Methods&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;horizontal-region-proposal&quot;&gt;Horizontal Region Proposal&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ê° ìŠ¬ë¼ì´ë”© ìœ„ì¹˜ì— kê°œì˜ anchorê°€ ì¡´ì¬.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;k anchorì— ëŒ€í•´ ì¢Œí‘œë¥¼ ë‚˜íƒ€ë‚´ëŠ” box regression(reg) layer
    &lt;ul&gt;
      &lt;li&gt;4k outputs (x, y, w, h)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;k anchorì— ëŒ€í•´ ì ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” box-classification (cls) layer
    &lt;ul&gt;
      &lt;li&gt;2k scores (object, non object)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;anchorë¥¼ ì •í•˜ê¸° ìœ„í•œ parameterë¡œëŠ” Scale, Ratioë¥¼ ì‚¬ìš©í•¨.
    &lt;ul&gt;
      &lt;li&gt;Ex) Scale : 1x, 2x, 4x, 1:2, Ratio : 1:1, 2:1&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ê¸°ì¡´ ìˆ˜í‰ anchor boxì„ íƒ ì „ëµì€ ì´ anchorì˜ ìˆ˜ë¥¼ ë‚®ê²Œ ìœ ì§€í•  ìˆ˜ ìˆìŒ.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;í•˜ì§€ë§Œ ì‹¤ì œ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ëŠ” ê²½ìš° ë°•ìŠ¤ê°€ ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ëª¨ì–‘ì„.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;íšŒì „ëœ ë°•ìŠ¤ë¥¼ ì°¾ëŠ” RPNì„ ì œì•ˆí•¨.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\2.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;rotated-bounding-box-representation&quot;&gt;Rotated Bounding Box Representation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\3.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;anchor-strategy&quot;&gt;Anchor Strategy&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\4.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;learning-of-rotated-proposal&quot;&gt;Learning of Rotated Proposal&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\9.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;skew-iou-computation&quot;&gt;Skew IoU Computation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\13.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;rroi-pooling-layer&quot;&gt;RRoI Pooling Layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\24.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;DataSet : MSRA-TD500, ICDAR2015, ICDAR2013&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implementation Details&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Learning rate : 0.001 for first 200,000 iterations, 0.0001 for the next 100,000 iterations&lt;/li&gt;
      &lt;li&gt;Weight decay : 0.0005&lt;/li&gt;
      &lt;li&gt;Momentum : 0.9&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\32.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ì „ì²´ì ì¸ êµ¬ì¡°ëŠ” Faster-RCNNê³¼ ë§¤ìš° ë˜‘ê°™ì•„ì„œ ì´í•´í•˜ê¸°ê°€ ì‰¬ì› ìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RPNì„ ë§Œë“¤ ë•Œ ê°ë„ ê°’ ğœƒë§Œ ì¶”ê°€í•´ ì£¼ë©´ ë˜ëŠ” ê°„ë‹¨í•œ ë¬¸ì œë¼ê³  ìƒê° í•  ìˆ˜ ìˆì§€ë§Œ, RoI Poolingì„ í•  ë•Œ íšŒì „ëœ ë°•ìŠ¤ì•ˆì— í”½ì…€ ê°’ì„ ë°›ì•„ì˜¤ëŠ” ì‘ì—…ì´ ì‰½ì§€ ì•Šë‹¤ê³  ëŠê»´ì§&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Tue, 26 Nov 2019 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Arbitrary-Oriented-Scene-Text-Detection-via-Rotation-proposals/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Arbitrary-Oriented-Scene-Text-Detection-via-Rotation-proposals/</guid>
        
        <category>Object Detection</category>
        
        <category>Oriented Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network@CVPR' 2017</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;super-resolutionì´ë€&quot;&gt;&lt;strong&gt;Super Resolutionì´ë€?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\6.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ë”¥ëŸ¬ë‹-srì˜-ì‹œì‘--srcnn&quot;&gt;&lt;strong&gt;ë”¥ëŸ¬ë‹ SRì˜ ì‹œì‘ â€“ SRCNN&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\14.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ë”¥ëŸ¬ë‹-srì˜-ë°œì „--vdsr&quot;&gt;&lt;strong&gt;ë”¥ëŸ¬ë‹ SRì˜ ë°œì „ â€“ VDSR&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\21.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ë”¥ëŸ¬ë‹-srì˜-ë°œì „--srgan&quot;&gt;&lt;strong&gt;ë”¥ëŸ¬ë‹ SRì˜ ë°œì „ â€“ SRGAN&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\35.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\40.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Object detectionê°™ì€ ê²½ìš° input image dataë¥¼ Super resolutionìœ¼ë¡œ í™”ì§ˆ ê°œì„ í•˜ì—¬ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì´ ì˜¬ë¼ê°ˆ ê²ƒì´ë‹¤. í•˜ì§€ë§Œ imageì˜ í¬ê¸°ê°€ 2ë°°or 4ë°° ê°€ëŸ‰ ì»¤ì§€ê¸° ë•Œë¬¸ì— memoryë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ëª…í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í•˜ë‚˜ì˜ í‰ê°€ ì§€í‘œê°€ ì—†ì–´ PSNR, SSIM, MOSì™€ ê°™ì´ ì—¬ëŸ¬ ì§€í‘œë¥¼ ë¹„êµí•´ê°€ë©° ì„±ëŠ¥ì„ í™•ì¸í•´ì•¼ í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Methodë¥¼ ì„ íƒí•  ë•Œ, PSNR, SSIM, MOS ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒì„ ì¤‘ì ìœ¼ë¡œ ë³´ê³  ì„ íƒí•˜ê³  ì‚¬ìš©í•´ì•¼ í• ì§€ ê³ ë¥´ê¸°ê°€ í˜ë“¤ ê²ƒ ê°™ìŒ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;https://hoya012.github.io/&lt;/li&gt;
  &lt;li&gt;https://www.slideshare.net/NaverEngineering/deep-learning-super-resolution&lt;/li&gt;
  &lt;li&gt;https://leedakyeong.tistory.com/&lt;/li&gt;
  &lt;li&gt;Image Super-Resolution Using Deep Convolutional Networks, 2014 ECCV&lt;/li&gt;
  &lt;li&gt;Accurate Image Super-Resolution Using Very Deep Convolutional Networks, 2016 CVPR&lt;/li&gt;
  &lt;li&gt;Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, 2016 arXiv&lt;/li&gt;
  &lt;li&gt;Deep Learning for Single Image Super-Resolution: A Brief Review, 2018 arXiv&lt;/li&gt;
  &lt;li&gt;A Deep Journey into Super-resolution: A Survey, 2019 arXiv&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 22 Oct 2019 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Photo-Realistic-Single-Image-Super-Resolution-Using-a-Generative-Adversarial-Network/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Photo-Realistic-Single-Image-Super-Resolution-Using-a-Generative-Adversarial-Network/</guid>
        
        <category>Super Resolution</category>
        
        <category>GAN</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Super Resolution</category>
        
      </item>
    
      <item>
        <title>Second Order Talyor Expansion</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\second-order Taylor expansion\second-order Taylor expansion-1.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\second-order Taylor expansion\second-order Taylor expansion-2.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\second-order Taylor expansion\second-order Taylor expansion-3.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\second-order Taylor expansion\second-order Taylor expansion-4.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\second-order Taylor expansion\second-order Taylor expansion-5.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;http://cs231n.stanford.edu/ lecture 8&lt;/li&gt;
  &lt;li&gt;https://ratsgo.github.io/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 20 Oct 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Second-Order-Taylor-Expansion/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Second-Order-Taylor-Expansion/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>Negative Sampling</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Negative samplingì€ Word2vecì˜ trainingë°©ë²•ì¤‘ í•˜ë‚˜ì´ë‹¤. Negative samplingì´ ë¬´ì—‡ì¸ì§€ ì‚´í´ë³´ê¸° ì „ì— ë¨¼ì € Word2vecë€ ë¬´ì—‡ì¸ì§€ ê°„ë‹¨í•˜ê²Œ ì•Œì•„ë³´ê³ , Word2vecì˜ ë‘ê°€ì§€ ë°©ì‹ì¸ CBOW, skip-gramì— ëŒ€í•´ì„œ ê°„ë‹¨íˆ ì•Œì•„ë³´ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ ë‹¨ì–´ê°€ ë²¡í„°ë¡œ ì˜ ë°”ë€” ìˆ˜ ìˆë„ë¡ trainingì‹œí‚¤ëŠ” ë°©ë²•ì¤‘ í•˜ë‚˜ì¸ Negative samplingì— ëŒ€í•´ì„œ ìì„¸íˆ ë‹¤ë£¨ê² ë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\negative sampling\negative sampling-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\negative sampling\negative sampling-2.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\negative sampling\negative sampling-3.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclousion&quot;&gt;&lt;strong&gt;Conclousion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Word2vecë¥¼ training í•  ë•Œ, negative samplingì„ í•œë‹¤ë©´ ì„±ëŠ¥ì´ í–¥ìƒë˜ê³  ê³„ì‚°ëŸ‰ì´ ë‚®ì•„ì§€ë¯€ë¡œ ì¢‹ì€ training ê¸°ë²•ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Y. Goldberg, et al., word2vec Explained: Deriving Mikolov et al.â€™s Negative-Sampling Word-Embedding Method. https://brunch.co.kr/&lt;/li&gt;
  &lt;li&gt;https://ratsgo.github.io/&lt;/li&gt;
  &lt;li&gt;http://solarisailab.com/&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 10 Oct 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Negative-Sampling/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Negative-Sampling/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>GRU Backpropagation</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;LSTMì—ì„œëŠ” ì¶œë ¥, ì…ë ¥, ì‚­ì œ ê²Œì´íŠ¸ë¼ëŠ” 3ê°œì˜ ê²Œì´íŠ¸ê°€ ì¡´ì¬í–ˆì—ˆë‹¤. ë°˜ë©´, GRUì—ì„œëŠ” ì—…ë°ì´íŠ¸ ê²Œì´íŠ¸ì™€ ë¦¬ì…‹ ê²Œì´íŠ¸ ë‘ ê°€ì§€ ê²Œì´íŠ¸ë§Œì´ ì¡´ì¬í•œë‹¤. GRUëŠ” LSTMë³´ë‹¤ ê²Œì´íŠ¸ ìˆ˜ê°€ í•˜ë‚˜ ì ì–´ í•™ìŠµ ì†ë„ë©´ì—ì„œëŠ” ë¹ ë¥´ì§€ë§Œ ì„±ëŠ¥ë©´ì—ì„œëŠ” LSTMì´ GRUë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ë‹¤ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤. ì´ ë¬¸ì„œì—ì„œëŠ” GRUì˜ ê²Œì´íŠ¸ë¥¼ ì‚´í´ë³´ê³  GRUì˜ weigthë“¤ì´ ì–´ë–»ê²Œ Updateë˜ëŠ”ì§€ Backpropagationì„ í†µí•´ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\GRU Backpropagation\GRU Backpropagation-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\GRU Backpropagation\GRU Backpropagation-2.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\GRU Backpropagation\GRU Backpropagation-3.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclousion&quot;&gt;&lt;strong&gt;Conclousion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;GRUì—ì„œ ê° ê²Œì´íŠ¸ì˜ íŒŒë¼ë¯¸í„° ë“¤ì´ ì–´ë–»ê²Œ updateë˜ëŠ”ì§€ ì‚´í´ë³´ì•˜ë‹¤. LSTMì— ë¹„í•´ ê²Œì´íŠ¸ ìˆ˜ê°€ í•˜ë‚˜ ì ì€ ë§Œí¼ ì„±ëŠ¥ì´ ë¹„êµì  ë–¨ì–´ì§€ëŠ” ê²ƒ ê°™ê³ , ì¼ë°˜ì ì¸ ê²½ìš°ì—ëŠ” LSTMì„ ì‚¬ìš©í•œë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;https://medium.com/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 01 Oct 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/GRU-Backpropagation/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/GRU-Backpropagation/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>Glove(Global Vectors for Word Representation)</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;GloVe(Global Vectors for Word Representation)ëŠ” count basedì™€ direct predictë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ 2014ë…„ì— ë¯¸êµ­ ìŠ¤íƒ í¬ë“œëŒ€í•™ì—ì„œ ê°œë°œí•œ ë‹¨ì–´ ì„ë² ë”© ë°©ë²•ë¡ ì´ë‹¤. Count basedì˜ LSA(Latent Semantic Analysis)ì™€ direct predictì˜ Word2Vecì— ë‹¨ì ì„ ì§€ì í•˜ë©° ì´ë¥¼ ë³´ì™„í•œë‹¤ëŠ” ëª©ì ìœ¼ë¡œ ë‚˜ì™”ë‹¤. ì‹¤ì œë¡œë„ GloVeëŠ” Word2Vecë§Œí¼ì´ë‚˜ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. ë‹¨ì •ì ìœ¼ë¡œ Word2Vecì™€ GloVe ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒì´ ë” ë›°ì–´ë‚˜ë‹¤ê³  ë§í•  ìˆ˜ëŠ” ì—†ê³ , ì´ ë‘ ê°€ì§€ ì „ë¶€ë¥¼ ì‚¬ìš©í•´ë³´ê³  ì„±ëŠ¥ì´ ë” ì¢‹ì€ ê²ƒì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•˜ë‹¤ê³  í•œë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\Glove\Glove-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Glove\Glove-2.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Glove\Glove-3.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Glove\Glove-4.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Glove\Glove-5.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclousion&quot;&gt;&lt;strong&gt;Conclousion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;GloVeëŠ” ì„ë² ë”© ëœ center wordì™€ context word ë²¡í„°ì˜ ë‚´ì ì´ ì „ì²´ corpusì—ì„œì˜ co-occurrence probabilityê°€ ë˜ë„ë¡ ë§Œë“  ê²ƒ ì¦‰, count basedì˜ ì¥ì ê³¼ direct predic-tionì˜ ì¥ì ì„ ê°€ì ¸ì™€ì„œ ê¸°ì¡´ ë°©ë²•ë¡ ì˜ ë‹¨ì ì„ ë³´ì™„í•œ ê²ƒì´ ì„±ëŠ¥í–¥ìƒì— ë„ì›€ì„ ì£¼ì—ˆë‹¤ê³  ìƒê°í•œë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;https://wikidocs.net/book/2155&lt;/li&gt;
  &lt;li&gt;https://ratsgo.github.io/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 27 Sep 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Glove/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Glove/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>Affinity Propagation Algorithm</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;ë¹„ì§€ë„ í•™ìŠµì˜ ê³¼ì—…ì—ëŠ” êµ°ì§‘í™”, ë°€ë„ ì¶”ì •, ê³µê°„ë³€í™˜ ì´ë ‡ê²Œ í¬ê²Œ ì„¸ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. ì´ì¤‘ êµ°ì§‘í™”ì˜ í•œ ì¢…ë¥˜ì¸, Affinity Propagation Algorithmì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë ¤ê³  í•œë‹¤. Affinity Propagation AlgorithmëŠ” ëª¨ë“  ë°ì´í„°ê°€ íŠ¹ì •í•œ ê¸°ì¤€ì— ë”°ë¼ ìì‹ ì„ ëŒ€í‘œí•  ëŒ€í‘œ ë°ì´í„°ë¥¼ ì„ íƒí•œë‹¤. ë§Œì•½ ìŠ¤ìŠ¤ë¡œê°€ ìê¸° ìì‹ ì„ ëŒ€í‘œí•˜ê²Œ ë˜ë©´ í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬ì´ ëœë‹¤. êµ°ì§‘ì˜ ê°œìˆ˜ë¥¼ kë¼ê³  í–ˆì„ ë•Œ, K-means ì•Œê³ ë¦¬ì¦˜ê³¼ëŠ” ë‹¤ë¥´ê²Œ ì´ê²ƒì„ hyperparameterë¡œì¨ ì„ íƒí•´ ì£¼ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ìë™ìœ¼ë¡œ ì•Œì•„ë‚¸ë‹¤. Responsibility, Availabilityë¼ëŠ” ë‘ ì¢…ë¥˜ì˜ ì¹œë°€ë„ í–‰ë ¬ì„ ì´ìš©í•˜ì—¬ êµ°ì§‘í™” í•œë‹¤. Responsibility, Availabilityí–‰ë ¬ì´ ë¬´ì—‡ì¸ì§€ ìì„¸íˆ ì•Œì•„ë³´ê³  ì•Œê³ ë¦¬ì¦˜ì´ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ì‚´í´ë³´ë ¤ê³  í•œë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\Affinity Propagation Algorithm\Affinity Propagation Algorithm-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Affinity Propagation Algorithm\Affinity Propagation Algorithm-2.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Affinity Propagation Algorithm\Affinity Propagation Algorithm-3.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Affinity Propagation Algorithm\Affinity Propagation Algorithm-4.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclousion&quot;&gt;&lt;strong&gt;Conclousion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;K-meansì˜ ì•Œê³ ë¦¬ì¦˜ì€ êµ°ì§‘ì˜ ê°œìˆ˜ì™€, ì´ˆê¸° êµ°ì§‘ëŒ€í‘œì˜ ê°’ì„ ì •í•´ì£¼ì–´ì•¼ í•˜ë©° ì´ˆê¸° êµ°ì§‘ì˜ ê°’ì„ ì–´ë–»ê²Œ ì„¤ì •í•´ì£¼ëŠ”ì§€ì— ëŒ€í•´ ê²°ê³¼ê°€ ë‹¬ë¼ì§„ë‹¤. í•˜ì§€ë§Œ Affinity Propagation Algorithmì€ êµ°ì§‘ì˜ ê°œìˆ˜ì™€, ì´ˆê¸° êµ°ì§‘ëŒ€í‘œì˜ ê°’ì„ ì •í•´ì£¼ì§€ ì•Šì•„ë„ ë˜ë©°, ìê°€ ìœ ì‚¬ë„ \(s(k, k)\)ì˜ ê°’ì„ ì–´ë–»ê²Œ ì„¤ì •í•´ì£¼ëŠ”ì§€ì— ë”°ë¼ êµ°ì§‘ì˜ ê°œìˆ˜ê°€ ë‹¬ë¼ì§„ë‹¤ëŠ” ì°¨ì´ì ì´ ìˆë‹¤. ë¹„ì§€ë„ í•™ìŠµì€ í¬ê²Œ êµ°ì§‘í™”, ë°€ë„ ì¶”ì •, ê³µê°„ ë³€í™˜ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ”ë° ì´ ì¤‘ êµ°ì§‘í™”ì— í•œ ì¢…ë¥˜ì¸ Affinity Propagation Algorithmì— ëŒ€í•´ ì‚´í´ ë³´ì•˜ë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;https://www.geeksforgeeks.org/&lt;/li&gt;
  &lt;li&gt;ì˜¤ì¼ì„, ê¸°ê³„í•™ìŠµ, í•œë¹›ì•„ì¹´ë°ë¯¸, p323~325&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 20 Sep 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Affinity-Propagation-Algorithm/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Affinity-Propagation-Algorithm/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>CS231 Lecture4 142p Backpropagation</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;CS231n Lecture4 142í˜ì´ì§€ì— ë‚˜ì˜¤ëŠ” computational graphì—ì„œ \(W_1, W_2\)ì— ëŒ€í•˜ì—¬ backporpagationì„ ì¦ëª…í•œë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-2.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-3.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-4.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-5.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-6.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-7.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-8.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;http://cs231n.stanford.edu/ lecture 4&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 15 Sep 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Backpropagation/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Backpropagation/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>Softmax &amp; Cross Entropy</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Softmax í•¨ìˆ˜ê°€ ë¬´ì—‡ì¸ì§€ ì•Œê¸° ìœ„í•´ ë¨¼ì € ìˆœì°¨ì ìœ¼ë¡œ ì •ë³´ì´ë¡ , Shannon entropy, Kullback-Leibler divergence, ê·¸ë¦¬ê³  Cross entropyì— ëŒ€í•´ì„œ ì‚´í´ë³¸ë‹¤. 
ë§ˆì§€ë§‰ìœ¼ë¡œ Softmax í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ Backpropagationë˜ì–´ weigthì´ update ë˜ëŠ”ì§€ ì‚´í´ ë³´ë„ë¡ í•œë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\Softmax, Cross entropy\Softmax, Cross entropy-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Softmax, Cross entropy\Softmax, Cross entropy-2.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Softmax, Cross entropy\Softmax, Cross entropy-3.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Softmax, Cross entropy\Softmax, Cross entropy-4.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Softmax, Cross entropy\Softmax, Cross entropy-5.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Softmax, Cross entropy\Softmax, Cross entropy-6.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Softmax í•¨ìˆ˜ê°€ ë¬´ì—‡ì¸ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì•Œê²Œ ë˜ì—ˆìœ¼ë©°, ì‹¤ì œ Scoreì™€ Softmaxë¥¼ í†µí•´ ë‚˜ì˜¨ í™•ë¥  ê°’ì„ ì–´ë–»ê²Œ ë¹„êµí•´ì•¼ í•˜ëŠ”ì§€ë„ ì•Œê²Œ ë˜ì—ˆë‹¤. Lossë¥¼ softmax í•¨ìˆ˜ xì— ëŒ€í•´ backpropagationì„ í•´ë´„ìœ¼ë¡œì¨ xê°€ ë³€í™”í•  ë•Œì˜ Lossì˜ ë³€í™”ëŸ‰ì„ ì•Œê²Œ ë˜ì—ˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;https://ratsgo.github.io/&lt;/li&gt;
  &lt;li&gt;https://brunch.co.kr/&lt;/li&gt;
  &lt;li&gt;https://selfish-developer.com/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 29 Aug 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Softmax-Cross-entropy/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Softmax-Cross-entropy/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>Activation Function Backpropagation</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Activation Functionë“¤ì´  Backpropagationê°€ ì§„í–‰ë˜ëŠ” ê³¼ì •ì„ Deriveí•˜ëŠ” ìë£Œì´ë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-01.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-02.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-03.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-03.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-04.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-05.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-06.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-07.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-08.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-09.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-10.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-11.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-12.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-13.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Fei-Fei Li, Justin Johnson, Serena Yeung / cs231n-2019-lecture04.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 22 Aug 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Activation-Function-Backpropagation/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Activation-Function-Backpropagation/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>Paper Review. Fully Convolutional Networks for Semantic Segmentation@CVPR' 2015</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8323240?casa_token=nbnxyMRMpUgAAAAA:zvlxwwUb0TsqpOtKJb5Fw4p-Synz9pT3foz9RiWCFkKTtt6GMoZplHw_Vvwqwdawvih9boY5984&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œÂ Segmentationì˜ ì‹œì´ˆë¼ê³  í• Â ìˆ˜ ìˆëŠ” ë…¼ë¬¸&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì´ ë…¼ë¬¸ì€Â fully convolutionì„ ì´ìš©í•˜ì—¬ ì–´ë–»ê²ŒÂ semantic segmentationì„Â deep learningìœ¼ë¡œ í’€ì—ˆëŠ”ì§€ ë³´ì—¬ì¤Œ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;semantic-segmentationì´ë€&quot;&gt;Semantic Segmentationì´ë€?&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\2.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-methods&quot;&gt;&lt;strong&gt;Proposed Methods&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;fully-convolutional-networks&quot;&gt;Fully Convolutional Networks&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\4.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;deconvolution&quot;&gt;Deconvolution&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\8.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;skip-layer&quot;&gt;Skip layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\9.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\10.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\14.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Fully connected layerë¥¼ Fully convolutional layerë¡œ ë°”ê¾¸ê³  ì´ì „ layerì˜ ì •ë³´ë¥¼ ì´ìš©í•´ upsamplingí•˜ëŠ” ë°©ë²•ì´ ì¸ìƒê¹Šì—ˆìŒ.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Activation mapì˜ í¬ê¸°ë¥¼ ë‚®ì·„ë‹¤ê°€ ë†’ì´ëŠ” ë°©ì‹ì´ë¯€ë¡œ ì¤‘ê°„ì— ì •ë³´ ì†ì‹¤ì´ ë°œìƒí•´ ìœ¤ê³½ì„ ì´ ëšœë ·í•˜ê²Œ ë‚˜ì˜¤ì§€ ì•ŠìŒ -&amp;gt; Dilated Convolution : í•„í„° ë‚´ë¶€ì— zero paddingì„ ì¶”ê°€í•´ ê°•ì œë¡œ receptive fieldë¥¼ ëŠ˜ë¦¬ëŠ” ë°©ë²•&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Thu, 25 Jul 2019 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Fully-Convolutional-Networks-for-Semantic-Segmentation/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Fully-Convolutional-Networks-for-Semantic-Segmentation/</guid>
        
        <category>Segmentation</category>
        
        <category>Semantic Segmenation</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Segmentation</category>
        
      </item>
    
  </channel>
</rss>