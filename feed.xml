<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CV Researcher</title>
    <description>Study Room</description>
    <link>https://joochann.github.io/</link>
    <atom:link href="https://joochann.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 17 Feb 2022 17:16:46 +0900</pubDate>
    <lastBuildDate>Thu, 17 Feb 2022 17:16:46 +0900</lastBuildDate>
    <generator>Jekyll v4.2.1</generator>
    
      <item>
        <title>Paper Review. Unsupervised Monocular Depth Estimation with Left-Right Consistency@CVPR' 20217</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2017/html/Godard_Unsupervised_Monocular_Depth_CVPR_2017_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;논문-선정-배경&quot;&gt;논문 선정 배경&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\5.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;depth-estimation-배경-지식&quot;&gt;Depth Estimation 배경 지식&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\24.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;introduction-1&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\25.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;depth-estimation-as-image-reconstruction&quot;&gt;Depth Estimation as Image Reconstruction&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\35.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\36.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;depth-estimation-network&quot;&gt;Depth Estimation Network&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\41.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\42.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\43.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\44.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\45.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\46.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\47.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\48.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\49.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\50.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\51.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\52.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\53.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\54.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\55.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\56.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-loss&quot;&gt;Training Loss&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\57.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\58.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\59.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\60.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\61.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation Details&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\62.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\63.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\64.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;experiments-1&quot;&gt;Experiments&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\65.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\66.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\67.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\68.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\69.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Unsupervised Monocular Depth Estimation with Left-Right Consistency\70.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Single image로부터 depth를 측정할 수 있음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unsupervised learning이기 때문에 고비용의 depth 이미지가 없어도 됨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Supervised learning 방법론들보다 성능이 높으며 KITTI 데이터셋에서 SOTA를 달성함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;최근에도 depth estimation관련하여 계속해서 연구가 되고 있음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;기본으로 알아 두어야할 지식이 방대함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Depth estimation 분야가 어떻게 이루어지는지 대략적으로 알게 됨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;최대한 자세히 살펴보려고 했으나 원론적인 부분 중 아직 이해하지 못하고 궁금한 부분이 다수 있었으며, 추후 공부할 예정&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;앞으로 알아 두어야 할 몇가지 논문을 더 보고 3D object detection 공부를 시작하고 싶음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Arnold, Eduardo, et al. “A survey on 3d object detection methods for autonomous driving applications.” IEEE Transactions on Intelligent Transportation Systems., 2019&lt;/li&gt;
  &lt;li&gt;Learning Depth-Guided Convolutions for Monocular 3D Object Detection@CVPR’ 2020&lt;/li&gt;
  &lt;li&gt;https://github.com/OniroAI/MonoDepth-PyTorch&lt;/li&gt;
  &lt;li&gt;http://vision.middlebury.edu/stereo/&lt;/li&gt;
  &lt;li&gt;https://blog.naver.com/dldlsrb45/&lt;/li&gt;
  &lt;li&gt;https://eehoeskrap.tistory.com/103&lt;/li&gt;
  &lt;li&gt;https://www.youtube.com/watch?v=jI1Qf7zMeIs&amp;amp;ab_channel=ComputerVisionFoundationVideos&lt;/li&gt;
  &lt;li&gt;Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. “Spatial transformer networks@NIPS’ 2015&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Wed, 11 Aug 2021 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Unsupervised-Monocular-Depth-Estimation-with-Left-Right-Consistency/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Unsupervised-Monocular-Depth-Estimation-with-Left-Right-Consistency/</guid>
        
        <category>Depth Estimation</category>
        
        <category>Disparity</category>
        
        <category>3D</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Depth Estimation</category>
        
      </item>
    
      <item>
        <title>Paper Review. Scale-aware Automatic Augmentation for Object Detection@CVPR' 2021</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/html/Chen_Scale-Aware_Automatic_Augmentation_for_Object_Detection_CVPR_2021_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Object detection task에서 최적의 augmentation policy를 찾는 &lt;strong&gt;scale-aware AutoAug&lt;/strong&gt;방법을 제안함&lt;/li&gt;
  &lt;li&gt;Scale 불변성을 유지하기 위해 image-level과 box-level에서 augmentation을 진행한 &lt;strong&gt;scale-aware search space&lt;/strong&gt;를 정의함&lt;/li&gt;
  &lt;li&gt;Object detection task에서 주로 사용하는 AutoAugment-det, RandAugment 보다 우수함&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Data Augmentation
    &lt;ul&gt;
      &lt;li&gt;Color operations : Brightness, contrast, and whitening …&lt;/li&gt;
      &lt;li&gt;Geometric operations : Re-scaling, and flip …&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;autoaug-det1&quot;&gt;AutoAug-det[1]&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Object detection의 data augmentation policy&lt;/li&gt;
  &lt;li&gt;Main idea : Classification에 사용되는 데이터셋보다 detection을 위한 데이터셋의 수가 더 적기 때문에, &lt;strong&gt;bounding box&lt;/strong&gt; 값 변화를 함께 주어야함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img1.PNG&quot; width=&quot;300&quot; /&gt;
&lt;em&gt;Fig1.Bounding box augmentation 예시&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;박스의 context를 보고 color or geometric augmentation을 수행하는 방법으로 총 세가지 operation이 있다고 보면 됨
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;Color operations : 이미지의 color 값을 변환하되, bounding box의 위치 좌표 값에는 변화를 주지 않음 
  ex) Equalize, contrast, brightness …&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Geometric operations : 이미지의 위치정보를 바꾸며, bounding box의 위치나 사이즈를 같이 변화함 
  ex) Rotate, shearX, tanslationY …&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Bounding box operations : 이미지 내에서 bounding box가 있는 부분의 픽셀만 변화시킴 
  ex) Bbox_Only_Equalize, Bbox_Only_Rotate …&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training 과정에서만 data augmentation을 수행하며, 하나의 이미지에 대해 순차적으로 여러 개의 augmentation이 적용되는데 어떤 순서로 사용할지 search method를 통해 최적화 시킴&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Search space의 계산&lt;/p&gt;

\[(22𝐿𝑀)^{𝑁𝐾}=(22×6×6)^{2×5}≈9.6×{10}^{28}\]
  &lt;/li&gt;
  &lt;li&gt;각각 N개의 순차적인 transformation operands를 갖는 K개의 sub-policy를 학습하고, training 과정에서 각 이미지에 적용될 policy가 랜덤으로 선택됨&lt;/li&gt;
  &lt;li&gt;M은 transformation이 적용될 확률, L은 transformation이 적용될 크기(magnitude)&lt;/li&gt;
  &lt;li&gt;문제점1 : object detection의 필수적인 image와 box level의 크기 문제를 고려하지 않았음&lt;/li&gt;
  &lt;li&gt;문제점2 : 연산량이 매우 큼 (400 TPU for 2days)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;-본 논문에서는 객체 탐지를 위한 자동적으로 학습하는 scale-aware data augmentation 전략을 제안함-&lt;/p&gt;

&lt;h3 id=&quot;review-of-autoaug&quot;&gt;Review of AutoAug&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Auto augmentation
    &lt;ul&gt;
      &lt;li&gt;세가지의 메인 구성 요소로 이루어짐
        &lt;ol&gt;
          &lt;li&gt;Search space : 얼마만큼의 augmentation 종류가 존재하는지&lt;/li&gt;
          &lt;li&gt;Search algorithm : 어떻게 좋은 augmentation을 선택할 것인지&lt;/li&gt;
          &lt;li&gt;Estimation metric : 어떻게 좋은 augmentation을 평가할 것인지&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;scale-aware-search-space&quot;&gt;Scale-Aware Search Space&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Motivation
    &lt;ul&gt;
      &lt;li&gt;저자들은 오브젝트 박스 이외의 pixel을 지우고 학습하여 성능 차이를 비교해 봄&lt;/li&gt;
      &lt;li&gt;그 결과 small object의 경우 주변 pixel이 중요함을 알 수 있으며, 이는 학습 모델이 모든 크기의 객체를 적절히 처리하지 못할 수 있음&lt;/li&gt;
      &lt;li&gt;Bounding box의 scale에 따라 augmentation을 다르게 해주는 area ratio를 제안함&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img2.PNG&quot; width=&quot;800&quot; /&gt;
  &lt;em&gt;Fig2. Motivation&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Image-level augmentations
    &lt;ul&gt;
      &lt;li&gt;Probabilities 𝑃
        &lt;ul&gt;
          &lt;li&gt;\(𝑃_{𝑜𝑢𝑡}\)이 선택될 확률 : \(×0 ~ ×0.5\)&lt;/li&gt;
          &lt;li&gt;\(𝑃_{𝑖𝑛}\)이 선택될 확률 : \(×0 ~ ×0.5\)&lt;/li&gt;
          &lt;li&gt;\(𝑃_{𝑜𝑟𝑖}\)이 선택될 확률 : \(1−𝑃_{𝑖𝑛}−𝑃_{𝑜𝑢𝑡}\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Magnitudes 𝑀.
        &lt;ul&gt;
          &lt;li&gt;\(𝑃_{𝑜𝑢𝑡}\)의 크기 : \(×1.0 ~ ×1.5\)&lt;/li&gt;
          &lt;li&gt;\(𝑃_{𝑖𝑛}\)의 크기 : \(×0.5 ~ ×1.0\)&lt;/li&gt;
          &lt;li&gt;\(𝑃_{𝑜𝑟𝑖}\)의 크기 : \(×1.0\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Zoom-in operation
        &lt;ul&gt;
          &lt;li&gt;probability range : 6 discrete values \([0, 0.1, 0.2, 0.3, 0.4, 0.5]\)&lt;/li&gt;
          &lt;li&gt;magnitude range : 6 discrete values \([0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Zoom-out operation
        &lt;ul&gt;
          &lt;li&gt;probability range : 6 discrete values \([0, 0.1, 0.2, 0.3, 0.4, 0.5]\)&lt;/li&gt;
          &lt;li&gt;magnitude range : 6 discrete values \([1.0, 1.1, 1.2, 1.3, 1.4, 1.5]\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Box-level augmentations
    &lt;ul&gt;
      &lt;li&gt;8 color operations와 6 geometric operations 존재
        &lt;ul&gt;
          &lt;li&gt;probability range : 6 discrete values \([0, 0.2, 0.4, 0.6, 0.8, 1.0]\)&lt;/li&gt;
          &lt;li&gt;magnitude range : 6 discrete values \([0, 2, 4, 6, 8, 10]\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Area ratio (scale ratio)
        &lt;ul&gt;
          &lt;li&gt;area ratio type : small, middle, large&lt;/li&gt;
          &lt;li&gt;area ratio range : 10 discrete values \([0.2, 0.4, 0.6, 0.8, 1.0, 2, 4, 6, 8, 10]\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img3.PNG&quot; width=&quot;800&quot; /&gt;
  &lt;em&gt;Fig3. Image-level, and box-level augmentations&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Box-level augmentations의 효과
    &lt;ol&gt;
      &lt;li&gt;Gaussian을 통한 부드러운 augmentation&lt;/li&gt;
    &lt;/ol&gt;

\[𝐴=𝛼(𝑥,𝑦)∙𝐼+(1−𝛼(𝑥,𝑦))∙𝑇\]

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img4.PNG&quot; width=&quot;800&quot; /&gt;
  &lt;em&gt;Fig4. An example of Gaussian-based box-level augmentation&lt;/em&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Area ratio라는 학습 가능한 파라미터
        &lt;ul&gt;
          &lt;li&gt;Area ratio \(r\)에 따라 표준편차를 다르게 적용하여 \(r\)이 크면 \(𝑟(𝑠_{𝑏𝑜𝑥})\)가 더 넓은 범위로 augmentation이 자연스럽게 적용됨&lt;/li&gt;
          &lt;li&gt;반대로 \(r\)이 작으면 \(𝑟(𝑠_{𝑏𝑜𝑥})\)가 더 작아져 부분적으로 augmentation이 적용됨&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;

\[𝛼(𝑥,𝑦)=exp⁡(−((𝑥−𝑥_𝑐)^2/(2𝜎_𝑥^2))+((𝑦−𝑦_𝑐)^2/(2𝜎_𝑦^2)))\]

\[𝑉=∫_0^𝐻∫_0^𝑊𝛼(𝑥,𝑦)d𝑥d𝑦\]

\[𝜎_𝑥=ℎ\sqrt{(𝑊/𝐻)/2𝜋}∙𝑟,  𝜎_𝑦=𝑤\sqrt{(𝐻/𝑊)/2𝜋}∙𝑟\]

\[𝑟(𝑠_{𝑏𝑜𝑥})=𝑉/𝑠_{𝑏𝑜𝑥}\]
  &lt;/li&gt;
  &lt;li&gt;Search space summary
    &lt;ul&gt;
      &lt;li&gt;Image-level augmentation : \((6^2)^2\)
        &lt;ul&gt;
          &lt;li&gt;Zoom-in : \(6×6\)&lt;/li&gt;
          &lt;li&gt;Zoom-out : \(6×6\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Box-level augmentation : \(({10}^3)×((8×6×6)×(6^3))^5\)
        &lt;ul&gt;
          &lt;li&gt;Area ratio : \(10×10×10\)&lt;/li&gt;
          &lt;li&gt;Color operation : \(8×6×6\)&lt;/li&gt;
          &lt;li&gt;Geometric operation : \(6×6×6\)&lt;/li&gt;
          &lt;li&gt;Box-level operation은 총 5개의 sub-policy로 구성, 1개의 sub-policy는 2개의 augmentation으로 구성 (color, geometric operation)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Total search space : \(({(6^2)}^2×{10}^3)×{((8×6×6)×(6^3))}^5=(1.2)^{30}\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;scale-aware-estimation-metric&quot;&gt;Scale-Aware Estimation Metric&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;“Balanced optimization over different scales”이 중요하다고 생각하여 search 과정 중에 different scale별로 accumulated loss와 accuracy를 이용함&lt;/p&gt;

\[min_𝑝⁡𝑓(\{𝐿_{(𝑖∈𝑆)}^𝑝\}, \{(AP)_{(𝑖∈𝑆)}^𝑝\})\]

    &lt;ul&gt;
      &lt;li&gt;\(\{(AP)_{(𝑖∈𝑆)}^𝑝\}\) : Data augmentation policy \(𝑝\)로 학습한 plain model의 각 scale \(𝑖\)별로 validation accuracy \((𝐴𝑃)_𝑖\)&lt;/li&gt;
      &lt;li&gt;\(\{𝐿_{(𝑖∈𝑆)}^𝑝\}\) : Data augmentation policy \(𝑝\)로 학습한 plain model의 각 scale \(𝑖\)별로 Accumulated loss \(𝐿_𝑖\)&lt;/li&gt;
      &lt;li&gt;다른 scale별로 balanced optimization은 필수적이며, 간단하게 다양한 scale별로 loss의 standard deviation을 측정하면 되지만, 이는 sub-optimal에 빠지게 됨&lt;/li&gt;
      &lt;li&gt;그래서 Pareto Optimality[2]를 수행함&lt;/li&gt;
      &lt;li&gt;Pareto Optimality[2]이란[3]
        &lt;ul&gt;
          &lt;li&gt;자원 배분이 가장 효율적으로 이루어진 상태를 pareto optimal이라고 함
  Pareto는 손해를 보는 사람은 하나도 없고 이익을 보는 사람만 있는 경우를 pareto 개선이라고 함&lt;/li&gt;
          &lt;li&gt;Pareto 개선이 불가능할 정도로 개선된 상태를 pareto optimal이라고 함
  즉, 모든 사람이 타인의 불만을 사는 일 없이는 자기 만족을 더 이상 증가시킬 수 없는 상태가 ‘파레토 최적’ 임&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Objective function
    &lt;ul&gt;
      &lt;li&gt;Pareto optimality에 의해 “The optimization over scales can not be better without hurting the accuracy of any other scale” 라는 개념을 가져옴&lt;/li&gt;
    &lt;/ul&gt;

\[min_𝑝⁡𝑓(\{𝐿_{(𝑖∈𝑆)}^𝑝\}, \{(AP)_{(𝑖∈𝑆)}^𝑝\}) = 𝜎(\{𝐿_{(𝑖∈𝑆)}^𝑝\})∙Φ(\{(AP)_{(𝑖∈\hat 𝑆)}^𝑝\})\]

\[Φ(\{(AP)_{(𝑖∈\hat 𝑆)}^𝑝\})=∏_{(𝑖∈\hat 𝑆)}{({𝐴𝑃}_𝑖)}/{({𝐴𝑃}_𝑖^𝑝)}\]

    &lt;ul&gt;
      &lt;li&gt;Policy \(𝑝\)을 통하여 fine-tuning 했을 때, 성능 저하가 발생하는 scale \(𝑆\) ̂에 처벌을 주는 penalization factor \(Φ\)를 제안함&lt;/li&gt;
      &lt;li&gt;\({({𝐴𝑃}_𝑖)}/{({𝐴𝑃}_𝑖^𝑝)}\) : scale-wise ratio of original and the fine-tuned accuracy&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Autoaugment 방법은 일반적으로 proxy task(훈련 이미지가 있는 작은 하위 집합)에 대한 validation accuracy를 search metric으로 사용함. 실험 결과, proxy task에서 accuracy로 구한 policy와 scale-aware metric을 이용하여 구한 policy를 실제 dataset에서 성능을 측정하였을 때, coefficent가 더 높음&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img5.PNG&quot; width=&quot;400&quot; /&gt;
  &lt;em&gt;Fig5. Coefficients between actual accuracy and metrics&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;search-algorithm&quot;&gt;Search algorithm&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Sample dataset에서 scale-aware estimation metric이 제일 최소가 되도록 fine-tuning하여 최적의 augmentation policy를 찾음&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Scratch부터 학습하면 시간이 오래 걸리기에, data augmentation없이 학습한 plain model을 augmentation policy로 fine-tuning하여 시간을 줄임&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img6.PNG&quot; width=&quot;400&quot; /&gt;
  &lt;em&gt;Fig6. Search algorithm&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Detector별 성능 지표&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img7.PNG&quot; width=&quot;400&quot; /&gt;
  &lt;em&gt;Fig7. Comparison with object detection augmentation strategies on MS COCO dataset&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Improvement details on RetinaNet ResNet-50&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img8.PNG&quot; width=&quot;400&quot; /&gt;
  &lt;em&gt;Fig8. Improvement details&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comparison with AutoAug-det on RetinaNet ResNet-50&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img9.PNG&quot; width=&quot;400&quot; /&gt;
  &lt;em&gt;Fig9. Comparison with AutoAug-det&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Search on RetinaNet ResNet-50 with different metrics&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img10.PNG&quot; width=&quot;400&quot; /&gt;
  &lt;em&gt;Fig10. Different scale-aware estimation metrics&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Improvements across detection and segmentation frameworks&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img11.PNG&quot; width=&quot;800&quot; /&gt;
  &lt;em&gt;Fig11. Different frameworks&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comparison with SOTA data augmentation methods for object detection&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;\assets\papers\Scale aware Automatic Augmentation for Object Detection\img12.PNG&quot; width=&quot;800&quot; /&gt;
  &lt;em&gt;Fig12. Comparison with SOTA data augmentation methods&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Augmentation policy를 찾는데 20 GPU-days가 걸리며, 다른 RL을 사용한 auto augmentation방법보다 빠르지만, 여전히 시간이 오래 걸림&lt;/li&gt;
  &lt;li&gt;데이터가 적은 object detection task에 좋음&lt;/li&gt;
  &lt;li&gt;Box level에서 데이터 augmentation하는 것과 가우시안 분포를 활용하여 이미지를 자연스럽게 변환하는 기법이 효과적인 것 같음&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Zoph, Barret, et al. “Learning data augmentation strategies for object detection.” European Conference on Computer Vision. Springer, Cham, 2020.&lt;/li&gt;
  &lt;li&gt;John Black, Nigar Hashimzade, and Gareth Myles. A dictionary of economics. Oxford university press, 2012&lt;/li&gt;
  &lt;li&gt;http://www.aistudy.co.kr/economics/pareto_optimal.htm&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 14 Jul 2021 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Scale-aware-Automatic-Augmentation-for-Object-Detection/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Scale-aware-Automatic-Augmentation-for-Object-Detection/</guid>
        
        <category>Augmentation</category>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Augmentation</category>
        
      </item>
    
      <item>
        <title>Paper Review. Learning from Noisy Anchors for One-stage Object Detection@CVPR' 2020</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Learning_From_Noisy_Anchors_for_One-Stage_Object_Detection_CVPR_2020_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;문제점
    &lt;ul&gt;
      &lt;li&gt;IoU(Intersection over Union)을 통해 positive와 negative anchor를 나누게 되면 noise anchor가 발생해 학습이 잘 되지 않는 문제가 발생함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;해결책
    &lt;ul&gt;
      &lt;li&gt;Anchor의 좋고(positive) 나쁨(negative)을 2분할하여 표현하지 않고 동적으로 연속적인 값으로 표현함&lt;/li&gt;
      &lt;li&gt;지난번 포스팅했던 DAL[1] 방법론과 유사하게 classification branch와 regression branch를 사용하여 좋은 anchor를 분류하는 cleanliness score를 제안함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;추가적으로 발생하는 계산량은 거의 없으며, COCO데이터셋에서 성능을 2%이상 증가시킴&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;positivenegative-anchor-선택의-중요성-및-문제점&quot;&gt;Positive/Negative Anchor 선택의 중요성 및 문제점&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide8.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;제안하는-positivenegative-anchor-선택-방법&quot;&gt;제안하는 Positive/Negative Anchor 선택 방법&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;문제점
    &lt;ul&gt;
      &lt;li&gt;IoU(Intersection over Union)을 통한 positive/negative anchor 선택으로 학습이 잘되지 않아 classification과 regression간의 불일치 문제 발생&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;제안하는 해결 방법 1
    &lt;ul&gt;
      &lt;li&gt;Anchor를 positive/negative로 discrete하게 나누지 말고 continuous한 값으로 표현하자&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;제안하는 해결 방법 2
    &lt;ul&gt;
      &lt;li&gt;Classification branch와 regression branch의 결과값을 사용하여 좋은(positive) anchor를 선택해보자&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide15.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;soft-label&quot;&gt;Soft Label&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide20.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;sample-re-weighting&quot;&gt;Sample Re-Weighting&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;One-stage detector는 two-stage detector와 다르게 RPN이 존재하지 않기 때문에 negative proposal이 매우 많으며, positive proposal은 적음&lt;/li&gt;
  &lt;li&gt;이 문제를 완화하기 위해 focal loss가 존재하지만, 레이블이 노이즈가 있는 즉 anchor의 좋고 나쁨이 잘못 판별된 위치의 proposal은 학습하는데 방해가 됨&lt;/li&gt;
  &lt;li&gt;그러므로 cleanliness score를 활용한 re-weight sample방법을 제안함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide35.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;학습용 : COCO trainval135k set&lt;/li&gt;
      &lt;li&gt;테스트용 : COCO test-dev2017&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Detectors
    &lt;ul&gt;
      &lt;li&gt;주로 사용한 모델 : RetinaNet&lt;/li&gt;
      &lt;li&gt;벡본 : ResNet-50, ResNet-101, ResNeXt-101 32x8d&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Implementation details
    &lt;ul&gt;
      &lt;li&gt;GPUs : 4&lt;/li&gt;
      &lt;li&gt;Batch size : 8 (2 images per GPU)&lt;/li&gt;
      &lt;li&gt;Optimizer : SGD&lt;/li&gt;
      &lt;li&gt;Input image size : 800 x 800&lt;/li&gt;
      &lt;li&gt;Augmentation : horizontal flip&lt;/li&gt;
      &lt;li&gt;Multi-scale training 사용 {640, 672, 704, 736, 768, 800}&lt;/li&gt;
      &lt;li&gt;Multi-scale testing 사용 {400, 500, 600, 700, 900, 1000, 1100, 1200}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ablation-study&quot;&gt;Ablation Study&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide38.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;main-results&quot;&gt;Main Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide39.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;discussions&quot;&gt;Discussions&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide41.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning from Noisy Anchors for One-stage Object Detection\slide42.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;IoU(Intersection over Union)을 통해 positive와 negative anchor를 나누게 되면 noise anchor가 발생해 학습이 잘 되지 않기 때문에 classification branch와 regression branch를 사용하여 좋은 anchor를 분류하는 cleanliness score를 제안함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Anchor의 좋고(positive) 나쁨(negative)을 2분할하여 표현하지 않고 동적으로 연속적인 값 soft label로 표현하여 anchor의 표현을 다양하게(풍부하게) 해주는 방법이 재밌었음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Anchor를 생성하는 것도 중요하지만 학습하기에 좋은 anchor인지 안 좋은 anchor인지 판별하는 것 또한 매우 중요함&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Ming, Qi, et al. “Dynamic Anchor Learning for Arbitrary-Oriented Object Detection.” arXiv preprint arXiv:2012.04150 (2020).&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 02 Jun 2021 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Learning-from-Noisy-Anchors-for-One-stage-Object-Detection/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Learning-from-Noisy-Anchors-for-One-stage-Object-Detection/</guid>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Align Deep Features for Oriented Object Detection@IEEE Transactions on Geoscience and Remote Sensing' 2021</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9377550?casa_token=ftqghqE0R94AAAAA:IrBS8DpsKG3WrrOyChjwLlgU_T7Tfo2OnmNiBv1cpI9SrNL_1gs9GVZbIW82mawhZ7-FTWov0cc&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;기존 방법들은 scale, angle, aspect ratio를 사용하여 heuristic하게 정의된 anchor에 의존함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;회전된 Anchor box와 회전되지 않은(axis-aligned) convolution사이의 misalignment로 인해 classification과 localization 정확도 간에 불일치가 존재함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Single-shot alignment Network(S^2A-Net)은 두개의 모듈로 이루어져 있음.
    &lt;ul&gt;
      &lt;li&gt;Feature Alignment Module(FAM) : 고품질 anchor를 생성하고, alignment convolution을 통해 anchor 위치에 맞는 convolution을 수행함&lt;/li&gt;
      &lt;li&gt;Oriented Detection Module(ODM) : Active Rotating Filters(ARF)를 사용해 방향 정보를 인코딩하여 orientation-sensitive feature를 제공함으로써 classification과 localization 정확도 간의 불일치를 어느정도 해결함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DOTA 데이터 셋에서 SOTA를 달성함&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\5.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Anchor box와 object간의 misalignment를 해결하기 위해 논문에서는 Single-shot alignment Network(S^2A-Net)을 제안함.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Feature Alignment Module(FAM)
    &lt;ul&gt;
      &lt;li&gt;다른 방법론들과 다르게 하나의 horizontal anchor를 갖음.&lt;/li&gt;
      &lt;li&gt;Anchor Refinement Network (ARN)에서 고품질의 회전된 anchor를 생성함.&lt;/li&gt;
      &lt;li&gt;Alignment convolution을 통해 anchor 위치에 맞는 convolution을 수행함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Oriented Detection Module(ODM)
    &lt;ul&gt;
      &lt;li&gt;Active Rotating Filters(ARF)를 사용해 방향 정보를 인코딩하여 orientation-sensitive feature를 제공함으로써 classification과 localization 정확도 간의 불일치를 어느정도 해결함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;model-architecture---fpn&quot;&gt;Model Architecture - FPN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\15.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;model-architecture---fam&quot;&gt;Model Architecture - FAM&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\16.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;anchor-refinement-network&quot;&gt;Anchor Refinement Network&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\27.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;alignment-convolution-layer&quot;&gt;Alignment Convolution Layer&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\35.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\41.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\42.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\43.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\44.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\45.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\46.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\47.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\48.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\49.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\50.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\51.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\52.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\53.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\54.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\55.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\56.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\57.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\58.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\59.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\60.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\61.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;model-architecture---odm&quot;&gt;Model Architecture - ODM&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\62.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\63.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\64.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;active-rotating-filters&quot;&gt;Active Rotating Filters&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\65.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\66.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\67.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\68.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\69.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\70.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\71.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\72.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\73.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\74.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;model-architecture---summary&quot;&gt;Model Architecture - Summary&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\75.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\76.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\77.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\78.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;loss-function&quot;&gt;Loss Function&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\79.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\80.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;DOTA
        &lt;ul&gt;
          &lt;li&gt;크기 범위 : 800 x 800 ~ 4000 x 4000&lt;/li&gt;
          &lt;li&gt;이미지 수 : 2806&lt;/li&gt;
          &lt;li&gt;Augmentation : 좌,우,상,하 flip, multi scale training, multi scale test&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;HRSC2016
        &lt;ul&gt;
          &lt;li&gt;크기 범위 : 300 x 300 ~ 1500 x 900&lt;/li&gt;
          &lt;li&gt;이미지 수 : 617&lt;/li&gt;
          &lt;li&gt;Augmentation : 좌,우 flip&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\81.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\82.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\83.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\84.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\85.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\86.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\87.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\88.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\89.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Align Deep Features for Oriented Object Detection\90.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Feature Alignment Module과 Oriented Detection Module이 포함된 Single-Shot Alignment Network을 제안하며 속도와 정확도 모두 챙김&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Predict box 정보를 Anchor box에 넣어서 학습 가능한 refine anchor를 만드는 방법이 좋았음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deformable convolution을 쓸 때, offset field를 refine anchor를 기반으로 가져오기 때문에 효과가 있었음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;모델이 갈수록 어려워지는 것 같고, 논문에서는 아주 간략하게 설명하기 때문에 전반적인 기초 지식이 없으면 이해하기 힘듬&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;코드를 뜯어보며 살펴보니 모델 구조를 명확히 이해할 수 있었고, Loss부분도 자세하게 뜯어볼 계획임&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Zhou, Yanzhao, et al. “Oriented response networks.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 14 Apr 2021 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Align-Deep-Features-for-Oriented-Object-Detection/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Align-Deep-Features-for-Oriented-Object-Detection/</guid>
        
        <category>Object Detection</category>
        
        <category>Oriented Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Feature Selective Anchor-Free Module for Single-Shot Object Detection@CVPR' 2019</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Feature_Selective_Anchor-Free_Module_for_Single-Shot_Object_Detection_CVPR_2019_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Anchor 기반의 detecting 방식에는 두 가지의 한계점이 존재
    &lt;ol&gt;
      &lt;li&gt;heuristic-guide를 통한 feature selection&lt;/li&gt;
      &lt;li&gt;overlap-based anchor sampling&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;모델의 전체적인 구조는 FPN을 통과한 Feature map에 기존 방식인 anchor-based branch와 논문에서 제안하는 anchor-free branch을 통해 객체를 검출&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Anchor-free branch에서 online feature selection을 하는 FSAF(Feature Selection Anchor Free)방식을 제안함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;FSAF 방식은 가볍고 빠르며 플러그인 시키기 쉬움&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\img1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;em&gt;Fig1. 제안하는 방법 FSAF는 작은 오브젝트를 잘 찾을 수 있음&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\6.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\8.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ground-truth-and-loss&quot;&gt;Ground-truth and Loss&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\21.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;online-feature-selection&quot;&gt;Online Feature Selection&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\24.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Feature Selective Anchor-Free Module for Single-Shot Object Detection\29.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;가볍고 빠르며 어디에든 추가하기 쉬운 FSAF 모듈을 제안함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Anchor-based 방법에서는 작은 오브젝트는 low level의 feature pyramid map을 사용하고, 큰 오브젝트는 high level의 feature map을 사용하여 detection을 하며 이는 성능적으로 한계가 있음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;논문에서는 anchor-free branch를 추가함으로써 feature map을 동적으로 선택하므로 오브젝트를 더 잘 찾을 수 있었음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;작은 오브젝트는 low level의 feature pyramid map을 사용하고, 큰 오브젝트는 high level의 feature map을 사용하는 이유는 IoU를 통해 positive anchor를 선택하기 때문이며, 결국 IoU를 통해 앵커를 선택하는 방법은 좋지 않음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Wed, 31 Mar 2021 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Feature-Selective-Anchor-Free-Module-for-Single-Shot-Object-Detection/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Feature-Selective-Anchor-Free-Module-for-Single-Shot-Object-Detection/</guid>
        
        <category>Object Detection</category>
        
        <category>Anchor Free</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Multiple Anchor Learning for Visual Object Detection@CVPR' 2020</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/html/Ke_Multiple_Anchor_Learning_for_Visual_Object_Detection_CVPR_2020_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;고정된 앵커 집합에 대하여 classification과 regression은 공동으로 최적화하지 못함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Anchor bag를 구성하여 객체별 대표적인 anchor를 선택하는 방식인 MAL(Multiple Anchor Learning) 방법을 제안함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;반복적인 anchor 선택 프로세스를 최적화하기 위해 anchor의 confidence를 낮춤으로 교란(perturbing)하여 가장 좋은 anchor를 선택하도록 함&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\10.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;retinanet-revisit&quot;&gt;RetinaNet Revisit&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\13.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;multiple-anchor-learning&quot;&gt;Multiple Anchor Learning&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\16.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;selection-depression-optimization&quot;&gt;Selection-Depression Optimization&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\21.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\25.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;optimization-analysis&quot;&gt;Optimization Analysis&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\26.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Multiple Anchor Learning for Visual Object Detection\31.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Best anchor를 선택하면서 Classification confidence와 localization을 같이 optimize하기 때문에 객체와 anchor를 매칭하는 것을 학습할 수 있음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;근본적으로는 IoU를 통해 anchor를 선택하는 것이 좋지 않기 때문에 이러한 방법들이 계속해서 연구되고 있음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Wed, 24 Feb 2021 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Multiple-Anchor-Learning-for-Visual-Object-Detection/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Multiple-Anchor-Learning-for-Visual-Object-Detection/</guid>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Dynamic Anchor Learning for Arbitrary-Oriented Object Detection@AAAI' 2021</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ojs.aaai.org/index.php/AAAI/article/view/16336&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;기존 모델들은 IoU를 적용하여 positive 와 negative 앵커를 샘플링을 함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Positive 앵커는 항상 정확한 탐지를 보장할 수 없지만, 일부 negative 앵커는 정확한 위치를 파악할 수 있음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이는 IoU를 통한 앵커의 품질 평가가 적절하지 않음을 나타내며, 이로 인해 classification confidence와 localization accuracy 사이에 불일치가 발생함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이 논문에서는 새롭게 정의된 matching degree를 활용하여 앵커의 localization의 가능성을 종합적으로 평가하는 dynamic anchor learning(DAL) 방법을 제안함&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\3.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이러한 문제를 해결하기 위해 DAL(Dynamic Anchor Learning) 방법을 제안함
    &lt;ol&gt;
      &lt;li&gt;앵커의 localization 가능성을 평가하기 위해 간단하면서도 효과적인 MD(Matching Degree)를 설계&lt;/li&gt;
      &lt;li&gt;Sample selection을 학습하기 위해 matching degree를 사용하여 false-positive 샘플을 제거하고 잠재적인 고품질 후보를 동적으로 선택함&lt;/li&gt;
      &lt;li&gt;matching-sensitive loss function을 통해 classification과 regression간의 불일치를 완화함&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;dynamic-anchor-selection&quot;&gt;Dynamic Anchor Selection&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\7.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;matching-sensitive-loss&quot;&gt;Matching-Sensitive Loss&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\11.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;Aerial datasets
        &lt;ul&gt;
          &lt;li&gt;HRSC2016&lt;/li&gt;
          &lt;li&gt;DOTA&lt;/li&gt;
          &lt;li&gt;UCAS-AOD&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Scene text datasets
        &lt;ul&gt;
          &lt;li&gt;ICDAR 2015&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Dynamic Anchor Learning for Arbitrary-Oriented Object Detection\19.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;고성능의 회전된 객체 탐지를 위해 Dynamic anchor learning 방법을 제안함.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Classification confidence와 localization accuracy 사이의 불일치를 어느정도 해결함 (양의 상관관계)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;여러 개의 앵커를 만드는 것이 중요한 것이 아니라 positive, negative 앵커를 어떻게 구분(평가)하느냐가 중요함&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Wed, 20 Jan 2021 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Dynamic-Anchor-Learning-for-Arbitrary-Oriented-Object-Detection/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Dynamic-Anchor-Learning-for-Arbitrary-Oriented-Object-Detection/</guid>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Transformer-XL Attentive Language Models Beyond a Fixed-Length Context@ACL' 2019</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.02860&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;기존의 transformer는 고정된 개수(512)의 token들을 갖는 한 개의 segment만을 input으로 사용하여, 연속된 segment들 간의 dependency를 반영하지 못함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;현재 segment를 처리할 때, 이전 segment를 처리할 때 계산된 hidden state들을 사용하는 recurrence를 추가하여 위 문제를 해결함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;모델에 적합한 positional encoding을 변형하였음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Transformer-XL은 language modeling에서 SOTA의 성능을 기록함&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;transformer&quot;&gt;Transformer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\7.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;vanilla-transformer-language-model&quot;&gt;Vanilla Transformer Language Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\9.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;segment-level-recurrence-with-state-reuse&quot;&gt;Segment-Level Recurrence with State Reuse&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\12.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;relative-position-encoding&quot;&gt;Relative Position Encoding&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\22.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;transformer-xl&quot;&gt;Transformer XL&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\23.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\31.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;RNN 계열 모델과 vanilla Transformer model보다 long-term dependency를 더 잘 잡아냈음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이전 segment를 저장해두고 사용함으로써 prediction시 상당한 속도 향상을 달성함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Recurrence 방법을 사용하기 위해 relative position encoding을 수식적으로 풀어서 의미부여를 하고, 적용한 것이 대단하다고 느낌&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Tue, 10 Nov 2020 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context/</guid>
        
        <category>Attention</category>
        
        <category>Machine Translation</category>
        
        <category>NLP</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Machine Translation</category>
        
      </item>
    
      <item>
        <title>Paper Review. Attention is all you need@NIPS' 2017</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;당시 가장 좋은 모델은 Attention 메커니즘을 통해 encoder와 decoder를 연결한 모델이 성능이 가장 좋음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;기존 모델들처럼 RNN또는 CNN을 사용하지 않고 attention 메커니즘만을 기반으로 하는 Transformer 모델을 제안함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;기계 번역 태스크 실험을 통해 모델이 병렬화 가능하며, 학습 시간이 단축되는 것을 보임&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\11.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\12.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;positional-encoding&quot;&gt;Positional Encoding&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\20.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;encoder&quot;&gt;Encoder&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\26.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;multi-head-attention&quot;&gt;Multi-Head Attention&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\35.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\41.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;position-wise-fc-layer&quot;&gt;Position-Wise FC Layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\42.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\43.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;decoder&quot;&gt;Decoder&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\44.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\45.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\46.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\47.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\48.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;why-self-attention&quot;&gt;Why Self-Attention&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\49.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;training&quot;&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\50.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\51.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\52.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\53.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Attention is all you need\54.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Attention is all you need\55.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Machine Translation 태스크에서 Transformer 모델은 빠르게 학습하며, 성능도 우수하다는 것을 보여줌&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Recurrent 모델을 사용하지 않고도 sequential 데이터를 처리할 수 있는 모델&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder와 decoder에서 attention을 통해 query와 가장 밀접한 연관성을 가지는 value를 강조 할 수 있음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;모델 병렬화가 가능해짐&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Attention을 다시 정리하면서 공부할 수 있었음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Tue, 29 Sep 2020 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Attention-is-all-you-need/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Attention-is-all-you-need/</guid>
        
        <category>Attention</category>
        
        <category>Machine Translation</category>
        
        <category>NLP</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Machine Translation</category>
        
      </item>
    
      <item>
        <title>Paper Review. Bidirectional LSTM-CRF Models for Sequence Tagging@arXiv' 2015</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1508.01991&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;LSTM 레이어와 CRF 레이어를 이용하여 LSTM-CRF 모델을 만듦&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BI-LSTM 레이어와 CRF 레이어를 이용하여 BI-LSTM-CRF 모델을 만듦&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BI-LSTM-CRF 모델을 이용하면 POS tagging태스크에서 높은 성능을 도달할 수 있음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;crf란&quot;&gt;CRF란&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\20.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;feature-function&quot;&gt;Feature Function&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\32.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;models&quot;&gt;Models&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\35.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\41.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\42.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;lstm-crf-model&quot;&gt;LSTM-CRF Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\43.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\44.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\45.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\46.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\47.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\48.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\49.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\50.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\51.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\52.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\53.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\54.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\55.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\56.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\57.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\58.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;bi-lstm-crf-model&quot;&gt;BI-LSTM-CRF Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\59.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\60.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\61.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\62.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\63.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\64.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Bidirectional LSTM-CRF Models for Sequence Tagging\65.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;POS tagging과 같이 규칙이 있는 알고리즘일 경우 CRF를 사용하는 것이 의미가 있음 (예. 부사 뒤에 부사가 올 수 없음)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CRF의 feature function을 정의하기 위해서는 모든 경우의 수를 고려해야함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Feature function이 많아질수록 연산량이 많아 속도가 느려질 텐데 보통 몇 개인지 궁금함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CRF를 사용하면 속도가 많이 느려질 텐데 속도에 대한 실험 결과가 없어 아쉬움&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Tue, 08 Sep 2020 14:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Bidirectional-LSTM-CRF-Models-for-Sequence-Tagging/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Bidirectional-LSTM-CRF-Models-for-Sequence-Tagging/</guid>
        
        <category>LSTM</category>
        
        <category>Tagging</category>
        
        <category>NLP</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Tagging</category>
        
      </item>
    
      <item>
        <title>Paper Review. SlowFast Networks for Video Recognition@ICCV' 2019</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2019/html/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;AVA Challenge 2019에서 Action recognition 1등&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Video Recognition을 위한 네트워크 구조&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;두개의 pathway가 존재
    &lt;ul&gt;
      &lt;li&gt;Slow pathway : spatial semantic 정보를 획득&lt;/li&gt;
      &lt;li&gt;Fast pathway : motion at fine temporal resolution 정보를 획득&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;사람의 시각 시스템을 모방하여 모델을 구축함&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\3.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;spatial-structure&quot;&gt;Spatial Structure&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\8.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;temporal-events&quot;&gt;Temporal Events&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\10.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;slowfast-networks&quot;&gt;SlowFast Networks&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\14.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;biological-derivation&quot;&gt;Biological Derivation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\19.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;slow-pathway&quot;&gt;Slow Pathway&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\22.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;fast-pathway&quot;&gt;Fast Pathway&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\29.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;overall-process&quot;&gt;Overall Process&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\35.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\41.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\42.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\43.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\44.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\45.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\46.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\47.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;Kinetics-400
        &lt;ul&gt;
          &lt;li&gt;306,245 video clips&lt;/li&gt;
          &lt;li&gt;400 human action classes&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Kinetics-600
        &lt;ul&gt;
          &lt;li&gt;495,547 video clips&lt;/li&gt;
          &lt;li&gt;600 human action classes&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Charades
        &lt;ul&gt;
          &lt;li&gt;9,848 video clips&lt;/li&gt;
          &lt;li&gt;157 human action classes&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;AVA dataset
        &lt;ul&gt;
          &lt;li&gt;430 video clips&lt;/li&gt;
          &lt;li&gt;80 human action classes&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;action-classification&quot;&gt;Action Classification&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\48.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\49.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\50.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\51.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\52.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ava-action-detection&quot;&gt;AVA Action Detection&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\53.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\54.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\SlowFast Networks for Video Recognition\55.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;사람의 인지 시스템을 모방해 모델구조를 설계하였기 때문에 왜 이렇게 모델 구조를 만들었는지 이해가 됨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;사람을 탐지하는 모델(Detectron)의 성능이 90프로가 넘으므로 모델을 그대로 사용하고, 사람의 액션을 예측하는데 초점을 둔 모델(Slowfast)을 만든 것 같음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AI Grand Challenge 첫번째 Task에서는 단순히 실신한 사람만 찾으면 되므로, 이 모델이 적합하지 않을 수 있음. 하지만, 나중에 복잡한 영상 인식 Task가 진행된다면 사용해 볼 수 있음 (클래스수가 많아지거나, 영상이 길어지는 경우)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Tue, 14 Jul 2020 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/SlowFast-Networks-for-Video-Recognition/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/SlowFast-Networks-for-Video-Recognition/</guid>
        
        <category>Activity Recognition</category>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Activity Recognition</category>
        
      </item>
    
      <item>
        <title>Paper Review. Gliding vertex on the horizontal bounding box for multi-oriented object@IEEE transactions on pattern analysis and machine intelligence' 2020</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9001201?casa_token=R_LrtCbzL4QAAAAA:3YPWwU6yo4ysQverP0MVY6M7ogWpzNIvCLQldEhaIJchm9r5DNjzHs3OiNYasEkLeYy4jqKn29s&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-oriented object를 detection하기 간단하지만 효과적인 프레임워크&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Oriented object의 네 꼭지점을 직접적으로 예측하는 것이 아닌, horizontal한 박스를 가지고 gliding offset을 예측하여 oriented object의 꼭지점을 예측함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Oriented object와 horizontal한 박스 사이의 면적 비율을 기반으로 obliquity factor를 측정하여 어떤 박스를 선택할지 결정함&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\2.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\8.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;multi-oriented-object-representation&quot;&gt;Multi-Oriented Object Representation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\10.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\11.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-objective&quot;&gt;Training Objective&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\16.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;DOTA
        &lt;ul&gt;
          &lt;li&gt;A large-scale and challenging dataset for object detection in aerial images with quadrangle annotations&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;HRSC2016
        &lt;ul&gt;
          &lt;li&gt;Ship detection in aerial images&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;MSRA-TD500
        &lt;ul&gt;
          &lt;li&gt;Detecting long and oriented texts&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;RCTW-17
        &lt;ul&gt;
          &lt;li&gt;Detecting long and oriented texts&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;MW-18Mar
        &lt;ul&gt;
          &lt;li&gt;Multi-target horizontal pedestrian tracking dataset&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Gliding vertex on the horizontal bounding box for multi-oriented object detection\25.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-oriented object를 detection하기 간단하지만 효과적인 프레임워크&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RPN에서 Anchor를 찾을 때 각도를 고려하지 않았기 때문에 다른 기존 2-stage 방법들보다 속도가 빠름&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;회전된 객체 탐지 다양한 데이터셋에서 당시 SOTA를 찍음으로써, 자신들이 제안하는 (회전된 객체를 바라보는 representation)방법이 회전된 객체를 찾는 일반적인 태스크에 효과적이라는 것을 보여줌&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;회전된 객체를 바라보는 일반적인 시선을 다른 시선으로 바라본 것이 새로웠음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Tue, 02 Jun 2020 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Gliding-vertex-on-the-horizontal-bounding-box-for-multi-oriented-object/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Gliding-vertex-on-the-horizontal-bounding-box-for-multi-oriented-object/</guid>
        
        <category>Object Detection</category>
        
        <category>Oriented Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Deformable Convolutional Networks@ICCV' 2017</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_iccv_2017/html/Dai_Deformable_Convolutional_Networks_ICCV_2017_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;기존 CNN에서 사용하는 conv, pooling이 기하학적으로 일정한 패턴을 가정하고 있기 때문에 복잡한 transformation에 유연하게 대처하기 어려움&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;기존에는 weight을 구하는 방법에 초첨을 맞췄다면, 이 논문은 어떤 데이터를 뽑을 것인지에 초점을 둠&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Convolution -&amp;gt; Convolution + learnable offset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RoI pooling -&amp;gt; RoI pooling + learnable offset&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\5.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;deformable-convolution&quot;&gt;Deformable Convolution&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\11.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;deformable-roi-pooling&quot;&gt;Deformable RoI Pooling&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\17.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;understanding-deformable-convnets&quot;&gt;Understanding Deformable ConvNets&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\20.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Deformable Convolutional Networks\24.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;데이터의 특징을 필터를 통해 찾는 것이 아니라 입력 데이터에서 offset을 통해 직접 찾으려 해서 참신함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deformable convolution과 deformable RoI pooling을 사용했을 때 의미 분석을 그림으로 보여주어 직관적으로 왜 좋아졌는지 느낄 수 있었음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;모든 물체가 정사각형으로 이루어져 있지 않기 때문에 정사각형의 필터를 가지고 convolution을 하는 것은 문제가 될 수 있으며, 이 문제를 해결하는 하나의 방향성을 제시해 준 것 같음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Wed, 29 Apr 2020 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Deformable-Convolutional-Networks/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Deformable-Convolutional-Networks/</guid>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Learning to Reweight Examples for Robust Deep Learning@ICML' 2018</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v80/ren18a.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;일반적인 지도학습 데이터는 bias와 noisy label가 존재함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gradient direction에 따라 훈련 데이터에 가중치를 할당 하는 방법을 제안하며 이것이 bias와 noisy label을 잡을 수 있다고 주장함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bias와 noisy label이 없는 소량의 검증 데이터(validation set)가 있다면 class imbalance와 noisy label 문제를 완화 할 수 있음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\11.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;&lt;strong&gt;Proposed Method&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;meta-learning-objective&quot;&gt;Meta-Learning Objective&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\21.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;online-approximation&quot;&gt;Online Approximation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\35.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;learning-to-reweight-examples-in-a-mlp&quot;&gt;Learning to Reweight Examples in a MLP&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\41.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\42.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\43.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\44.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\45.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\46.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\47.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\48.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MNIST 및 CIFAR 벤치 마크에서 임의로 class imbalance와 noisy label을 설정하고 reweighting 알고리즘의 효과를 테스트 함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MNIST data setting&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Class 4와 9에 대해서 총 5,000장의 이미지를 다양한 비율로 class imbalance가 발생하도록 샘플링 하여 training set으로 둠&lt;/li&gt;
      &lt;li&gt;Training set에서 10장의 이미지로 구성된 class balance한 validation set을 생성&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\49.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\50.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\51.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\52.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\53.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Learning to Reweight Examples for Robust Deep Learning\54.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Training examples에 reweighting하는 online meta-learning 알고리즘을 제안함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Class imbalance, noisy label, class imbalance + noisy label인 training examples에 성능이 좋음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deep learning architecture에 바로 적용 가능함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;모든 training step에 validation을 사용하는 것은 참신했음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;소량이지만 clean한 validation set만 가지고 class imbalance와 noise가 들어간 label을 어느정도 해결 할 수 있다는 점이 흥미로웠음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;학습 할 때 validation set을 사용해 loss를 주는 방식을 생각해볼 수 있지만 어떻게 줄지를 생각해 내는 것이 어려운 것 같고 어떻게 줄지를 생각해도 이렇게 수학적으로 의미 있게 푸는 방식이 대단했음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;수식이 많지만 notation이 자세히 되어있지 않아 추론해야 하는 부분이 많았음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Convergence of the reweighted training에 대한 내용을 수학적으로 증명하는 부분이 있는데 이해하지 못해서 다음에 이해하면 좋을 것 같음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Wed, 15 Apr 2020 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Learning-to-Reweight-Examples-for-Robust-Deep-Learning/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Learning-to-Reweight-Examples-for-Robust-Deep-Learning/</guid>
        
        <category>Meta Learning</category>
        
        <category>Machine Learning</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Machine Learning</category>
        
      </item>
    
      <item>
        <title>Mutually exclusive, Correlation, Independence간의 관계</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Mutually exclusive, correlation, independence가 무엇인지에 대해 개념적으로 알아보고 mutual exclusive와 independence간에 무슨 관계가 있는지 그리고 corre-lation 와 independence간에 무슨 관계가 있는지 알아보도록 한다.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\Mutually exclusive, Correlation, Independence\Mutually exclusive, Correlation, Independence-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Mutually exclusive, Correlation, Independence\Mutually exclusive, Correlation, Independence-2.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Mutually exclusive, Correlation, Independence가 무엇인지에 대해 개념적으로 알아보았으며, 서로 무슨 관계가 있는지에 대해 알아보았다. Independence와 mu-tually exclusive는 아무런 관계가 없다. Independence와 Correlation는 서로 관계가 있다. 만약 두 random variable이 independence하다면 두 random variable은 서로 uncorrelation이지만, 역은 성립하지 않는다.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;cs109, lecture05&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 25 Mar 2020 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Mutually-exclusive,Correlation,Independence/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Mutually-exclusive,Correlation,Independence/</guid>
        
        <category>Probability</category>
        
        
        <category>Documentations</category>
        
        <category>Probability</category>
        
      </item>
    
      <item>
        <title>Zero One Few Shot Learning</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://web.stanford.edu/class/archive/cs/cs109/cs109.1196/schedule.html&quot;&gt;CS109&lt;/a&gt;, Lecture 21 MLE부분을 공부하면서 zero, one shot learning에 대한 예시가 나왔는데 이해가 부족하여 정리해보려 한다. 먼저 zero-shot leaning이 무엇인지부터 시작해 one-shot leaning, few-shot learning이 무엇인지 살펴보도록 하겠다.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;zero-shot-learning&quot;&gt;Zero Shot Learning&lt;/h3&gt;
&lt;p&gt;Zero-shot learning이란 학습에 한번도 보지 못했던 데이터를 분류하기 원하는 학습 방법이다. 예를 들어 자전거라는 데이터가 많아 자전거 클래스에 대해서 성공적으로 학습을 했는데 테스트 이미지로 오토바이가 갑자기 나타났다고 하자. 우리가 원하는 것은 모델이 오토바이라는 자전거와 가까운 클래스를 만들어 주는 것이다. 하지만 기존 모델들은 한번도 못 본 이미지를 새로운 클래스로 분류할 수 없었다. 그래서 이러한 문제를 해결하기 위해 만든 방법이 zero-shot learning이다. 알아서 새로운 클래스를 만들어 분류해주는 방법이다. 많은 데이터가 필요한 딥러닝에서 레이블링이 된 수많은 카테고리에 대한 데이터를 수집하기 어렵기 때문에 zero-shot learning이 필요하다.&lt;/p&gt;

&lt;h3 id=&quot;one-shot-learning&quot;&gt;One Shot Learning&lt;/h3&gt;
&lt;p&gt;One-shot leaning이란 새로운 클래스에 대해 데이터 샘플이 단 한 개만 주어지고 이를 구별하는 것이다. 예를 들어 여러 개의 자전거가 있다고 하자. 사람이라면 자전거에 대해 아무런 지식이 없더라도 자전거 이미지 한 장만 보면 자전거라는 개념을 바로 학습할 수 있다. 그 이후로는 단 한번도 본적 없는 특이한 모양의 자전거를 본다면 “자전거”라는 사실을 바로 알아낼 수 있다. 하지만 일반적인 딥러닝 네트워크들은 한 클래스를 학습하기 위해 수백 또는 수천장이 넘는 이미지가 필요하다. 한 클래스에 대한 이미지가 수백 또는 수천장을 갖고 레이블링이 된 상태가 아니라면 딥러닝에 사용하기 어렵다. 그렇게 때문에 적은 이미지로도 학습하는 방법이 매우 중요하며 이를 해결하는 방법 중 하나는 one-shot learning이다.&lt;/p&gt;

&lt;h3 id=&quot;few-shot-learning&quot;&gt;Few Shot Learning&lt;/h3&gt;
&lt;p&gt;Few-shot learning이란 아주 적은 몇 장의 이미지로 데이터의 특징을 뽑아 식별할 수 있도록 하는 것이다. One-shot leaning에서는 한 장의 이미지만 보았다면 few-shot learning은 많지는 않지만 적은 양의 이미지만 가지고 이미지 분류를 하는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Zero, one, few shot learning모두 데이터의 수가 제한적이기 때문에 등장한 방법론이다. 많은 데이터를 이용해 복잡한 모델을 사용하여 이미지 분류를 잘 이끌어내는 연구 방법과는 반대로 적은 데이터로 이미지를 잘 분류하고자 방법론들이다.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;https://towardsdatascience.com/applications-of-zero-shot-learning-f65bb232963f&lt;/li&gt;
  &lt;li&gt;https://jayhey.github.io/&lt;/li&gt;
  &lt;li&gt;https://www.kakaobrain.com/blog&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 27 Feb 2020 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Zero,one,few-shot-learning/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Zero,one,few-shot-learning/</guid>
        
        <category>Probability</category>
        
        
        <category>Documentations</category>
        
        <category>Probability</category>
        
      </item>
    
      <item>
        <title>Paper Review. Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks@NIPS' 2015</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;r-cnn&quot;&gt;&lt;strong&gt;R-CNN&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;최초로 object detection task에 CNN을 사용하여 정확도와 속도를 획기적으로 향상시킴&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;r-cnn의-구조&quot;&gt;R-CNN의 구조&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\4.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;r-cnn-학습방법&quot;&gt;R-CNN 학습방법&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\15.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;r-cnn-experiments&quot;&gt;R-CNN Experiments&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\16.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;r-cnn-conclusions--reviews&quot;&gt;R-CNN Conclusions &amp;amp; Reviews&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;학습이 일어나는 부분
    &lt;ul&gt;
      &lt;li&gt;이미지 넷으로 이미 학습된 모델을 가져와 fine tuning하는 부분&lt;/li&gt;
      &lt;li&gt;SVM classifier를 학습시키는 부분&lt;/li&gt;
      &lt;li&gt;Bounding box regression하는 부분&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;학습시간이 오래 걸림
    &lt;ul&gt;
      &lt;li&gt;Selective search에서 뽑아낸 2000개의 영역 이미지들에 대해서 모두 CNN모델을 통과하므로 오래걸림&lt;/li&gt;
      &lt;li&gt;Selective search는 CPU를 사용하는 알고리즘&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AlexNet을 그대로 사용하기 위해 이미지를 224 x 224 크기로 강제로 warping시켰기 때문에 이미지 변형으로 인한 성능 손실&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CNN, SVM, Bounding box regression 총 세가지 모델을 필요로 하는 복잡한 구조&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-Stage Training을 수행, SVM에서 학습한 결과가 CNN을 업데이트 시키지 못함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;R-CNN은 최초로 object detection에 deep learning 방법인 CNN을 적용시킴&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;fast-r-cnn&quot;&gt;&lt;strong&gt;Fast R-CNN&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CNN fine tuning, bounding box regression, classification을 모두 하나의 네트워크에서 학습시키는 end to end 기법&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;R-CNN의 단점을 보완하기 위해 나온 논문&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;핵심 아이디어는 RoI pooling&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fast-r-cnn의-구조&quot;&gt;Fast R-CNN의 구조&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\22.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;fast-r-cnn-roi-pooling&quot;&gt;Fast R-CNN RoI Pooling&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\29.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;fast-r-cnn-multi-task-loss&quot;&gt;Fast R-CNN Multi Task Loss&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\31.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;fast-r-cnn-experiments&quot;&gt;Fast R-CNN Experiments&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\33.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;r-cnn-conclusions--reviews-1&quot;&gt;R-CNN Conclusions &amp;amp; Reviews&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Fast R-CNN의 training은 end to end로 single stage training을 하지만 region proposal로 selective search를 수행하기 때문에 전체적으로 2-stage detector로 봄&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fast R-CNN은 R-CNN이나 SPP-net에 비하면 뛰어난 성능을 보이며 학습시간과 테스트 시간이 감소됨, 하지만 region proposal에서 걸리는 시간이 총 2.3초 중 약 2초의 시간이 걸리기 때문에 Bottleneck이 생김&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;faster-r-cnn&quot;&gt;&lt;strong&gt;Faster R-CNN&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Region proposal 방법을 GPU를 통한 학습으로 진행하면 확실히 성능이 증가하고 시간이 감소될 것이라고 말함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Selective search를 사용하여 계산해왔던 region proposal 단계를 neural network 안으로 가져옴&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;핵심 아이디어는 region proposal network(PRN)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;faster-r-cnn-구조&quot;&gt;Faster R-CNN 구조&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\35.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\37.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;faster-r-cnn-region-proposal-network&quot;&gt;Faster R-CNN Region Proposal Network&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\40.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\41.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\42.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\43.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\44.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\45.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\46.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\47.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\48.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\49.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;faster-r-cnn-vs-fast-r-cnn&quot;&gt;Faster R-CNN vs Fast R-CNN&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\50.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-stage-구조-vs-1-stage-구조&quot;&gt;2-Stage 구조 vs 1-Stage 구조&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\51.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;faster-r-cnn-experiments&quot;&gt;Faster R-CNN Experiments&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\52.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\53.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;faster-r-cnn-conclusions--reviews&quot;&gt;Faster R-CNN Conclusions &amp;amp; Reviews&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CNN모델을 통해 Region proposal을 뽑아 빠른 속도를 냄&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RPN이 있어 positive한 anchor와 negative anchor의 비율을 맞춰 줄 수 있다는 점이 좋음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RPN에서 sliding window를 통해 anchor box를 만들고 난 뒤에 positive와 negative를 판별 할 수 있으므로 결국 RPN도 많은 anchor box를 만들어 주고 GT와 IoU 계산을 다 해주어야 함 (feature map이 클 경우)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RPN이 학습이 잘 되려면 좋은 anchor box를 만들어 주는 것이 중요하다고 생각됨&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;roi-align&quot;&gt;&lt;strong&gt;RoI Align&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Mask-RCNN 논문에서 제시한 방법&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;기존 RoI pooling의 문제점을 지적하여 나온 RoI align&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;roi-pooling의-문제점&quot;&gt;RoI Pooling의 문제점&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Fast R-CNN은 object detection을 위한 모델이었기 때문에 RoI pooling에서 아주 정확한 위치 정보를 담는 것이 중요하지 않았음 (Bounding box안에 object이 있으면 됨)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\54.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\55.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\56.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\57.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\58.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;roi-align-1&quot;&gt;RoI Align&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;RoI align은 Mask R-CNN에서 좋은 성능을 냈음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;2-Stage object detection에서 RoI pooling보다 RoI align을 썼을 때, 더 자세한 박스 좌표를 얻기 때문에 성능이 더 높음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Faster R-CNN\59.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\60.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Faster R-CNN\61.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;2-Stage기반의 모델을 자세히 살펴보았으며, RPN이 있으므로 확실히 Object을 잘 찾을 수 있다고 봄&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;여름에 열리는 그랜드 챌린지 2020에는 2-Stage기반의 모델을 선택해 구현해보고 싶음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;앞으로 2-Stage 기반의 최신 Object Detection 논문을 찾아볼 계획임&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Girshick et al, “Rich feature hierarchies for accurate object detection and semantic segmentation Tech report ”, CVPR, 2014&lt;/li&gt;
  &lt;li&gt;Uijlings et al, “Selective Search for Object Recognition”, IJCV, 2012&lt;/li&gt;
  &lt;li&gt;Stanford CS231n, lecture12, p54&lt;/li&gt;
  &lt;li&gt;https://deepsense.ai/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 18 Feb 2020 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks/</guid>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Coin Flip Independent</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://web.stanford.edu/class/archive/cs/cs109/cs109.1196/schedule.html&quot;&gt;CS109&lt;/a&gt;, Lecture14, 25page에 나오는 coin filp 문제에 대해 해결하려고 한다.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\Coin flip independent\Coin flip independent-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Coin flip independent\Coin flip independent-2.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Random variable X와 Y가 있을 때 서로 독립인지 아닌지를 살펴보았다. 독립이기 위해서는 \(P(X=x,Y=y)=P(X=x)P(Y=y)\)식을 만족해야 하며 이 식을 만족하지 않는다면, X와 Y는 서로 독립이 아니게 된다.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Mon, 10 Feb 2020 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Coin-Flip-Independent/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Coin-Flip-Independent/</guid>
        
        <category>Probability</category>
        
        
        <category>Documentations</category>
        
        <category>Probability</category>
        
      </item>
    
      <item>
        <title>Paper Review. EfficientDet - Scalable and Efficient Object Detection@CVPR' 2020</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/html/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Feature Pyramid Networ(FPN)의 구조 Weighted bi-directional FPN(BiFPN)을 제안함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;EfficientNet에서 제안한 Compound Scaling기법을 Object Detection에 적용함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One-stage Detection&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;proposed-methods&quot;&gt;&lt;strong&gt;Proposed Methods&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;bifpn&quot;&gt;BiFPN&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;One-Stage Detector의 여러 대표 모델인 SSD, RetinaNet, M2Det의 FPN들은 서로 다른 input feature들을 합칠 때 단순히 resize후 더해주는 방식을 지적함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;서로 다른 input feature들은 해상도가 다르기 때문에 output feature에 기여하는 정도를 다르게 가져야함을 주장함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;간단하지만 효과적인 weighted bi-directional FPN 구조를 제안함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BiFPN구조는 서로 다른 input feature들의 중요성을 학습을 통해 배울 수 있고, 성능을 많이 향상 시킬 수 있음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\EfficientDet\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\4.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;efficientdet&quot;&gt;EfficientDet&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\EfficientDet\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\9.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;focal-loss&quot;&gt;Focal Loss&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cross Entropy Loss를 조금 수정함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;잘 분류되는 이미지들에 대해서는 작은 가중치를 부여하는 반면 분류하기 어려운 일부 이미지에는 큰 가중치를 부여함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;쉽게 분류되는 대부분의 negative 샘플들에 의해서 학습이 압도 되는 문제를 해결 함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One-Stage 방식에서도 Two-Stage 만큼의 성능을 낼 수 있음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\EfficientDet\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\11.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\EfficientDet\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\EfficientDet\16.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;EfficientNet을 활용한 Object Detection이였으며, 앞으로 Segmentation에도 EfficientNet을 활용한 논문이 나올 것 같음&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Wed, 08 Jan 2020 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/EfficientDet-Scalable-and-Efficient-Object-Detection/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/EfficientDet-Scalable-and-Efficient-Object-Detection/</guid>
        
        <category>Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Arbitrary-Oriented Scene Text Detection via Rotation proposals@IEEE Transactions on Multimedia' 2018</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8323240?casa_token=nbnxyMRMpUgAAAAA:zvlxwwUb0TsqpOtKJb5Fw4p-Synz9pT3foz9RiWCFkKTtt6GMoZplHw_Vvwqwdawvih9boY5984&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이미지에서의 텍스트 감지를 위한 새로운 회전 기반 프레임 워크를 소개&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;텍스트 방향 각도 정보가 포함된 Rotation Region Proposal Networks (RRPN)를 제안함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rotation Region-of-Interest (RRoI) pooling layer을 제안함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Faster-RCNN과 같은 구조로 region proposal-based architecture이다&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\1.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-methods&quot;&gt;&lt;strong&gt;Proposed Methods&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;horizontal-region-proposal&quot;&gt;Horizontal Region Proposal&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;각 슬라이딩 위치에 k개의 anchor가 존재.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;k anchor에 대해 좌표를 나타내는 box regression(reg) layer
    &lt;ul&gt;
      &lt;li&gt;4k outputs (x, y, w, h)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;k anchor에 대해 점수를 나타내는 box-classification (cls) layer
    &lt;ul&gt;
      &lt;li&gt;2k scores (object, non object)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;anchor를 정하기 위한 parameter로는 Scale, Ratio를 사용함.
    &lt;ul&gt;
      &lt;li&gt;Ex) Scale : 1x, 2x, 4x, 1:2, Ratio : 1:1, 2:1&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;기존 수평 anchor box선택 전략은 총 anchor의 수를 낮게 유지할 수 있음.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;하지만 실제 이미지에서 텍스트를 찾는 경우 박스가 부자연스러운 모양임.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;회전된 박스를 찾는 RPN을 제안함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\2.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;rotated-bounding-box-representation&quot;&gt;Rotated Bounding Box Representation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\3.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;anchor-strategy&quot;&gt;Anchor Strategy&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\4.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;learning-of-rotated-proposal&quot;&gt;Learning of Rotated Proposal&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\9.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;skew-iou-computation&quot;&gt;Skew IoU Computation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\13.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;rroi-pooling-layer&quot;&gt;RRoI Pooling Layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\14.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\21.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\22.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\24.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;DataSet : MSRA-TD500, ICDAR2015, ICDAR2013&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implementation Details&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Learning rate : 0.001 for first 200,000 iterations, 0.0001 for the next 100,000 iterations&lt;/li&gt;
      &lt;li&gt;Weight decay : 0.0005&lt;/li&gt;
      &lt;li&gt;Momentum : 0.9&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Arbitrary-Oriented Scene Text Detection via Rotation proposals\32.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;전체적인 구조는 Faster-RCNN과 매우 똑같아서 이해하기가 쉬웠음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RPN을 만들 때 각도 값 𝜃만 추가해 주면 되는 간단한 문제라고 생각 할 수 있지만, RoI Pooling을 할 때 회전된 박스안에 픽셀 값을 받아오는 작업이 쉽지 않다고 느껴짐&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Tue, 26 Nov 2019 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Arbitrary-Oriented-Scene-Text-Detection-via-Rotation-proposals/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Arbitrary-Oriented-Scene-Text-Detection-via-Rotation-proposals/</guid>
        
        <category>Object Detection</category>
        
        <category>Oriented Object Detection</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Object Detection</category>
        
      </item>
    
      <item>
        <title>Paper Review. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network@CVPR' 2017</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;super-resolution이란&quot;&gt;&lt;strong&gt;Super Resolution이란?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\2.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\4.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\6.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;딥러닝-sr의-시작--srcnn&quot;&gt;&lt;strong&gt;딥러닝 SR의 시작 – SRCNN&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\8.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\9.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\10.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\14.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;딥러닝-sr의-발전--vdsr&quot;&gt;&lt;strong&gt;딥러닝 SR의 발전 – VDSR&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\15.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\16.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\17.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\18.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\19.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\20.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\21.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;딥러닝-sr의-발전--srgan&quot;&gt;&lt;strong&gt;딥러닝 SR의 발전 – SRGAN&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\23.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\24.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\25.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\26.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\27.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\28.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\29.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\30.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\31.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\32.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\33.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\34.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\35.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\36.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\37.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\38.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\39.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\40.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Object detection같은 경우 input image data를 Super resolution으로 화질 개선하여 사용하면 성능이 올라갈 것이다. 하지만 image의 크기가 2배or 4배 가량 커지기 때문에 memory문제가 발생할 수 있음.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;명확하고 신뢰할 수 있는 하나의 평가 지표가 없어 PSNR, SSIM, MOS와 같이 여러 지표를 비교해가며 성능을 확인해야 함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Method를 선택할 때, PSNR, SSIM, MOS 중에서 어떤 것을 중점으로 보고 선택하고 사용해야 할지 고르기가 힘들 것 같음&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;https://hoya012.github.io/&lt;/li&gt;
  &lt;li&gt;https://www.slideshare.net/NaverEngineering/deep-learning-super-resolution&lt;/li&gt;
  &lt;li&gt;https://leedakyeong.tistory.com/&lt;/li&gt;
  &lt;li&gt;Image Super-Resolution Using Deep Convolutional Networks, 2014 ECCV&lt;/li&gt;
  &lt;li&gt;Accurate Image Super-Resolution Using Very Deep Convolutional Networks, 2016 CVPR&lt;/li&gt;
  &lt;li&gt;Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, 2016 arXiv&lt;/li&gt;
  &lt;li&gt;Deep Learning for Single Image Super-Resolution: A Brief Review, 2018 arXiv&lt;/li&gt;
  &lt;li&gt;A Deep Journey into Super-resolution: A Survey, 2019 arXiv&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 22 Oct 2019 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Photo-Realistic-Single-Image-Super-Resolution-Using-a-Generative-Adversarial-Network/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Photo-Realistic-Single-Image-Super-Resolution-Using-a-Generative-Adversarial-Network/</guid>
        
        <category>Super Resolution</category>
        
        <category>GAN</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Super Resolution</category>
        
      </item>
    
      <item>
        <title>Second Order Talyor Expansion</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\second-order Taylor expansion\second-order Taylor expansion-1.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\second-order Taylor expansion\second-order Taylor expansion-2.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\second-order Taylor expansion\second-order Taylor expansion-3.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\second-order Taylor expansion\second-order Taylor expansion-4.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\second-order Taylor expansion\second-order Taylor expansion-5.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;http://cs231n.stanford.edu/ lecture 8&lt;/li&gt;
  &lt;li&gt;https://ratsgo.github.io/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 20 Oct 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Second-Order-Taylor-Expansion/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Second-Order-Taylor-Expansion/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>Negative Sampling</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Negative sampling은 Word2vec의 training방법중 하나이다. Negative sampling이 무엇인지 살펴보기 전에 먼저 Word2vec란 무엇인지 간단하게 알아보고, Word2vec의 두가지 방식인 CBOW, skip-gram에 대해서 간단히 알아보고, 마지막으로 단어가 벡터로 잘 바뀔 수 있도록 training시키는 방법중 하나인 Negative sampling에 대해서 자세히 다루겠다.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\negative sampling\negative sampling-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\negative sampling\negative sampling-2.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\negative sampling\negative sampling-3.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclousion&quot;&gt;&lt;strong&gt;Conclousion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Word2vec를 training 할 때, negative sampling을 한다면 성능이 향상되고 계산량이 낮아지므로 좋은 training 기법이라고 할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Y. Goldberg, et al., word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method. https://brunch.co.kr/&lt;/li&gt;
  &lt;li&gt;https://ratsgo.github.io/&lt;/li&gt;
  &lt;li&gt;http://solarisailab.com/&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 10 Oct 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Negative-Sampling/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Negative-Sampling/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>GRU Backpropagation</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;LSTM에서는 출력, 입력, 삭제 게이트라는 3개의 게이트가 존재했었다. 반면, GRU에서는 업데이트 게이트와 리셋 게이트 두 가지 게이트만이 존재한다. GRU는 LSTM보다 게이트 수가 하나 적어 학습 속도면에서는 빠르지만 성능면에서는 LSTM이 GRU보다 성능이 뛰어나다는 것으로 알려져 있다. 이 문서에서는 GRU의 게이트를 살펴보고 GRU의 weigth들이 어떻게 Update되는지 Backpropagation을 통해 살펴보도록 하겠다.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\GRU Backpropagation\GRU Backpropagation-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\GRU Backpropagation\GRU Backpropagation-2.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\GRU Backpropagation\GRU Backpropagation-3.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclousion&quot;&gt;&lt;strong&gt;Conclousion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;GRU에서 각 게이트의 파라미터 들이 어떻게 update되는지 살펴보았다. LSTM에 비해 게이트 수가 하나 적은 만큼 성능이 비교적 떨어지는 것 같고, 일반적인 경우에는 LSTM을 사용한다.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;https://medium.com/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 01 Oct 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/GRU-Backpropagation/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/GRU-Backpropagation/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>Glove(Global Vectors for Word Representation)</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;GloVe(Global Vectors for Word Representation)는 count based와 direct predict를 모두 사용하는 방법론으로 2014년에 미국 스탠포드대학에서 개발한 단어 임베딩 방법론이다. Count based의 LSA(Latent Semantic Analysis)와 direct predict의 Word2Vec에 단점을 지적하며 이를 보완한다는 목적으로 나왔다. 실제로도 GloVe는 Word2Vec만큼이나 뛰어난 성능을 보여준다. 단정적으로 Word2Vec와 GloVe 중에서 어떤 것이 더 뛰어나다고 말할 수는 없고, 이 두 가지 전부를 사용해보고 성능이 더 좋은 것을 사용하는 것이 바람직하다고 한다.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\Glove\Glove-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Glove\Glove-2.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Glove\Glove-3.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Glove\Glove-4.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Glove\Glove-5.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclousion&quot;&gt;&lt;strong&gt;Conclousion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;GloVe는 임베딩 된 center word와 context word 벡터의 내적이 전체 corpus에서의 co-occurrence probability가 되도록 만든 것 즉, count based의 장점과 direct predic-tion의 장점을 가져와서 기존 방법론의 단점을 보완한 것이 성능향상에 도움을 주었다고 생각한다.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;https://wikidocs.net/book/2155&lt;/li&gt;
  &lt;li&gt;https://ratsgo.github.io/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 27 Sep 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Glove/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Glove/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>Affinity Propagation Algorithm</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;비지도 학습의 과업에는 군집화, 밀도 추정, 공간변환 이렇게 크게 세가지로 나눌 수 있다. 이중 군집화의 한 종류인, Affinity Propagation Algorithm에 대해서 알아보려고 한다. Affinity Propagation Algorithm는 모든 데이터가 특정한 기준에 따라 자신을 대표할 대표 데이터를 선택한다. 만약 스스로가 자기 자신을 대표하게 되면 클러스터의 중심이 된다. 군집의 개수를 k라고 했을 때, K-means 알고리즘과는 다르게 이것을 hyperparameter로써 선택해 주는 것이 아니라 자동으로 알아낸다. Responsibility, Availability라는 두 종류의 친밀도 행렬을 이용하여 군집화 한다. Responsibility, Availability행렬이 무엇인지 자세히 알아보고 알고리즘이 어떻게 동작하는지 살펴보려고 한다.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\Affinity Propagation Algorithm\Affinity Propagation Algorithm-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Affinity Propagation Algorithm\Affinity Propagation Algorithm-2.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Affinity Propagation Algorithm\Affinity Propagation Algorithm-3.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Affinity Propagation Algorithm\Affinity Propagation Algorithm-4.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclousion&quot;&gt;&lt;strong&gt;Conclousion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;K-means의 알고리즘은 군집의 개수와, 초기 군집대표의 값을 정해주어야 하며 초기 군집의 값을 어떻게 설정해주는지에 대해 결과가 달라진다. 하지만 Affinity Propagation Algorithm은 군집의 개수와, 초기 군집대표의 값을 정해주지 않아도 되며, 자가 유사도 \(s(k, k)\)의 값을 어떻게 설정해주는지에 따라 군집의 개수가 달라진다는 차이점이 있다. 비지도 학습은 크게 군집화, 밀도 추정, 공간 변환으로 나눌 수 있는데 이 중 군집화에 한 종류인 Affinity Propagation Algorithm에 대해 살펴 보았다.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;https://www.geeksforgeeks.org/&lt;/li&gt;
  &lt;li&gt;오일석, 기계학습, 한빛아카데미, p323~325&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 20 Sep 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Affinity-Propagation-Algorithm/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Affinity-Propagation-Algorithm/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>CS231 Lecture4 142p Backpropagation</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;CS231n Lecture4 142페이지에 나오는 computational graph에서 \(W_1, W_2\)에 대하여 backporpagation을 증명한다.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-2.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-3.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-4.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-5.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-6.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-7.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Backpropagation\Backpropagation-8.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;http://cs231n.stanford.edu/ lecture 4&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 15 Sep 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Backpropagation/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Backpropagation/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>Softmax &amp; Cross Entropy</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Softmax 함수가 무엇인지 알기 위해 먼저 순차적으로 정보이론, Shannon entropy, Kullback-Leibler divergence, 그리고 Cross entropy에 대해서 살펴본다. 
마지막으로 Softmax 함수가 어떻게 Backpropagation되어 weigth이 update 되는지 살펴 보도록 한다.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\Softmax, Cross entropy\Softmax, Cross entropy-1.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Softmax, Cross entropy\Softmax, Cross entropy-2.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Softmax, Cross entropy\Softmax, Cross entropy-3.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Softmax, Cross entropy\Softmax, Cross entropy-4.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Softmax, Cross entropy\Softmax, Cross entropy-5.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Softmax, Cross entropy\Softmax, Cross entropy-6.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Softmax 함수가 무엇인지 구체적으로 알게 되었으며, 실제 Score와 Softmax를 통해 나온 확률 값을 어떻게 비교해야 하는지도 알게 되었다. Loss를 softmax 함수 x에 대해 backpropagation을 해봄으로써 x가 변화할 때의 Loss의 변화량을 알게 되었다.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;https://ratsgo.github.io/&lt;/li&gt;
  &lt;li&gt;https://brunch.co.kr/&lt;/li&gt;
  &lt;li&gt;https://selfish-developer.com/&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 29 Aug 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Softmax-Cross-entropy/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Softmax-Cross-entropy/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>Activation Function Backpropagation</title>
        <description>&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Activation Function들이  Backpropagation가 진행되는 과정을 Derive하는 자료이다.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-01.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-02.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-03.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-03.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-04.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-05.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-06.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-07.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-08.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-09.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-10.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-11.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-12.png&quot; width=&quot;900&quot; /&gt;
&lt;img src=&quot;\assets\doc\Activation Function Backpropagation\Activation Function Backpropagation-13.png&quot; width=&quot;900&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Fei-Fei Li, Justin Johnson, Serena Yeung / cs231n-2019-lecture04.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 22 Aug 2019 20:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Activation-Function-Backpropagation/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Activation-Function-Backpropagation/</guid>
        
        <category>Deep Learning</category>
        
        
        <category>Documentations</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>Paper Review. Fully Convolutional Networks for Semantic Segmentation@CVPR' 2015</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8323240?casa_token=nbnxyMRMpUgAAAAA:zvlxwwUb0TsqpOtKJb5Fw4p-Synz9pT3foz9RiWCFkKTtt6GMoZplHw_Vvwqwdawvih9boY5984&quot;&gt;Paper link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;딥러닝을 이용한 Segmentation의 시초라고 할 수 있는 논문&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이 논문은 fully convolution을 이용하여 어떻게 semantic segmentation을 deep learning으로 풀었는지 보여줌&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;semantic-segmentation이란&quot;&gt;Semantic Segmentation이란?&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\1.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\2.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;proposed-methods&quot;&gt;&lt;strong&gt;Proposed Methods&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;fully-convolutional-networks&quot;&gt;Fully Convolutional Networks&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\3.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\4.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;deconvolution&quot;&gt;Deconvolution&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\5.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\6.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\7.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\8.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;skip-layer&quot;&gt;Skip layer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\9.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\10.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\11.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\12.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\13.PNG&quot; width=&quot;800&quot; /&gt;
&lt;img src=&quot;\assets\papers\Fully_Convolutional_Networks_for_Semantic_Segmentation\14.PNG&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions--reviews&quot;&gt;&lt;strong&gt;Conclusions &amp;amp; Reviews&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Fully connected layer를 Fully convolutional layer로 바꾸고 이전 layer의 정보를 이용해 upsampling하는 방법이 인상깊었음.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Activation map의 크기를 낮췄다가 높이는 방식이므로 중간에 정보 손실이 발생해 윤곽선이 뚜렷하게 나오지 않음 -&amp;gt; Dilated Convolution : 필터 내부에 zero padding을 추가해 강제로 receptive field를 늘리는 방법&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/h2&gt;
</description>
        <pubDate>Thu, 25 Jul 2019 19:00:00 +0900</pubDate>
        <link>https://joochann.github.io/posts/Fully-Convolutional-Networks-for-Semantic-Segmentation/</link>
        <guid isPermaLink="true">https://joochann.github.io/posts/Fully-Convolutional-Networks-for-Semantic-Segmentation/</guid>
        
        <category>Segmentation</category>
        
        <category>Semantic Segmenation</category>
        
        <category>CV</category>
        
        
        <category>Paper Reviews</category>
        
        <category>Segmentation</category>
        
      </item>
    
  </channel>
</rss>