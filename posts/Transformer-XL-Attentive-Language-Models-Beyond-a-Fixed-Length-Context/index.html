<!DOCTYPE html><html lang="ko-KR" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="layout-lang" content="ko-KR"><meta name="day-prompt" content="d ago"><meta name="hour-prompt" content="hr ago"><meta name="minute-prompt" content="min ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="Paper Review. Transformer-XL Attentive Language Models Beyond a Fixed-Length Context@ACL’ 2019" /><meta name="author" content="JooChan Park" /><meta property="og:locale" content="ko_KR" /><meta name="description" content="Paper link" /><meta property="og:description" content="Paper link" /><link rel="canonical" href="https://joochann.github.io/posts/Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context/" /><meta property="og:url" content="https://joochann.github.io/posts/Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context/" /><meta property="og:site_name" content="CV Researcher" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-11-10T14:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Paper Review. Transformer-XL Attentive Language Models Beyond a Fixed-Length Context@ACL’ 2019" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@JooChan Park" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JooChan Park"},"dateModified":"2020-11-10T14:00:00+09:00","datePublished":"2020-11-10T14:00:00+09:00","description":"Paper link","headline":"Paper Review. Transformer-XL Attentive Language Models Beyond a Fixed-Length Context@ACL’ 2019","mainEntityOfPage":{"@type":"WebPage","@id":"https://joochann.github.io/posts/Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context/"},"url":"https://joochann.github.io/posts/Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context/"}</script><title>Paper Review. Transformer-XL Attentive Language Models Beyond a Fixed-Length Context@ACL' 2019 | CV Researcher</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="CV Researcher"><meta name="application-name" content="CV Researcher"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang=""><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/my_img.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">CV Researcher</a></div><div class="site-subtitle font-italic">Wear Your Failure As A Badge Of Honour</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about-me/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT-ME</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/JOOCHANN" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['green261535','gmail.com'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Paper Review. Transformer-XL Attentive Language Models Beyond a Fixed-Length Context@ACL' 2019</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Paper Review. Transformer-XL Attentive Language Models Beyond a Fixed-Length Context@ACL' 2019</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> JooChan Park </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Nov 10, 2020, 2:00 PM +0900" >Nov 10, 2020<i class="unloaded">2020-11-10T14:00:00+09:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="231 words">1 min read</span></div></div><div class="post-content"><ul><li><a href="https://arxiv.org/abs/1901.02860">Paper link</a></ul><h2 id="abstract"><strong>Abstract</strong></h2><ul><li><p>기존의 transformer는 고정된 개수(512)의 token들을 갖는 한 개의 segment만을 input으로 사용하여, 연속된 segment들 간의 dependency를 반영하지 못함</p><li><p>현재 segment를 처리할 때, 이전 segment를 처리할 때 계산된 hidden state들을 사용하는 recurrence를 추가하여 위 문제를 해결함</p><li><p>모델에 적합한 positional encoding을 변형하였음</p><li><p>Transformer-XL은 language modeling에서 SOTA의 성능을 기록함</p></ul><h2 id="introduction"><strong>Introduction</strong></h2><h3 id="transformer">Transformer</h3><p><img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\1.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\2.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\3.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\4.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\5.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\6.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\7.PNG" width="800" /></p><h2 id="proposed-method"><strong>Proposed Method</strong></h2><h3 id="vanilla-transformer-language-model">Vanilla Transformer Language Model</h3><p><img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\8.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\9.PNG" width="800" /></p><h3 id="segment-level-recurrence-with-state-reuse">Segment-Level Recurrence with State Reuse</h3><p><img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\10.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\11.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\12.PNG" width="800" /></p><h3 id="relative-position-encoding">Relative Position Encoding</h3><p><img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\13.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\14.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\15.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\16.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\17.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\18.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\19.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\20.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\21.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\22.PNG" width="800" /></p><h3 id="transformer-xl">Transformer XL</h3><p><img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\23.PNG" width="800" /></p><h2 id="experiments"><strong>Experiments</strong></h2><p><img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\25.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\26.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\27.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\28.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\29.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\30.PNG" width="800" /> <img data-proofer-ignore data-src="\assets\papers\Transformer-XL Attentive Language Models Beyond a Fixed-Length Context\31.PNG" width="800" /></p><h2 id="conclusions--reviews"><strong>Conclusions &amp; Reviews</strong></h2><ul><li><p>RNN 계열 모델과 vanilla Transformer model보다 long-term dependency를 더 잘 잡아냈음</p><li><p>이전 segment를 저장해두고 사용함으로써 prediction시 상당한 속도 향상을 달성함</p><li><p>Recurrence 방법을 사용하기 위해 relative position encoding을 수식적으로 풀어서 의미부여를 하고, 적용한 것이 대단하다고 느낌</p></ul><h2 id="reference"><strong>Reference</strong></h2></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/paper-reviews/'>Paper Reviews</a>, <a href='/categories/machine-translation/'>Machine Translation</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/attention/" class="post-tag no-text-decoration" >Attention</a> <a href="/tags/machine-translation/" class="post-tag no-text-decoration" >Machine Translation</a> <a href="/tags/nlp/" class="post-tag no-text-decoration" >NLP</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> <span class="text-muted small"></span></div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Paper Review. Transformer-XL Attentive Language Models Beyond a Fixed-Length Context@ACL' 2019 - CV Researcher&url=https://joochann.github.io/posts/Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Paper Review. Transformer-XL Attentive Language Models Beyond a Fixed-Length Context@ACL' 2019 - CV Researcher&u=https://joochann.github.io/posts/Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Paper Review. Transformer-XL Attentive Language Models Beyond a Fixed-Length Context@ACL' 2019 - CV Researcher&url=https://joochann.github.io/posts/Transformer-XL-Attentive-Language-Models-Beyond-a-Fixed-Length-Context/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink('', '')" data-toggle="tooltip" data-placement="top" title=""> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Bidirectional-LSTM-CRF-Models-for-Sequence-Tagging/">Paper Review. Bidirectional LSTM-CRF Models for Sequence Tagging@arXiv' 2015</a><li><a href="/posts/Fully-Convolutional-Networks-for-Semantic-Segmentation/">Paper Review. Fully Convolutional Networks for Semantic Segmentation@CVPR' 2015</a><li><a href="/posts/Arbitrary-Oriented-Scene-Text-Detection-via-Rotation-proposals/">Paper Review. Arbitrary-Oriented Scene Text Detection via Rotation proposals@IEEE Transactions on Multimedia' 2018</a><li><a href="/posts/EfficientDet-Scalable-and-Efficient-Object-Detection/">Paper Review. EfficientDet - Scalable and Efficient Object Detection@CVPR' 2020</a><li><a href="/posts/Learning-to-Reweight-Examples-for-Robust-Deep-Learning/">Paper Review. Learning to Reweight Examples for Robust Deep Learning@ICML' 2018</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/cv/">CV</a> <a class="post-tag" href="/tags/object-detection/">Object Detection</a> <a class="post-tag" href="/tags/deep-learning/">Deep Learning</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/oriented-object-detection/">Oriented Object Detection</a> <a class="post-tag" href="/tags/probability/">Probability</a> <a class="post-tag" href="/tags/attention/">Attention</a> <a class="post-tag" href="/tags/machine-translation/">Machine Translation</a> <a class="post-tag" href="/tags/3d/">3D</a> <a class="post-tag" href="/tags/activity-recognition/">Activity Recognition</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Attention-is-all-you-need/"><div class="card-body"> <span class="timeago small" >Sep 29, 2020<i class="unloaded">2020-09-29T14:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Paper Review. Attention is all you need@NIPS' 2017</h3><div class="text-muted small"><p> Paper link Abstract 당시 가장 좋은 모델은 Attention 메커니즘을 통해 encoder와 decoder를 연결한 모델이 성능이 가장 좋음 기존 모델들처럼 RNN또는 CNN을 사용하지 않고 attention 메커니즘만을 기반으로 하는 Transformer 모델을 제안함 기계 ...</p></div></div></a></div><div class="card"> <a href="/posts/Bidirectional-LSTM-CRF-Models-for-Sequence-Tagging/"><div class="card-body"> <span class="timeago small" >Sep 8, 2020<i class="unloaded">2020-09-08T14:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Paper Review. Bidirectional LSTM-CRF Models for Sequence Tagging@arXiv' 2015</h3><div class="text-muted small"><p> Paper link Abstract LSTM 레이어와 CRF 레이어를 이용하여 LSTM-CRF 모델을 만듦 BI-LSTM 레이어와 CRF 레이어를 이용하여 BI-LSTM-CRF 모델을 만듦 BI-LSTM-CRF 모델을 이용하면 POS tagging태스크에서 높은 성능을 도달할 수 있음 ...</p></div></div></a></div><div class="card"> <a href="/posts/Dynamic-Anchor-Learning-for-Arbitrary-Oriented-Object-Detection/"><div class="card-body"> <span class="timeago small" >Jan 20, 2021<i class="unloaded">2021-01-20T14:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Paper Review. Dynamic Anchor Learning for Arbitrary-Oriented Object Detection@AAAI' 2021</h3><div class="text-muted small"><p> Paper link Abstract 기존 모델들은 IoU를 적용하여 positive 와 negative 앵커를 샘플링을 함 Positive 앵커는 항상 정확한 탐지를 보장할 수 없지만, 일부 negative 앵커는 정확한 위치를 파악할 수 있음 이는 IoU를 통한 앵커의 품질 평가가 적절하지 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Attention-is-all-you-need/" class="btn btn-outline-primary" prompt="previous"><p>Paper Review. Attention is all you need@NIPS' 2017</p></a> <a href="/posts/Dynamic-Anchor-Learning-for-Arbitrary-Oriented-Object-Detection/" class="btn btn-outline-primary" prompt="next"><p>Paper Review. Dynamic Anchor Learning for Arbitrary-Oriented Object Detection@AAAI' 2021</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://twitter.com/username">JOOCHANN</a>. <span class="text-muted"></span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/cv/">CV</a> <a class="post-tag" href="/tags/object-detection/">Object Detection</a> <a class="post-tag" href="/tags/deep-learning/">Deep Learning</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/oriented-object-detection/">Oriented Object Detection</a> <a class="post-tag" href="/tags/probability/">Probability</a> <a class="post-tag" href="/tags/attention/">Attention</a> <a class="post-tag" href="/tags/machine-translation/">Machine Translation</a> <a class="post-tag" href="/tags/3d/">3D</a> <a class="post-tag" href="/tags/activity-recognition/">Activity Recognition</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://joochann.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
